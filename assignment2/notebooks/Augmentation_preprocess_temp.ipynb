{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Augmentation_preprocess",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNo//ov5cafDZMmDVnbW9DN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment2/notebooks/Augmentation_preprocess_temp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_7k2MLtA0Q9",
        "outputId": "ae308684-ee82-4d3f-a1cb-302042ee1449"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Dec 27 11:50:37 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "drive.mount('/content/gdrive')\n",
        "#shutil.copy('/content/gdrive/MyDrive/MTUNLPA2/saved_pickle.tar.gz', '/tmp/saved_pickle.tar.gz')"
      ],
      "metadata": {
        "id": "daoDMfvJpfqa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "453fcf2f-e063-4f19-d9d9-12ce49989236"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile copy_train.sh\n",
        "#!/bin/bash\n",
        "cp /content/gdrive/MyDrive/NLP/NLP-Save-Train/train.tar.gz /tmp\n",
        "cd /tmp\n",
        "gunzip train.tar.gz\n",
        "tar -xf train.tar\n",
        "ls /tmp/train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B54kD1pBclhf",
        "outputId": "a86118f3-0471-472b-e705-c9b53b7ad173"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting copy_train.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/bin/bash copy_train.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrV4qjppc6CL",
        "outputId": "e02e7e0b-f473-48ec-f396-9eb2704ac6a1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gzip: train.tar already exists; do you wish to overwrite (y or n)? y\n",
            "attention_mask\tid\t   offset_mapping\t       start_positions\n",
            "end_positions\tinput_ids  overflow_to_sample_mapping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2nEpGzfbvwj-"
      },
      "outputs": [],
      "source": [
        "!pip install torchvision==0.8.2         -q -q -q\n",
        "!pip install torchtext==0.8.1           -q -q -q\n",
        "!pip install torchaudio==0.7.2          -q -q -q\n",
        "!pip install torch==1.7.1               -q -q -q\n",
        "!pip install tqdm==4.49.0               -q -q -q\n",
        "!pip install transformers==4.2.2        -q -q -q\n",
        "!pip install tensorflow                 -q -q -q\n",
        "!pip install tensorboard                -q -q -q\n",
        "!pip install tensorboardX               -q -q -q\n",
        "!pip install --upgrade virtualenv       -q -q -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/gdrive/MyDrive/NLP/augmented/augmented.pickle /tmp/augmented_data.pickle"
      ],
      "metadata": {
        "id": "VRZiyEjd-TGG"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/tmp/augmented_data.pickle\", \"rb\") as f:\n",
        "    import pickle\n",
        "    ddd = pickle.load(f)"
      ],
      "metadata": {
        "id": "qrptRnT2zmlp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min([len(c) for c in ddd['context']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRuw7Q80zyLz",
        "outputId": "29f932f9-56e1-4b6c-984e-cce2a0e18bdc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!USERNAME=bhattacharjee PASSWORD=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 git clone \"https://${USERNAME}:${PASSWORD}@github.com/bhattacharjee/mtu-nlp-assignment.git\"\n",
        "!git clone https://bhattacharjee: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@github.com/bhattacharjee/mtu-nlp-assignment.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOK9thHC0rC1",
        "outputId": "6beb59e6-9418-4990-b754-6edf37e068e1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'mtu-nlp-assignment' already exists and is not an empty directory.\n",
            "fatal: destination path 'mtu-nlp-assignment' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# args.py"
      ],
      "metadata": {
        "id": "454WJr-Rq5rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /tmp/args.py\n",
        "import argparse\n",
        "\n",
        "def get_train_test_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--batch-size', type=int, default=32)\n",
        "    parser.add_argument('--num-epochs', type=int, default=3)\n",
        "    parser.add_argument('--lr', type=float, default=3e-5)\n",
        "    parser.add_argument('--num-visuals', type=int, default=10)\n",
        "    parser.add_argument('--seed', type=int, default=42)\n",
        "    parser.add_argument('--save-dir', type=str, default='save/')\n",
        "    parser.add_argument('--train', action='store_true')\n",
        "    parser.add_argument('--eval', action='store_true')\n",
        "    parser.add_argument('--train-datasets', type=str, default='squad,nat_questions,newsqa')\n",
        "    parser.add_argument('--run-name', type=str, default='multitask_distilbert')\n",
        "    parser.add_argument('--recompute-features', action='store_true')\n",
        "    parser.add_argument('--train-dir', type=str, default='datasets/indomain_train')\n",
        "    parser.add_argument('--val-dir', type=str, default='datasets/indomain_val')\n",
        "    parser.add_argument('--eval-dir', type=str, default='datasets/oodomain_test')\n",
        "    parser.add_argument('--eval-datasets', type=str, default='race,relation_extraction,duorc')\n",
        "    parser.add_argument('--do-train', action='store_true')\n",
        "    parser.add_argument('--do-eval', action='store_true')\n",
        "    parser.add_argument('--sub-file', type=str, default='')\n",
        "    parser.add_argument('--visualize-predictions', action='store_true')\n",
        "    parser.add_argument('--eval-every', type=int, default=2500)\n",
        "    args = parser.parse_args()\n",
        "    return args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr1LRFt1GAUJ",
        "outputId": "accf7129-56b7-465c-b616-01ae6d078a50"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /tmp/args.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# chunkdata.py"
      ],
      "metadata": {
        "id": "j_3XY82PI7uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /tmp/ChunkData.py\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "# Copyright 2021 Rajbir Bhattacharjee\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "# \n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "# SOFTWARE.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pathlib\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "import threading\n",
        "import glob\n",
        "\n",
        "class ChunkDataCommon():\n",
        "    def __init__(self, directory:str, chunk_size=16):\n",
        "        self.directory = directory\n",
        "        self.chunk_size = chunk_size\n",
        "        self.length = -1\n",
        "        self.lock = None\n",
        "        self.current_slice = None\n",
        "        self.current_slice_start = -1\n",
        "\n",
        "    def get_pickle_file_name(self, st, en):\n",
        "        return f\"{self.directory}/array_chunk-{st:08d}-{en:08d}.pickle\"\n",
        "\n",
        "    def get_conf_file_name(self):\n",
        "        with self.lock:\n",
        "            return f\"{self.directory}/config.json\"\n",
        "\n",
        "    def get_slice_start_for_index(self, ind):\n",
        "        return (ind // self.chunk_size) * self.chunk_size\n",
        "\n",
        "    def load_slice(self, ind_start):\n",
        "        with self.lock:\n",
        "            ind_end = ind_start + self.chunk_size - 1\n",
        "            filename =self.get_pickle_file_name(ind_start, ind_end)\n",
        "            with open(filename, \"rb\") as f:\n",
        "                self.current_slice = pickle.load(f)\n",
        "                self.current_slice_start = ind_start\n",
        "                return\n",
        "            self.current_slice = None\n",
        "            self.current_slice_start = -1\n",
        "            raise IndexError\n",
        "\n",
        "    def __len__(self):\n",
        "        with self.lock:\n",
        "            return self.length if self.length > 0 else 0\n",
        "\n",
        "\n",
        "class ChunkDataReader(ChunkDataCommon):\n",
        "    def __init__(self, d, chunk_size=16, lock=None):\n",
        "        if isinstance(d, str):\n",
        "            directory = d\n",
        "        elif isinstance(d, ChunkDataCommon):\n",
        "            directory = d.directory\n",
        "            chunk_size = d.chunk_size\n",
        "\n",
        "        super(self.__class__, self).__init__(directory, chunk_size=chunk_size)\n",
        "\n",
        "        self.lock = threading.RLock() if lock is None else lock\n",
        "        self.length = -1\n",
        "\n",
        "        self.current_slice = None\n",
        "        self.current_slice_start = -1\n",
        "        self.chunk_size = chunk_size\n",
        "        self.directory = directory\n",
        "\n",
        "        with open(self.get_conf_file_name(), \"r\") as f:\n",
        "            conf = json.load(f)\n",
        "            length = conf['length']\n",
        "            self.length = length if length > 0 else -1\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        with self.lock:\n",
        "            if ind >= self.length:\n",
        "                raise IndexError\n",
        "            if (ind < 0):\n",
        "                ind = self.length + ind \n",
        "            with self.lock:\n",
        "                ind_start = self.get_slice_start_for_index(ind)\n",
        "                if ind_start != self.current_slice_start:\n",
        "                    self.load_slice(ind_start)\n",
        "                ind = ind - self.current_slice_start\n",
        "                return self.current_slice[ind]\n",
        "\n",
        "\n",
        "\n",
        "class ChunkDataWriter(ChunkDataCommon):\n",
        "    \"\"\"Uses pickle, but splits into multiple files.\n",
        "    behaves like an array and supports the append() method,\n",
        "    and finalize method()\"\"\"\n",
        "    def __init__(self, directory:str, chunk_size=16):\n",
        "        super(self.__class__, self).__init__(directory, chunk_size)\n",
        "        self.lock = threading.RLock()\n",
        "\n",
        "        # The reader object for __getitem__\n",
        "        self.reader = None\n",
        "\n",
        "        # the current slice of the array\n",
        "        self.current_slice = list()\n",
        "\n",
        "        # Length of all items\n",
        "        self.length = 0\n",
        "\n",
        "        # Where does the current slice start from\n",
        "        self.current_slice_start = -1\n",
        "\n",
        "        self.__setitem__ = None\n",
        "\n",
        "        if not os.path.exists(self.directory):\n",
        "            pathlib.Path(self.directory).mkdir(parents=True, exist_ok=False)\n",
        "\n",
        "        if os.path.exists(self.get_conf_file_name()):\n",
        "            self.load_existing()\n",
        "        else:\n",
        "            self.write_conf()\n",
        "\n",
        "    def load_existing(self):\n",
        "        with open(self.get_conf_file_name(), \"r\") as f:\n",
        "            d = json.load(f)\n",
        "            self.length = d['length']\n",
        "            self.length = self.length if self.length > 0 else 0\n",
        "            if self.length > 0:\n",
        "                ind_start = self.get_slice_start_for_index(self.length - 1)\n",
        "                self.load_slice(ind_start)\n",
        "                self.current_slice_start = ind_start\n",
        "\n",
        "    def finalize(self):\n",
        "        with self.lock:\n",
        "            self.write_conf()\n",
        "\n",
        "    def get_current_slice_max(self):\n",
        "        # Return the index of the last element that can\n",
        "        # be stored in the current slice\n",
        "        with self.lock:\n",
        "            if self.current_slice_start == -1:\n",
        "                return -1\n",
        "            else:\n",
        "                return self.current_slice_start + self.chunk_size - 1\n",
        "\n",
        "    def get_current_slice_end(self):\n",
        "        # Return the end of the current slice\n",
        "        with self.lock:\n",
        "            if self.current_slice_start == -1:\n",
        "                return -1\n",
        "            else:\n",
        "                return self.current_slice_start + len(self.current_slice)\n",
        "\n",
        "    def get_current_slice_size(self):\n",
        "        # return the size of the current slice\n",
        "        with self.lock:\n",
        "            return len(self.current_slice)\n",
        "\n",
        "    def get_current_slice_offsets(self):\n",
        "        with self.lock:\n",
        "            return self.current_slice_start, \\\n",
        "                self.get_current_slice_end(), \\\n",
        "                self.get_current_slice_max()\n",
        "\n",
        "    def get_current_file_name(self):\n",
        "        with self.lock:\n",
        "            if -1 == self.current_slice_start:\n",
        "                return None\n",
        "            st = self.current_slice_start\n",
        "            en = st + self.chunk_size - 1\n",
        "            return self.get_pickle_file_name(st, en)\n",
        "\n",
        "    def write_chunk(self):\n",
        "        with self.lock:\n",
        "            with open(self.get_current_file_name(), \"wb\") as f:\n",
        "                pickle.dump(self.current_slice, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "    def write_conf(self):\n",
        "        with self.lock:\n",
        "            thedict = {}\n",
        "            thedict[\"length\"] = self.length\n",
        "            #print(\"Exists?\", os.path.exists(self.directory))\n",
        "            with open(self.get_conf_file_name(), \"w\") as f:\n",
        "                json.dump(thedict, f)\n",
        "\n",
        "    def append(self, x):\n",
        "        with self.lock:\n",
        "            if 0 == len(self.current_slice) % self.chunk_size:\n",
        "                self.current_slice = list()\n",
        "                if self.current_slice_start != -1:\n",
        "                    self.current_slice_start += self.chunk_size\n",
        "            if self.current_slice_start == -1:\n",
        "                self.current_slice_start = 0\n",
        "            self.length += 1\n",
        "            self.current_slice.append(x)\n",
        "            self.write_chunk()\n",
        "            self.write_conf()\n",
        "        pass\n",
        "\n",
        "    def __del__(self):\n",
        "        with self.lock:\n",
        "            try:\n",
        "                # Directory may have been removed by now\n",
        "                # because del might be called quite late\n",
        "                if os.path.exists(self.directory):\n",
        "                    self.write_conf()\n",
        "            except Exception as e:\n",
        "                traceback.print_tb(e.__traceback__, file=sys.stderr)\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        with self.lock:\n",
        "            try:\n",
        "                # print(self.length, self.directory)\n",
        "                if self.reader is None or self.reader.length <= ind:\n",
        "                    self.reader = ChunkDataReader(self, lock=self.lock)\n",
        "            except Exception as e:\n",
        "                # print(self.length, self.directory)\n",
        "                traceback.print_tb(e.__traceback__, file=sys.stderr)\n",
        "                raise IndexError\n",
        "            return self.reader[ind]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzaXb5IjI5sB",
        "outputId": "19778b8e-c885-4f31-f348-7ee46f474605"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /tmp/ChunkData.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# util.py"
      ],
      "metadata": {
        "id": "t444EYHjrbrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /tmp/util.py\n",
        "\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "import logging\n",
        "import pickle\n",
        "import string\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import Counter, OrderedDict, defaultdict as ddict\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "from ChunkData import *\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def load_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        obj = pickle.load(f)\n",
        "    return obj\n",
        "\n",
        "def save_pickle(obj, path):\n",
        "    \"\"\"\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "    \"\"\"\n",
        "    return\n",
        "\n",
        "def visualize(tbx, pred_dict, gold_dict, step, split, num_visuals):\n",
        "    \"\"\"Visualize text examples to TensorBoard.\n",
        "\n",
        "    Args:\n",
        "        tbx (tensorboardX.SummaryWriter): Summary writer.\n",
        "        pred_dict (dict): dict of predictions of the form id -> pred.\n",
        "        step (int): Number of examples seen so far during training.\n",
        "        split (str): Name of data split being visualized.\n",
        "        num_visuals (int): Number of visuals to select at random from preds.\n",
        "    \"\"\"\n",
        "    if num_visuals <= 0:\n",
        "        return\n",
        "    if num_visuals > len(pred_dict):\n",
        "        num_visuals = len(pred_dict)\n",
        "    id2index = {curr_id : idx for idx, curr_id in enumerate(gold_dict['id'])}\n",
        "    visual_ids = np.random.choice(list(pred_dict), size=num_visuals, replace=False)\n",
        "    for i, id_ in enumerate(visual_ids):\n",
        "        pred = pred_dict[id_] or 'N/A'\n",
        "        idx_gold_dict = id2index[id_]\n",
        "        question = gold_dict['question'][idx_gold_dict]\n",
        "        context = gold_dict['context'][idx_gold_dict]\n",
        "        answers = gold_dict['answer'][idx_gold_dict]\n",
        "        gold = answers['text'][0] if answers else 'N/A'\n",
        "        tbl_fmt = (f'- **Question:** {question}\\n'\n",
        "                   + f'- **Context:** {context}\\n'\n",
        "                   + f'- **Answer:** {gold}\\n'\n",
        "                   + f'- **Prediction:** {pred}')\n",
        "        tbx.add_text(tag=f'{split}/{i+1}_of_{num_visuals}',\n",
        "                     text_string=tbl_fmt,\n",
        "                     global_step=step)\n",
        "\n",
        "\n",
        "def get_save_dir(base_dir, name, id_max=100):\n",
        "    for uid in range(1, id_max):\n",
        "        save_dir = os.path.join(base_dir, f'{name}-{uid:02d}')\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "            return save_dir\n",
        "\n",
        "    raise RuntimeError('Too many save directories created with the same name. \\\n",
        "                       Delete old save directories or use another name.')\n",
        "\n",
        "\n",
        "def filter_encodings(encodings):\n",
        "    filter_idx = [idx for idx, val in enumerate(encodings['end_positions'])\n",
        "                 if not val]\n",
        "    filter_idx = set(filter_idx)\n",
        "    encodings_filtered = {key : [] for key in encodings}\n",
        "    sz = len(encodings['input_ids'])\n",
        "    for idx in range(sz):\n",
        "        if idx not in filter_idx:\n",
        "            for key in encodings:\n",
        "                encodings_filtered[key].append(encodings[key][idx])\n",
        "    return encodings_filtered\n",
        "\n",
        "def merge(encodings, new_encoding):\n",
        "    if not encodings:\n",
        "        return new_encoding\n",
        "    else:\n",
        "        for key in new_encoding:\n",
        "            encodings[key] += new_encoding[key]\n",
        "        return encodings\n",
        "\n",
        "def get_logger(log_dir, name):\n",
        "    \"\"\"Get a `logging.Logger` instance that prints to the console\n",
        "    and an auxiliary file.\n",
        "\n",
        "    Args:\n",
        "        log_dir (str): Directory in which to create the log file.\n",
        "        name (str): Name to identify the logs.\n",
        "\n",
        "    Returns:\n",
        "        logger (logging.Logger): Logger instance for logging events.\n",
        "    \"\"\"\n",
        "    class StreamHandlerWithTQDM(logging.Handler):\n",
        "        \"\"\"Let `logging` print without breaking `tqdm` progress bars.\n",
        "\n",
        "        See Also:\n",
        "            > https://stackoverflow.com/questions/38543506\n",
        "        \"\"\"\n",
        "        def emit(self, record):\n",
        "            try:\n",
        "                msg = self.format(record)\n",
        "                tqdm.write(msg)\n",
        "                self.flush()\n",
        "            except (KeyboardInterrupt, SystemExit):\n",
        "                raise\n",
        "            except:\n",
        "                self.handleError(record)\n",
        "\n",
        "    # Create logger\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "\n",
        "    # Log everything (i.e., DEBUG level and above) to a file\n",
        "    log_path = os.path.join(log_dir, f'{name}.txt')\n",
        "    file_handler = logging.FileHandler(log_path)\n",
        "    file_handler.setLevel(logging.DEBUG)\n",
        "\n",
        "    # Log everything except DEBUG level (i.e., INFO level and above) to console\n",
        "    console_handler = StreamHandlerWithTQDM()\n",
        "    console_handler.setLevel(logging.INFO)\n",
        "\n",
        "    # Create format for the logs\n",
        "    file_formatter = logging.Formatter('[%(asctime)s] %(message)s',\n",
        "                                       datefmt='%m.%d.%y %H:%M:%S')\n",
        "    file_handler.setFormatter(file_formatter)\n",
        "    console_formatter = logging.Formatter('[%(asctime)s] %(message)s',\n",
        "                                          datefmt='%m.%d.%y %H:%M:%S')\n",
        "    console_handler.setFormatter(console_formatter)\n",
        "\n",
        "    # add the handlers to the logger\n",
        "    logger.addHandler(file_handler)\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"Keep track of average values over time.\n",
        "\n",
        "    Adapted from:\n",
        "        > https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset meter.\"\"\"\n",
        "        self.__init__()\n",
        "\n",
        "    def update(self, val, num_samples=1):\n",
        "        \"\"\"Update meter with new value `val`, the average of `num` samples.\n",
        "\n",
        "        Args:\n",
        "            val (float): Average value to update the meter with.\n",
        "            num_samples (int): Number of samples that were averaged to\n",
        "                produce `val`.\n",
        "        \"\"\"\n",
        "        self.count += num_samples\n",
        "        self.sum += val * num_samples\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, encodings, train=True):\n",
        "        self.encodings = encodings\n",
        "        self.keys = ['input_ids', 'attention_mask']\n",
        "        if train:\n",
        "            self.keys += ['start_positions', 'end_positions']\n",
        "        assert(all(key in self.encodings for key in self.keys))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key : torch.tensor(self.encodings[key][idx]) for key in self.keys}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "def read_squad(path):\n",
        "    path = Path(path)\n",
        "    with open(path, 'rb') as f:\n",
        "        squad_dict = json.load(f)\n",
        "    data_dict = {'question': [], 'context': [], 'id': [], 'answer': []}\n",
        "    for group in squad_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                if len(qa['answers']) == 0:\n",
        "                    data_dict['question'].append(question)\n",
        "                    data_dict['context'].append(context)\n",
        "                    data_dict['id'].append(qa['id'])\n",
        "                else:\n",
        "                    for answer in  qa['answers']:\n",
        "                        data_dict['question'].append(question)\n",
        "                        data_dict['context'].append(context)\n",
        "                        data_dict['id'].append(qa['id'])\n",
        "                        data_dict['answer'].append(answer)\n",
        "    id_map = ddict(list)\n",
        "    for idx, qid in enumerate(data_dict['id']):\n",
        "        id_map[qid].append(idx)\n",
        "\n",
        "    data_dict_collapsed = {'question': [], 'context': [], 'id': []}\n",
        "    if data_dict['answer']:\n",
        "        data_dict_collapsed['answer'] = []\n",
        "    for qid in id_map:\n",
        "        ex_ids = id_map[qid]\n",
        "        data_dict_collapsed['question'].append(data_dict['question'][ex_ids[0]])\n",
        "        data_dict_collapsed['context'].append(data_dict['context'][ex_ids[0]])\n",
        "        data_dict_collapsed['id'].append(qid)\n",
        "        if data_dict['answer']:\n",
        "            all_answers = [data_dict['answer'][idx] for idx in ex_ids]\n",
        "            data_dict_collapsed['answer'].append({'answer_start': [answer['answer_start'] for answer in all_answers],\n",
        "                                                  'text': [answer['text'] for answer in all_answers]})\n",
        "    return data_dict_collapsed\n",
        "\n",
        "def add_token_positions(encodings, answers, tokenizer):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
        "\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "        # if end position is None, the 'char_to_token' function points to the space before the correct token - > add + 1\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] + 1)\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "\n",
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        gold_text = answer['text']\n",
        "        start_idx = answer['answer_start']\n",
        "        end_idx = start_idx + len(gold_text)\n",
        "\n",
        "        # sometimes squad answers are off by a character or two – fix this\n",
        "        if context[start_idx:end_idx] == gold_text:\n",
        "            answer['answer_end'] = end_idx\n",
        "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 1\n",
        "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
        "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 2\n",
        "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
        "\n",
        "def convert_tokens(eval_dict, qa_id, y_start_list, y_end_list):\n",
        "    \"\"\"Convert predictions to tokens from the context.\n",
        "\n",
        "    Args:\n",
        "        eval_dict (dict): Dictionary with eval info for the dataset. This is\n",
        "            used to perform the mapping from IDs and indices to actual text.\n",
        "        qa_id (int): List of QA example IDs.\n",
        "        y_start_list (list): List of start predictions.\n",
        "        y_end_list (list): List of end predictions.\n",
        "        no_answer (bool): Questions can have no answer. E.g., SQuAD 2.0.\n",
        "\n",
        "    Returns:\n",
        "        pred_dict (dict): Dictionary index IDs -> predicted answer text.\n",
        "        sub_dict (dict): Dictionary UUIDs -> predicted answer text (submission).\n",
        "    \"\"\"\n",
        "    pred_dict = {}\n",
        "    sub_dict = {}\n",
        "    for qid, y_start, y_end in zip(qa_id, y_start_list, y_end_list):\n",
        "        context = eval_dict[str(qid)][\"context\"]\n",
        "        spans = eval_dict[str(qid)][\"spans\"]\n",
        "        uuid = eval_dict[str(qid)][\"uuid\"]\n",
        "        start_idx = spans[y_start][0]\n",
        "        end_idx = spans[y_end][1]\n",
        "        pred_dict[str(qid)] = context[start_idx: end_idx]\n",
        "        sub_dict[uuid] = context[start_idx: end_idx]\n",
        "    return pred_dict, sub_dict\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    if not ground_truths:\n",
        "        return metric_fn(prediction, '')\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def eval_dicts(gold_dict, pred_dict):\n",
        "    avna = f1 = em = total = 0\n",
        "    id2index = {curr_id : idx for idx, curr_id in enumerate(gold_dict['id'])}\n",
        "    for curr_id in pred_dict:\n",
        "        total += 1\n",
        "        index = id2index[curr_id]\n",
        "        ground_truths = gold_dict['answer'][index]['text']\n",
        "        prediction = pred_dict[curr_id]\n",
        "        em += metric_max_over_ground_truths(compute_em, prediction, ground_truths)\n",
        "        f1 += metric_max_over_ground_truths(compute_f1, prediction, ground_truths)\n",
        "\n",
        "    eval_dict = {'EM': 100. * em / total,\n",
        "                 'F1': 100. * f1 / total}\n",
        "    return eval_dict\n",
        "\n",
        "def postprocess_qa_predictions(examples, features, predictions,\n",
        "                               n_best_size=20, max_answer_length=30):\n",
        "    all_start_logits, all_end_logits = predictions\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = ddict(list)\n",
        "    for i, feat_id in enumerate(features['id']):\n",
        "        features_per_example[example_id_to_index[feat_id]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    all_predictions = OrderedDict()\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    #for example_index in tqdm(range(len(examples['id']))):\n",
        "    for example_index in range(len(examples['id'])):\n",
        "        example = {key : examples[key][example_index] for key in examples}\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "        prelim_predictions = []\n",
        "\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            seq_ids = features.sequence_ids(feature_index)\n",
        "            non_pad_idx = len(seq_ids) - 1\n",
        "            while not seq_ids[non_pad_idx]:\n",
        "                non_pad_idx -= 1\n",
        "            start_logits = start_logits[:non_pad_idx]\n",
        "            end_logits = end_logits[:non_pad_idx]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[\"offset_mapping\"][feature_index]\n",
        "            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n",
        "            # available in the current feature.\n",
        "            token_is_max_context = features.get(\"token_is_max_context\", None)\n",
        "            if token_is_max_context:\n",
        "                token_is_max_context = token_is_max_context[feature_index]\n",
        "\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either = 0 or > max_answer_length.\n",
        "                    if end_index <= start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "                    # Don't consider answer that don't have the maximum context available (if such information is\n",
        "                    # provided).\n",
        "                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n",
        "                        continue\n",
        "                    prelim_predictions.append(\n",
        "                        {\n",
        "                            \"start_index\": start_index,\n",
        "                            \"end_index\": end_index,\n",
        "                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"start_logit\": start_logits[start_index],\n",
        "                            \"end_logit\": end_logits[end_index],\n",
        "                        }\n",
        "                    )\n",
        "        # Only keep the best `n_best_size` predictions.\n",
        "        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
        "\n",
        "        # Use the offsets to gather the answer text in the original context.\n",
        "        context = example[\"context\"]\n",
        "        for pred in predictions:\n",
        "            offsets = pred['offsets']\n",
        "            pred[\"text\"] = context[offsets[0] : offsets[1]]\n",
        "\n",
        "        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "        # failure.\n",
        "        if len(predictions) == 0:\n",
        "            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n",
        "\n",
        "        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n",
        "        # the LogSumExp trick).\n",
        "        scores = np.array([pred.pop(\"score\") for pred in predictions])\n",
        "        exp_scores = np.exp(scores - np.max(scores))\n",
        "        probs = exp_scores / exp_scores.sum()\n",
        "\n",
        "        # Include the probabilities in our predictions.\n",
        "        for prob, pred in zip(probs, predictions):\n",
        "            pred[\"probability\"] = prob\n",
        "\n",
        "        # need to find the best non-empty prediction.\n",
        "        i = 0\n",
        "        while i < len(predictions):\n",
        "            if predictions[i]['text'] != '':\n",
        "                break\n",
        "            i += 1\n",
        "        if i == len(predictions):\n",
        "            import pdb; pdb.set_trace();\n",
        "\n",
        "        best_non_null_pred = predictions[i]\n",
        "        all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "\n",
        "\n",
        "# All methods below this line are from the official SQuAD 2.0 eval script\n",
        "# https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Convert to lowercase and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "        return re.sub(regex, ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "    if not s:\n",
        "        return []\n",
        "    return normalize_answer(s).split()\n",
        "\n",
        "def compute_em(a_gold, a_pred):\n",
        "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = Counter(gold_toks) & Counter(pred_toks)\n",
        "    num_same = sum(common.values())\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "        return int(gold_toks == pred_toks)\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(pred_toks)\n",
        "    recall = 1.0 * num_same / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn72CZjoq-zB",
        "outputId": "29342e78-81ce-4fa2-d9a4-15f788cb1762"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /tmp/util.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /tmp/ChunkData.py ."
      ],
      "metadata": {
        "id": "B-GBqTg-_dn8"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ChunkData import *\n",
        "from glob import glob\n",
        "\n",
        "[x.split('/')[-1] for x in glob(\"/tmp/train/*\")]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0yNExCK_RwW",
        "outputId": "14fa6000-13ab-44bd-d968-0e2d8042235f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['end_positions',\n",
              " 'overflow_to_sample_mapping',\n",
              " 'id',\n",
              " 'attention_mask',\n",
              " 'start_positions',\n",
              " 'input_ids',\n",
              " 'offset_mapping']"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train.py"
      ],
      "metadata": {
        "id": "nU1kuEaIq8O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /tmp/train.py\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import csv\n",
        "import util\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import DistilBertForQuestionAnswering\n",
        "from transformers import AdamW\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
        "from args import get_train_test_args\n",
        "\n",
        "from tqdm import tqdm\n",
        "from ChunkData import *\n",
        "\n",
        "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "def prepare_eval_data(dataset_dict, tokenizer):\n",
        "    tokenized_examples = tokenizer(dataset_dict['question'],\n",
        "                                   dataset_dict['context'],\n",
        "                                   truncation=\"only_second\",\n",
        "                                   stride=128,\n",
        "                                   max_length=384,\n",
        "                                   return_overflowing_tokens=True,\n",
        "                                   return_offsets_mapping=True,\n",
        "                                   padding='max_length')\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
        "    # corresponding example_id and we will store the offset mappings.\n",
        "    tokenized_examples[\"id\"] = []\n",
        "    for i in tqdm(range(len(tokenized_examples[\"input_ids\"]))):\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"id\"].append(dataset_dict[\"id\"][sample_index])\n",
        "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "        # position is part of the context or not.\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == 1 else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "def prepare_train_data(dataset_dict, tokenizer):\n",
        "    import gc\n",
        "    import shutil\n",
        "\n",
        "\n",
        "    tokenized_examples = tokenizer(dataset_dict['question'],\n",
        "                                   dataset_dict['context'],\n",
        "                                   truncation=\"only_second\",\n",
        "                                   stride=128,\n",
        "                                   max_length=384,\n",
        "                                   return_overflowing_tokens=True,\n",
        "                                   return_offsets_mapping=True,\n",
        "                                   padding='max_length')\n",
        "    orig_tokenized_examples = tokenized_examples\n",
        "    sample_mapping = orig_tokenized_examples[\"overflow_to_sample_mapping\"]\n",
        "    offset_mapping = orig_tokenized_examples[\"offset_mapping\"]\n",
        "\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "    tokenized_examples['id'] = []\n",
        "\n",
        "    temp_tokenized_examples = tokenized_examples\n",
        "\n",
        "    # TODO: RAJBIR: my additions, not sure if they work\n",
        "    tokenized_examples = {str(c): ChunkDataWriter(f\"/tmp/train/{str(c)}\") for c in temp_tokenized_examples.keys()}\n",
        "\n",
        "    for key in temp_tokenized_examples.keys():\n",
        "        key = str(key)\n",
        "        if key not in [\"start_positions\", \"end_positions\", \"id\", \"sequence_ids\", \"overflow_to_sample_mapping\", \"offset_mapping\"]:\n",
        "            v_from = temp_tokenized_examples[key]\n",
        "            v_to = tokenized_examples[key]\n",
        "            #print(f\"Copying {key}... {len(v_from)} {type(v_from)}\")\n",
        "            for i in range(len(v_from)):\n",
        "                v_to.append(v_from[i])\n",
        "                if key not in [\"offset_mapping\", \"overflow_mapping\"]:\n",
        "                    v_from[i] = None\n",
        "                    gc.collect(0)\n",
        "                    gc.collect(1)\n",
        "                    gc.collect(2)\n",
        "            temp_tokenized_examples[key] = None\n",
        "            gc.collect(0)\n",
        "            gc.collect(1)\n",
        "            gc.collect(2)\n",
        "            # print(\"done\")\n",
        "            v_to.finalize()\n",
        "\n",
        "    inaccurate = 0\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = orig_tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answer = dataset_dict['answer'][sample_index]\n",
        "        # Start/end character index of the answer in the text.\n",
        "        start_char = answer['answer_start'][0]\n",
        "        end_char = start_char + len(answer['text'][0])\n",
        "        tokenized_examples['id'].append(dataset_dict['id'][sample_index])\n",
        "        # Start token index of the current span in the text.\n",
        "        token_start_index = 0\n",
        "        while sequence_ids[token_start_index] != 1:\n",
        "            token_start_index += 1\n",
        "\n",
        "        # End token index of the current span in the text.\n",
        "        token_end_index = len(input_ids) - 1\n",
        "        while sequence_ids[token_end_index] != 1:\n",
        "            token_end_index -= 1\n",
        "\n",
        "        # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "            # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                token_start_index += 1\n",
        "            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "            while offsets[token_end_index][1] >= end_char:\n",
        "                token_end_index -= 1\n",
        "            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "            # assertion to check if this checks out\n",
        "            context = dataset_dict['context'][sample_index]\n",
        "            offset_st = offsets[tokenized_examples['start_positions'][-1]][0]\n",
        "            offset_en = offsets[tokenized_examples['end_positions'][-1]][1]\n",
        "            if context[offset_st : offset_en] != answer['text'][0]:\n",
        "                inaccurate += 1\n",
        "\n",
        "    total = len(tokenized_examples['id'])\n",
        "    # print(f\"Preprocessing not completely accurate for {inaccurate}/{total} instances\")\n",
        "    # print([key for key in tokenized_examples.keys()])\n",
        "    # TODO: RAJBIR: my additions, not sure if they work\n",
        "    tokenized_examples = {k: ChunkDataReader(v) for k, v in tokenized_examples.items()}\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "\n",
        "def get_cached_dataset():\n",
        "    import os\n",
        "    import glob\n",
        "    if os.path.exists(\"/tmp/train/input_ids/config.json\"):\n",
        "        g = glob.glob(\"/tmp/train/*\")\n",
        "        g = [x.split('/')[-1] for x in g]\n",
        "        out_dict = {}\n",
        "        for name in g:\n",
        "            val = ChunkDataReader(f\"/tmp/train/{name}\", chunk_size=16)\n",
        "            out_dict[name] = val\n",
        "        return out_dict\n",
        "    return None\n",
        "\n",
        "def read_and_process(args, tokenizer, dataset_dict, dir_name, dataset_name, split):\n",
        "    #TODO: cache this if possible\n",
        "    def get_dataset_slice(dataset_dict, slice_start, slice_length):\n",
        "        out_dict = {k: [] for k in dataset_dict.keys()}\n",
        "        for k in dataset_dict.keys():\n",
        "            arr = dataset_dict[k]\n",
        "            out_dict[k] = arr[slice_start:slice_start+slice_length]\n",
        "        return out_dict\n",
        "\n",
        "    def get_dataset_length(dataset_dict):\n",
        "        column = [c for c in dataset_dict.keys()][0]\n",
        "        return len(dataset_dict[column])\n",
        "\n",
        "    for key in dataset_dict.keys():\n",
        "        dataset_dict[key] = dataset_dict[key]\n",
        "    if split=='train':\n",
        "        import pickle\n",
        "        with open(\"/tmp/augmented_data.pickle\", \"rb\") as f:\n",
        "            dataset_dict = pickle.load(f)\n",
        "        stride = 512\n",
        "        for i in tqdm(range(150 * stride, get_dataset_length(dataset_dict), stride), \"Preparing training data\"):\n",
        "\n",
        "            # Some items are causing crashes in the libraries, skipping them\n",
        "            skip_iteration = False\n",
        "            for j in range(i, i + stride):\n",
        "                if j in [77824, 78336, 83456]:\n",
        "                    skip_iteration = True\n",
        "            if skip_iteration:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                print(f\"Preprocessing at sequence {i}\")\n",
        "                temp_dict = get_dataset_slice(dataset_dict, i, stride)\n",
        "                tokenized_examples = prepare_train_data(temp_dict, tokenizer)\n",
        "                cleardir = False\n",
        "            except:\n",
        "                print(\"Error in translating sequence starting at index \", i)\n",
        "    else:\n",
        "        assert(False)\n",
        "        # Should not reach here\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "#TODO: use a logger, use tensorboard\n",
        "class Trainer():\n",
        "    def __init__(self, args, log):\n",
        "        self.lr = args.lr\n",
        "        self.num_epochs = args.num_epochs\n",
        "        self.device = args.device\n",
        "        self.eval_every = args.eval_every\n",
        "        self.path = os.path.join(args.save_dir, 'checkpoint')\n",
        "        self.num_visuals = args.num_visuals\n",
        "        self.save_dir = args.save_dir\n",
        "        self.log = log\n",
        "        self.visualize_predictions = args.visualize_predictions\n",
        "        if not os.path.exists(self.path):\n",
        "            os.makedirs(self.path)\n",
        "\n",
        "    def save(self, model):\n",
        "        model.save_pretrained(self.path)\n",
        "\n",
        "    def evaluate(self, model, data_loader, data_dict, return_preds=False, split='validation'):\n",
        "        device = self.device\n",
        "\n",
        "        # print(\"\\nEvaluating\")\n",
        "        model.eval()\n",
        "        pred_dict = {}\n",
        "        all_start_logits = []\n",
        "        all_end_logits = []\n",
        "        with torch.no_grad(): #, \\\n",
        "                #tqdm(total=len(data_loader.dataset)) as progress_bar:\n",
        "            for batch in data_loader:\n",
        "                # Setup for forward\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                batch_size = len(input_ids)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                # Forward\n",
        "                start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
        "                # TODO: compute loss\n",
        "\n",
        "                all_start_logits.append(start_logits)\n",
        "                all_end_logits.append(end_logits)\n",
        "                #progress_bar.update(batch_size)\n",
        "\n",
        "        # Get F1 and EM scores\n",
        "        start_logits = torch.cat(all_start_logits).cpu().numpy()\n",
        "        end_logits = torch.cat(all_end_logits).cpu().numpy()\n",
        "        preds = util.postprocess_qa_predictions(data_dict,\n",
        "                                                 data_loader.dataset.encodings,\n",
        "                                                 (start_logits, end_logits))\n",
        "        if split == 'validation':\n",
        "            results = util.eval_dicts(data_dict, preds)\n",
        "            results_list = [('F1', results['F1']),\n",
        "                            ('EM', results['EM'])]\n",
        "        else:\n",
        "            results_list = [('F1', -1.0),\n",
        "                            ('EM', -1.0)]\n",
        "        results = OrderedDict(results_list)\n",
        "        if return_preds:\n",
        "            return preds, results\n",
        "        return results\n",
        "\n",
        "    def train(self, model, train_dataloader, eval_dataloader, val_dict):\n",
        "        device = self.device\n",
        "        model.to(device)\n",
        "        optim = AdamW(model.parameters(), lr=self.lr)\n",
        "        global_idx = 0\n",
        "        best_scores = {'F1': -1.0, 'EM': -1.0}\n",
        "        tbx = SummaryWriter(self.save_dir)\n",
        "\n",
        "        for epoch_num in range(self.num_epochs):\n",
        "            self.log.info(f'Epoch: {epoch_num}')\n",
        "            with torch.enable_grad(), tqdm(total=len(train_dataloader.dataset)) as progress_bar:\n",
        "                for batch in train_dataloader:\n",
        "                    optim.zero_grad()\n",
        "                    model.train()\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    start_positions = batch['start_positions'].to(device)\n",
        "                    end_positions = batch['end_positions'].to(device)\n",
        "                    outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                                    start_positions=start_positions,\n",
        "                                    end_positions=end_positions)\n",
        "                    loss = outputs[0]\n",
        "                    loss.backward()\n",
        "                    optim.step()\n",
        "                    progress_bar.update(len(input_ids))\n",
        "                    progress_bar.set_postfix(epoch=epoch_num, NLL=loss.item())\n",
        "                    tbx.add_scalar('train/NLL', loss.item(), global_idx)\n",
        "                    if (global_idx % self.eval_every) == 0:\n",
        "                        #self.log.info(f'Evaluating at step {global_idx}...')\n",
        "                        preds, curr_score = self.evaluate(model, eval_dataloader, val_dict, return_preds=True)\n",
        "                        results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in curr_score.items())\n",
        "                        #self.log.info('Visualizing in TensorBoard...')\n",
        "                        for k, v in curr_score.items():\n",
        "                            tbx.add_scalar(f'val/{k}', v, global_idx)\n",
        "                        self.log.info(f'Eval {results_str}')\n",
        "                        if self.visualize_predictions:\n",
        "                            util.visualize(tbx,\n",
        "                                           pred_dict=preds,\n",
        "                                           gold_dict=val_dict,\n",
        "                                           step=global_idx,\n",
        "                                           split='val',\n",
        "                                           num_visuals=self.num_visuals)\n",
        "                        if curr_score['F1'] >= best_scores['F1']:\n",
        "                            best_scores = curr_score\n",
        "                            self.save(model)\n",
        "                    global_idx += 1\n",
        "        preds, curr_score = self.evaluate(model, eval_dataloader, val_dict, return_preds=True)\n",
        "        results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in curr_score.items())\n",
        "        self.log.info(f'Final eval results after epoch: {results_str}')\n",
        "        self.log.info(f'After epoch, best scores so far: {best_scores}')\n",
        "        return best_scores\n",
        "\n",
        "def get_dataset(args, datasets, data_dir, tokenizer, split_name):\n",
        "    datasets = datasets.split(',')\n",
        "    dataset_dict = None\n",
        "    dataset_name=''\n",
        "    for dataset in datasets:\n",
        "        dataset_name += f'_{dataset}'\n",
        "        #dataset_dict_curr = util.read_squad(f'{data_dir}/{dataset}')\n",
        "        #dataset_dict = util.merge(dataset_dict, dataset_dict_curr)\n",
        "    with open (\"/tmp/augmented_data.pickle\", \"rb\") as f:\n",
        "        dataset_dict = pickle.load(f)\n",
        "    data_encodings = read_and_process(args, tokenizer, dataset_dict, data_dir, dataset_name, split_name)\n",
        "\n",
        "#import torch_xla.core.xla_model as xm\n",
        "def main():\n",
        "    # define parser and arguments\n",
        "    args = get_train_test_args()\n",
        "\n",
        "    util.set_seed(args.seed)\n",
        "    model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    if args.do_train:\n",
        "        if not os.path.exists(args.save_dir):\n",
        "            os.makedirs(args.save_dir)\n",
        "        args.save_dir = util.get_save_dir(args.save_dir, args.run_name)\n",
        "        log = util.get_logger(args.save_dir, 'log_train')\n",
        "        log.info(f'Args: {json.dumps(vars(args), indent=4, sort_keys=True)}')\n",
        "        log.info(\"Preparing Training Data...\")\n",
        "        args.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        if torch.cuda.is_available():\n",
        "            print(\"USING CUDA !!!!\")\n",
        "        else:\n",
        "            #print(xm.xla_device())\n",
        "            #if xm.xla_device() != None:\n",
        "            if False:\n",
        "                args.device = xm.xla_device()\n",
        "                print(\"USING TPU!!! \")\n",
        "            else:\n",
        "                printt(\"USING CPU !!!\")\n",
        "        get_dataset(args, args.train_datasets, args.train_dir, tokenizer, 'train')\n",
        "\n",
        "        \n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVWkdkWYHgC0",
        "outputId": "76503a60-64bb-4f7a-b4dd-40aa9f595d79"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /tmp/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /tmp/train -name \"*.json\" -exec cat {} \\; -exec echo \\;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25r0KO1_fBk9",
        "outputId": "c26e7107-3651-4e9e-c88e-9f36c2c547e8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"length\": 242304}\n",
            "{\"length\": 0}\n",
            "{\"length\": 242304}\n",
            "{\"length\": 242304}\n",
            "{\"length\": 242304}\n",
            "{\"length\": 242304}\n",
            "{\"length\": 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run.sh"
      ],
      "metadata": {
        "id": "STjJF5WnEmq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run.sh\n",
        "#!/bin/bash\n",
        "\n",
        "# Modify this if python version is different\n",
        "export PATH=$PATH:/usr/lib/python3.7/venv/scripts/common/:/usr/lib/python3.6/venv/scripts/common/activate\n",
        "cd mtu-nlp-assignment/assignment2/\n",
        "rm -rf robustqa\n",
        "unzip -o robustqa.zip\n",
        "cd robustqa\n",
        "mv datasets_50k.tar.gz  datasets_50k.tar\n",
        "tar -xf datasets_50k.tar\n",
        "source activate robustqa\n",
        "cp -f /tmp/args.py ./args.py\n",
        "cp -f /tmp/train.py ./train.py\n",
        "cp -f /tmp/util.py ./util.py\n",
        "cp -f /tmp/ChunkData.py ./ChunkData.py\n",
        "grep batch.size args.py\n",
        "python train.py --do-train --run-name baseline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZQpzgaT95wt",
        "outputId": "489d4fa7-f281-4ee1-da57-46b880748e08"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the model"
      ],
      "metadata": {
        "id": "qvxLkNEWEqsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./run.sh "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7BYx8jAEQCj",
        "outputId": "b6150c43-c63d-4add-ec21-6bb7ec3d31a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  robustqa.zip\n",
            "   creating: robustqa/\n",
            "  inflating: robustqa/datasets_50k.tar.gz  \n",
            "  inflating: __MACOSX/robustqa/._datasets_50k.tar.gz  \n",
            "  inflating: robustqa/convert_to_squad.py  \n",
            "  inflating: robustqa/environment.yml  \n",
            "  inflating: robustqa/util.py        \n",
            "  inflating: robustqa/README.md      \n",
            "  inflating: __MACOSX/robustqa/._README.md  \n",
            "  inflating: robustqa/train.py       \n",
            "  inflating: robustqa/args.py        \n",
            "    parser.add_argument('--batch-size', type=int, default=32)\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[12.27.21 12:07:56] Args: {\n",
            "    \"batch_size\": 32,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race,relation_extraction,duorc\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 2500,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": false,\n",
            "    \"run_name\": \"baseline\",\n",
            "    \"save_dir\": \"save/baseline-01\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad,nat_questions,newsqa\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[12.27.21 12:07:56] Preparing Training Data...\n",
            "USING CUDA !!!!\n",
            "Preparing training data:   0% 0/143 [00:00<?, ?it/s]Preprocessing at sequence 76800\n",
            "Preparing training data:   1% 1/143 [05:16<12:28:41, 316.35s/it]Preprocessing at sequence 77312\n",
            "Preparing training data:   1% 2/143 [10:23<12:16:51, 313.55s/it]Preprocessing at sequence 78848\n",
            "Preparing training data:   3% 5/143 [15:20<9:33:13, 249.23s/it] Preprocessing at sequence 79360\n",
            "Preparing training data:   4% 6/143 [20:34<10:13:16, 268.59s/it]Preprocessing at sequence 79872\n",
            "Preparing training data:   5% 7/143 [25:34<10:30:20, 278.09s/it]Preprocessing at sequence 80384\n",
            "Preparing training data:   6% 8/143 [30:35<10:41:10, 284.96s/it]Preprocessing at sequence 80896\n",
            "Preparing training data:   6% 9/143 [35:39<10:49:08, 290.66s/it]Preprocessing at sequence 81408\n",
            "Preparing training data:   7% 10/143 [40:51<10:58:13, 296.95s/it]Preprocessing at sequence 81920\n",
            "Preparing training data:   8% 11/143 [46:19<11:13:45, 306.26s/it]Preprocessing at sequence 82432\n",
            "Preparing training data:   8% 12/143 [51:09<10:57:56, 301.35s/it]Preprocessing at sequence 82944\n",
            "Preparing training data:   9% 13/143 [56:16<10:56:55, 303.19s/it]Preprocessing at sequence 83968\n",
            "Preparing training data:  10% 15/143 [1:01:22<9:10:35, 258.09s/it]Preprocessing at sequence 84480\n",
            "Preparing training data:  11% 16/143 [1:06:16<9:28:54, 268.77s/it]Preprocessing at sequence 84992\n",
            "Preparing training data:  12% 17/143 [1:11:19<9:46:14, 279.16s/it]Preprocessing at sequence 85504\n",
            "Preparing training data:  13% 18/143 [1:16:54<10:16:41, 296.01s/it]Preprocessing at sequence 86016\n",
            "Preparing training data:  13% 19/143 [1:22:06<10:21:43, 300.84s/it]Preprocessing at sequence 86528\n",
            "Preparing training data:  14% 20/143 [1:27:19<10:23:48, 304.29s/it]Preprocessing at sequence 87040\n",
            "Preparing training data:  15% 21/143 [1:32:24<10:19:28, 304.66s/it]Preprocessing at sequence 87552\n",
            "Preparing training data:  15% 22/143 [1:37:23<10:10:42, 302.83s/it]Preprocessing at sequence 88064\n",
            "Preparing training data:  16% 23/143 [1:42:34<10:10:23, 305.19s/it]Preprocessing at sequence 88576\n",
            "Preparing training data:  17% 24/143 [1:47:29<9:59:36, 302.32s/it] Preprocessing at sequence 89088\n",
            "Preparing training data:  17% 25/143 [1:52:38<9:58:04, 304.10s/it]Preprocessing at sequence 89600\n",
            "Preparing training data:  18% 26/143 [1:57:36<9:49:38, 302.38s/it]Preprocessing at sequence 90112\n",
            "Preparing training data:  19% 27/143 [2:02:45<9:48:47, 304.55s/it]Preprocessing at sequence 90624\n",
            "Preparing training data:  20% 28/143 [2:08:15<9:58:16, 312.14s/it]Preprocessing at sequence 91136\n",
            "Preparing training data:  20% 29/143 [2:13:25<9:51:28, 311.30s/it]Preprocessing at sequence 91648\n",
            "Preparing training data:  21% 30/143 [2:18:39<9:47:55, 312.17s/it]Preprocessing at sequence 92160\n",
            "Preparing training data:  22% 31/143 [2:23:55<9:44:59, 313.39s/it]Preprocessing at sequence 92672\n",
            "Preparing training data:  22% 32/143 [2:29:12<9:41:34, 314.37s/it]Preprocessing at sequence 93184\n",
            "Preparing training data:  23% 33/143 [2:34:26<9:36:26, 314.42s/it]Preprocessing at sequence 93696\n",
            "Preparing training data:  24% 34/143 [2:39:08<9:13:28, 304.67s/it]Preprocessing at sequence 94208\n",
            "Preparing training data:  24% 35/143 [2:44:21<9:12:55, 307.18s/it]Preprocessing at sequence 94720\n",
            "Preparing training data:  25% 36/143 [2:49:36<9:11:56, 309.50s/it]Preprocessing at sequence 95232\n",
            "Preparing training data:  26% 37/143 [2:54:24<8:55:28, 303.10s/it]Preprocessing at sequence 95744\n",
            "Preparing training data:  27% 38/143 [2:59:32<8:52:34, 304.33s/it]Preprocessing at sequence 96256\n",
            "Preparing training data:  27% 39/143 [3:04:37<8:48:17, 304.78s/it]Preprocessing at sequence 96768\n",
            "Preparing training data:  28% 40/143 [3:09:39<8:41:43, 303.91s/it]Preprocessing at sequence 97280\n",
            "Preparing training data:  29% 41/143 [3:14:59<8:44:47, 308.70s/it]Preprocessing at sequence 97792\n",
            "Preparing training data:  29% 42/143 [3:20:03<8:37:12, 307.25s/it]Preprocessing at sequence 98304\n",
            "Preparing training data:  30% 43/143 [3:24:56<8:24:45, 302.85s/it]Preprocessing at sequence 98816\n",
            "Preparing training data:  31% 44/143 [3:30:45<8:42:49, 316.87s/it]Preprocessing at sequence 99328\n",
            "Preparing training data:  31% 45/143 [3:36:09<8:40:49, 318.87s/it]Preprocessing at sequence 99840\n",
            "Preparing training data:  32% 46/143 [3:43:51<9:44:49, 361.75s/it]Preprocessing at sequence 100352\n",
            "Preparing training data:  33% 47/143 [3:53:48<11:31:59, 432.49s/it]Preprocessing at sequence 100864\n",
            "Preparing training data:  34% 48/143 [4:03:42<12:41:35, 481.00s/it]Preprocessing at sequence 101376\n",
            "Preparing training data:  34% 49/143 [4:13:19<13:18:43, 509.83s/it]Preprocessing at sequence 101888\n",
            "Preparing training data:  35% 50/143 [4:22:34<13:31:05, 523.29s/it]Preprocessing at sequence 102400\n",
            "Preparing training data:  36% 51/143 [4:32:38<13:59:40, 547.61s/it]Preprocessing at sequence 102912\n",
            "Preparing training data:  36% 52/143 [4:42:10<14:01:24, 554.77s/it]Preprocessing at sequence 103424\n",
            "Preparing training data:  37% 53/143 [4:51:47<14:02:08, 561.43s/it]Preprocessing at sequence 103936\n",
            "Preparing training data:  38% 54/143 [5:00:45<13:42:15, 554.33s/it]Preprocessing at sequence 104448\n",
            "Preparing training data:  38% 55/143 [5:10:41<13:51:18, 566.80s/it]Preprocessing at sequence 104960\n",
            "Preparing training data:  39% 56/143 [5:20:43<13:57:28, 577.57s/it]Preprocessing at sequence 105472\n",
            "Preparing training data:  40% 57/143 [5:30:07<13:41:42, 573.28s/it]Preprocessing at sequence 105984\n",
            "Preparing training data:  41% 58/143 [5:38:54<13:12:29, 559.41s/it]Preprocessing at sequence 106496\n",
            "Preparing training data:  41% 59/143 [5:48:46<13:17:03, 569.33s/it]Preprocessing at sequence 107008\n",
            "Preparing training data:  42% 60/143 [5:58:51<13:22:31, 580.14s/it]Preprocessing at sequence 107520\n",
            "Preparing training data:  43% 61/143 [6:08:47<13:19:17, 584.85s/it]Preprocessing at sequence 108032\n",
            "Preparing training data:  43% 62/143 [6:18:59<13:20:17, 592.81s/it]Preprocessing at sequence 108544\n",
            "Preparing training data:  44% 63/143 [6:28:57<13:12:50, 594.63s/it]Preprocessing at sequence 109056\n",
            "Preparing training data:  45% 64/143 [6:38:19<12:49:48, 584.67s/it]Preprocessing at sequence 109568\n",
            "Preparing training data:  45% 65/143 [6:48:42<12:55:11, 596.31s/it]Preprocessing at sequence 110080\n",
            "Preparing training data:  46% 66/143 [6:58:13<12:35:31, 588.72s/it]Preprocessing at sequence 110592\n",
            "Preparing training data:  47% 67/143 [7:08:58<12:47:06, 605.61s/it]Preprocessing at sequence 111104\n",
            "Preparing training data:  48% 68/143 [7:18:21<12:20:58, 592.78s/it]Preprocessing at sequence 111616\n",
            "Preparing training data:  48% 69/143 [7:27:42<11:59:16, 583.20s/it]Preprocessing at sequence 112128\n",
            "Preparing training data:  49% 70/143 [7:38:10<12:05:58, 596.70s/it]Preprocessing at sequence 112640\n",
            "Preparing training data:  50% 71/143 [7:48:53<12:12:33, 610.46s/it]Preprocessing at sequence 113152\n",
            "Preparing training data:  50% 72/143 [7:58:45<11:55:55, 605.01s/it]Preprocessing at sequence 113664\n",
            "Preparing training data:  51% 73/143 [8:08:50<11:45:46, 604.95s/it]Preprocessing at sequence 114176\n",
            "Preparing training data:  52% 74/143 [8:19:13<11:41:49, 610.29s/it]Preprocessing at sequence 114688\n",
            "Preparing training data:  52% 75/143 [8:29:03<11:24:47, 604.23s/it]Preprocessing at sequence 115200\n",
            "Preparing training data:  53% 76/143 [8:38:58<11:11:37, 601.46s/it]Preprocessing at sequence 115712\n",
            "Preparing training data:  54% 77/143 [8:48:23<10:49:30, 590.46s/it]Preprocessing at sequence 116224\n",
            "Preparing training data:  55% 78/143 [8:58:26<10:43:51, 594.33s/it]Preprocessing at sequence 116736\n",
            "Preparing training data:  55% 79/143 [9:08:12<10:31:24, 591.94s/it]Preprocessing at sequence 117248\n",
            "Preparing training data:  56% 80/143 [9:17:59<10:19:59, 590.47s/it]Preprocessing at sequence 117760\n",
            "Preparing training data:  57% 81/143 [9:27:57<10:12:28, 592.72s/it]Preprocessing at sequence 118272\n",
            "Preparing training data:  57% 82/143 [9:37:22<9:54:00, 584.28s/it] Preprocessing at sequence 118784\n",
            "Preparing training data:  58% 83/143 [9:46:52<9:40:06, 580.11s/it]Preprocessing at sequence 119296\n",
            "Preparing training data:  59% 84/143 [9:56:52<9:36:12, 585.98s/it]Preprocessing at sequence 119808\n",
            "Preparing training data:  59% 85/143 [10:05:49<9:12:10, 571.22s/it]Preprocessing at sequence 120320\n",
            "Preparing training data:  60% 86/143 [10:14:52<8:54:41, 562.84s/it]Preprocessing at sequence 120832\n",
            "Preparing training data:  61% 87/143 [10:24:23<8:47:29, 565.17s/it]Preprocessing at sequence 121344\n",
            "Preparing training data:  62% 88/143 [10:33:43<8:36:52, 563.86s/it]Preprocessing at sequence 121856\n",
            "Preparing training data:  62% 89/143 [10:43:27<8:32:49, 569.81s/it]Preprocessing at sequence 122368\n",
            "Preparing training data:  63% 90/143 [10:53:48<8:36:55, 585.20s/it]Preprocessing at sequence 122880\n",
            "Preparing training data:  64% 91/143 [11:03:48<8:30:55, 589.52s/it]Preprocessing at sequence 123392\n",
            "Preparing training data:  64% 92/143 [11:13:34<8:20:09, 588.42s/it]Preprocessing at sequence 123904\n",
            "Preparing training data:  65% 93/143 [11:24:30<8:27:14, 608.69s/it]Preprocessing at sequence 124416\n",
            "Preparing training data:  66% 94/143 [11:34:55<8:21:02, 613.53s/it]Preprocessing at sequence 124928\n",
            "Preparing training data:  66% 95/143 [11:44:53<8:07:10, 608.97s/it]Preprocessing at sequence 125440\n",
            "Preparing training data:  67% 96/143 [11:54:21<7:47:29, 596.80s/it]Preprocessing at sequence 125952\n",
            "Preparing training data:  68% 97/143 [12:03:29<7:26:15, 582.08s/it]Preprocessing at sequence 126464\n",
            "Preparing training data:  69% 98/143 [12:13:26<7:19:49, 586.43s/it]Preprocessing at sequence 126976\n",
            "Preparing training data:  69% 99/143 [12:23:47<7:17:43, 596.90s/it]Preprocessing at sequence 127488\n",
            "Preparing training data:  70% 100/143 [12:34:44<7:20:42, 614.95s/it]Preprocessing at sequence 128000\n",
            "Preparing training data:  71% 101/143 [12:44:56<7:09:53, 614.13s/it]Preprocessing at sequence 128512\n",
            "Preparing training data:  71% 102/143 [12:55:08<6:59:15, 613.54s/it]Preprocessing at sequence 129024\n",
            "Preparing training data:  72% 103/143 [13:05:17<6:48:07, 612.18s/it]Preprocessing at sequence 129536\n",
            "Preparing training data:  73% 104/143 [13:15:22<6:36:32, 610.05s/it]Preprocessing at sequence 130048\n",
            "Preparing training data:  73% 105/143 [13:25:28<6:25:34, 608.79s/it]Preprocessing at sequence 130560\n",
            "Preparing training data:  74% 106/143 [13:35:46<6:17:01, 611.40s/it]Preprocessing at sequence 131072\n",
            "Preparing training data:  75% 107/143 [13:45:59<6:07:09, 611.94s/it]Preprocessing at sequence 131584\n",
            "Preparing training data:  76% 108/143 [13:55:25<5:48:54, 598.12s/it]Preprocessing at sequence 132096\n",
            "Preparing training data:  76% 109/143 [14:05:27<5:39:38, 599.36s/it]Preprocessing at sequence 132608\n",
            "Preparing training data:  77% 110/143 [14:15:12<5:27:18, 595.12s/it]Preprocessing at sequence 133120\n",
            "Preparing training data:  78% 111/143 [14:25:54<5:24:48, 609.00s/it]Preprocessing at sequence 133632\n",
            "Preparing training data:  78% 112/143 [14:35:55<5:13:29, 606.75s/it]Preprocessing at sequence 134144\n",
            "Preparing training data:  79% 113/143 [14:46:19<5:05:54, 611.81s/it]Preprocessing at sequence 134656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!find /tmp/train -name \"*.json\" -exec echo {} \\; -exec cat {} \\;  -exec echo \\; -exec echo \\;"
      ],
      "metadata": {
        "id": "8TaexXaugC19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save pre-processed dataset\n",
        "Save the tokenized and pre-processed dataset for future use"
      ],
      "metadata": {
        "id": "Mjcg_tNu8RHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile save_train.sh\n",
        "#!/bin/bash\n",
        "THEDATE=`date +%Y-%m-%d_%H-%M-%S`\n",
        "\n",
        "THEDATE=`date +%Y-%m-%d_%H-%M-%S`\n",
        "\n",
        "echo \"----------------------------------------------------------\"\n",
        "\n",
        "if [[  -f /tmp/train/input_ids/config.json ]]\n",
        "then\n",
        "    echo \"COPYING PREPROCESSED TRAINING DATA\"\n",
        "    echo \"----------------------------------\"\n",
        "    cd /tmp\n",
        "    tar -cf \"preprocessed-train-augmented-${THEDATE}.tar\" train\n",
        "    gzip \"preprocessed-train-augmented-${THEDATE}.tar\"\n",
        "    mkdir -p /content/gdrive/MyDrive/NLP/augmented\n",
        "    cp \"preprocessed-train-augmented-${THEDATE}.tar.gz\" \"/content/gdrive/MyDrive/NLP/augmented/preprocessed-train-augmented-${THEDATE}.tar.gz\"\n",
        "    du -sh \"/content/gdrive/MyDrive/NLP/augmented/preprocessed-train-augmented-${THEDATE}.tar.gz\"\n",
        "fi"
      ],
      "metadata": {
        "id": "CglTX0qbLJuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!/bin/bash save_train.sh"
      ],
      "metadata": {
        "id": "1QJ8T1yGbX23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find /tmp/train -name config.json -exec ls {} \\; -exec cat {} \\; -exec echo \\; -exec echo \\;\n"
      ],
      "metadata": {
        "id": "7y97rZCDwvcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xYbmQ11dxpxz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}