{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment2/notebooks/MTU_NLP_Assignment_2_QA_augmented_gradual_unfreezing_wip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKy9mwehY7Hl"
   },
   "source": [
    "# Try a couple of things\n",
    "# - Slanted triangle learning rate\n",
    "# - gradual unfreezing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3_7k2MLtA0Q9",
    "outputId": "1b10ae33-d14f-4a38-efd8-7221a6dee23e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan  1 12:51:47 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   40C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "daoDMfvJpfqa",
    "outputId": "305b5e02-6d7f-43cd-ff2d-0fa44a296d81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2nEpGzfbvwj-",
    "outputId": "dad6c26c-5855-43c5-f72f-6fca0e59f773"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 12.8 MB 7.8 MB/s \n",
      "\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
      "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
      "\u001b[K     |████████████████████████████████| 7.0 MB 6.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 10.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 69 kB 4.1 MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 7.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 36.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 895 kB 51.7 MB/s \n",
      "\u001b[K     |████████████████████████████████| 124 kB 9.2 MB/s \n",
      "\u001b[K     |████████████████████████████████| 6.5 MB 9.4 MB/s \n",
      "\u001b[K     |████████████████████████████████| 461 kB 48.4 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install torchvision==0.8.2  -q -q -q\n",
    "!pip install torchtext==0.8.1    -q -q -q\n",
    "!pip install torchaudio==0.7.2   -q -q -q\n",
    "!pip install torch==1.7.1        -q -q -q\n",
    "!pip install tqdm==4.49.0        -q -q -q\n",
    "!pip install transformers==4.2.2 -q -q -q\n",
    "!pip install tensorflow          -q -q -q\n",
    "!pip install tensorboard         -q -q -q\n",
    "!pip install tensorboardX        -q -q -q\n",
    "!pip install --upgrade virtualenv -q -q -q\n",
    "!pip install pytorch_warmup      -q -q -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B54kD1pBclhf",
    "outputId": "1fa63437-3c49-4b4c-c98e-5a20c8573803"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing copy_train.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile copy_train.sh\n",
    "#!/bin/bash\n",
    "cp /content/gdrive/MyDrive/NLP/NLP-Save-Train/train-augmented-final.tar.gz /tmp\n",
    "cd /tmp\n",
    "gunzip train-augmented-final.tar.gz\n",
    "tar -xf train-augmented-final.tar\n",
    "ls /tmp/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YrV4qjppc6CL",
    "outputId": "887be7a9-8f0b-478d-f709-1f9278f11e25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_mask\tid\t   offset_mapping\t       start_positions\n",
      "end_positions\tinput_ids  overflow_to_sample_mapping\n"
     ]
    }
   ],
   "source": [
    "!/bin/bash copy_train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JGz5aTv1eZ6o",
    "outputId": "99db45f9-1f24-45c4-bea3-5aade3dabea6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"length\": 488383}"
     ]
    }
   ],
   "source": [
    "!cat /tmp/train/input_ids/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOK9thHC0rC1",
    "outputId": "e95f501d-d765-4320-fcdd-001f41142320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'mtu-nlp-assignment'...\n",
      "remote: Repository not found.\n",
      "fatal: Authentication failed for 'https://:@github.com/bhattacharjee/mtu-nlp-assignment.git/'\n",
      "Cloning into 'mtu-nlp-assignment'...\n",
      "remote: Enumerating objects: 1433, done.\u001b[K\n",
      "remote: Counting objects: 100% (1433/1433), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1272/1272), done.\u001b[K\n",
      "remote: Total 1433 (delta 824), reused 423 (delta 157), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (1433/1433), 96.54 MiB | 32.20 MiB/s, done.\n",
      "Resolving deltas: 100% (824/824), done.\n"
     ]
    }
   ],
   "source": [
    "!USERNAME=bhattacharjee PASSWORD=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 git clone \"https://${USERNAME}:${PASSWORD}@github.com/bhattacharjee/mtu-nlp-assignment.git\"\n",
    "!git clone https://bhattacharjee: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@github.com/bhattacharjee/mtu-nlp-assignment.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "454WJr-Rq5rh"
   },
   "source": [
    "# args.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nr1LRFt1GAUJ",
    "outputId": "e76a4157-d512-4d04-ed6f-14c3545c49cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/args.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/args.py\n",
    "import argparse\n",
    "\n",
    "def get_train_test_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--batch-size', type=int, default=32)\n",
    "    parser.add_argument('--num-epochs', type=int, default=4)\n",
    "    parser.add_argument('--lr', type=float, default=3e-5)\n",
    "    parser.add_argument('--num-visuals', type=int, default=10)\n",
    "    parser.add_argument('--seed', type=int, default=42)\n",
    "    parser.add_argument('--save-dir', type=str, default='save/')\n",
    "    parser.add_argument('--train', action='store_true')\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--train-datasets', type=str, default='squad,nat_questions,newsqa')\n",
    "    parser.add_argument('--run-name', type=str, default='multitask_distilbert')\n",
    "    parser.add_argument('--recompute-features', action='store_true')\n",
    "    parser.add_argument('--train-dir', type=str, default='datasets/indomain_train')\n",
    "    parser.add_argument('--val-dir', type=str, default='datasets/indomain_val')\n",
    "    parser.add_argument('--eval-dir', type=str, default='datasets/oodomain_test')\n",
    "    parser.add_argument('--eval-datasets', type=str, default='race,relation_extraction,duorc')\n",
    "    parser.add_argument('--do-train', action='store_true')\n",
    "    parser.add_argument('--do-eval', action='store_true')\n",
    "    parser.add_argument('--sub-file', type=str, default='submission_file')\n",
    "    parser.add_argument('--visualize-predictions', action='store_true')\n",
    "    parser.add_argument('--eval-every', type=int, default=2500)\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_3XY82PI7uC"
   },
   "source": [
    "# chunkdata.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzaXb5IjI5sB",
    "outputId": "58001fe2-d55f-4258-90bc-1109f3802daf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/ChunkData.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/ChunkData.py\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Copyright 2021 Rajbir Bhattacharjee\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "# \n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
    "# SOFTWARE.\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import threading\n",
    "import glob\n",
    "\n",
    "class ChunkDataCommon():\n",
    "    def __init__(self, directory:str, chunk_size=16):\n",
    "        self.directory = directory\n",
    "        self.chunk_size = chunk_size\n",
    "        self.length = -1\n",
    "        self.lock = None\n",
    "        self.current_slice = None\n",
    "        self.current_slice_start = -1\n",
    "\n",
    "    def get_pickle_file_name(self, st, en):\n",
    "        return f\"{self.directory}/array_chunk-{st:08d}-{en:08d}.pickle\"\n",
    "\n",
    "    def get_conf_file_name(self):\n",
    "        with self.lock:\n",
    "            return f\"{self.directory}/config.json\"\n",
    "\n",
    "    def get_slice_start_for_index(self, ind):\n",
    "        return (ind // self.chunk_size) * self.chunk_size\n",
    "\n",
    "    def load_slice(self, ind_start):\n",
    "        with self.lock:\n",
    "            ind_end = ind_start + self.chunk_size - 1\n",
    "            filename =self.get_pickle_file_name(ind_start, ind_end)\n",
    "            with open(filename, \"rb\") as f:\n",
    "                self.current_slice = pickle.load(f)\n",
    "                self.current_slice_start = ind_start\n",
    "                return\n",
    "            self.current_slice = None\n",
    "            self.current_slice_start = -1\n",
    "            raise IndexError\n",
    "\n",
    "    def __len__(self):\n",
    "        with self.lock:\n",
    "            # TODO: RAJBIR: For testing I'll just return a fixed length\n",
    "            #return 2 ** 8 if 2 ** 8 < self.length else self.length\n",
    "            return self.length if self.length > 0 else 0\n",
    "\n",
    "\n",
    "class ChunkDataReader(ChunkDataCommon):\n",
    "    def __init__(self, d, chunk_size=16, lock=None):\n",
    "        if isinstance(d, str):\n",
    "            directory = d\n",
    "        elif isinstance(d, ChunkDataCommon):\n",
    "            directory = d.directory\n",
    "            chunk_size = d.chunk_size\n",
    "\n",
    "        super(self.__class__, self).__init__(directory, chunk_size=chunk_size)\n",
    "\n",
    "        self.lock = threading.RLock() if lock is None else lock\n",
    "        self.length = -1\n",
    "\n",
    "        self.current_slice = None\n",
    "        self.current_slice_start = -1\n",
    "        self.chunk_size = chunk_size\n",
    "        self.directory = directory\n",
    "\n",
    "        with open(self.get_conf_file_name(), \"r\") as f:\n",
    "            conf = json.load(f)\n",
    "            length = conf['length']\n",
    "            self.length = length if length > 0 else -1\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        with self.lock:\n",
    "            if ind >= self.length:\n",
    "                raise IndexError\n",
    "            if (ind < 0):\n",
    "                ind = self.length + ind \n",
    "            with self.lock:\n",
    "                ind_start = self.get_slice_start_for_index(ind)\n",
    "                if ind_start != self.current_slice_start:\n",
    "                    self.load_slice(ind_start)\n",
    "                ind = ind - self.current_slice_start\n",
    "                return self.current_slice[ind]\n",
    "\n",
    "\n",
    "\n",
    "class ChunkDataWriter(ChunkDataCommon):\n",
    "    \"\"\"Uses pickle, but splits into multiple files.\n",
    "    behaves like an array and supports the append() method,\n",
    "    and finalize method()\"\"\"\n",
    "    def __init__(self, directory:str, chunk_size=16):\n",
    "        super(self.__class__, self).__init__(directory, chunk_size)\n",
    "        self.lock = threading.RLock()\n",
    "\n",
    "        # The reader object for __getitem__\n",
    "        self.reader = None\n",
    "\n",
    "        # the current slice of the array\n",
    "        self.current_slice = list()\n",
    "\n",
    "        # Length of all items\n",
    "        self.length = 0\n",
    "\n",
    "        # Where does the current slice start from\n",
    "        self.current_slice_start = -1\n",
    "\n",
    "        self.__setitem__ = None\n",
    "\n",
    "        if not os.path.exists(self.directory):\n",
    "            pathlib.Path(self.directory).mkdir(parents=True, exist_ok=False)\n",
    "\n",
    "        if os.path.exists(self.get_conf_file_name()):\n",
    "            self.load_existing()\n",
    "        else:\n",
    "            self.write_conf()\n",
    "\n",
    "    def load_existing(self):\n",
    "        with open(self.get_conf_file_name(), \"r\") as f:\n",
    "            d = json.load(f)\n",
    "            self.length = d['length']\n",
    "            self.length = self.length if self.length > 0 else 0\n",
    "            if self.length > 0:\n",
    "                ind_start = self.get_slice_start_for_index(self.length - 1)\n",
    "                self.load_slice(ind_start)\n",
    "                self.current_slice_start = ind_start\n",
    "\n",
    "    def finalize(self):\n",
    "        with self.lock:\n",
    "            self.write_conf()\n",
    "\n",
    "    def get_current_slice_max(self):\n",
    "        # Return the index of the last element that can\n",
    "        # be stored in the current slice\n",
    "        with self.lock:\n",
    "            if self.current_slice_start == -1:\n",
    "                return -1\n",
    "            else:\n",
    "                return self.current_slice_start + self.chunk_size - 1\n",
    "\n",
    "    def get_current_slice_end(self):\n",
    "        # Return the end of the current slice\n",
    "        with self.lock:\n",
    "            if self.current_slice_start == -1:\n",
    "                return -1\n",
    "            else:\n",
    "                return self.current_slice_start + len(self.current_slice)\n",
    "\n",
    "    def get_current_slice_size(self):\n",
    "        # return the size of the current slice\n",
    "        with self.lock:\n",
    "            return len(self.current_slice)\n",
    "\n",
    "    def get_current_slice_offsets(self):\n",
    "        with self.lock:\n",
    "            return self.current_slice_start, \\\n",
    "                self.get_current_slice_end(), \\\n",
    "                self.get_current_slice_max()\n",
    "\n",
    "    def get_current_file_name(self):\n",
    "        with self.lock:\n",
    "            if -1 == self.current_slice_start:\n",
    "                return None\n",
    "            st = self.current_slice_start\n",
    "            en = st + self.chunk_size - 1\n",
    "            return self.get_pickle_file_name(st, en)\n",
    "\n",
    "    def write_chunk(self):\n",
    "        with self.lock:\n",
    "            with open(self.get_current_file_name(), \"wb\") as f:\n",
    "                pickle.dump(self.current_slice, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    def write_conf(self):\n",
    "        with self.lock:\n",
    "            thedict = {}\n",
    "            thedict[\"length\"] = self.length\n",
    "            #print(\"Exists?\", os.path.exists(self.directory))\n",
    "            with open(self.get_conf_file_name(), \"w\") as f:\n",
    "                json.dump(thedict, f)\n",
    "\n",
    "    def append(self, x):\n",
    "        with self.lock:\n",
    "            if 0 == len(self.current_slice) % self.chunk_size:\n",
    "                self.current_slice = list()\n",
    "                if self.current_slice_start != -1:\n",
    "                    self.current_slice_start += self.chunk_size\n",
    "            if self.current_slice_start == -1:\n",
    "                self.current_slice_start = 0\n",
    "            self.length += 1\n",
    "            self.current_slice.append(x)\n",
    "            self.write_chunk()\n",
    "            self.write_conf()\n",
    "        pass\n",
    "\n",
    "    def __del__(self):\n",
    "        with self.lock:\n",
    "            try:\n",
    "                # Directory may have been removed by now\n",
    "                # because del might be called quite late\n",
    "                if os.path.exists(self.directory):\n",
    "                    self.write_conf()\n",
    "            except Exception as e:\n",
    "                traceback.print_tb(e.__traceback__, file=sys.stderr)\n",
    "\n",
    "    def __getitem__(self, ind):\n",
    "        with self.lock:\n",
    "            try:\n",
    "                # print(self.length, self.directory)\n",
    "                if self.reader is None or self.reader.length <= ind:\n",
    "                    self.reader = ChunkDataReader(self, lock=self.lock)\n",
    "            except Exception as e:\n",
    "                # print(self.length, self.directory)\n",
    "                traceback.print_tb(e.__traceback__, file=sys.stderr)\n",
    "                raise IndexError\n",
    "            return self.reader[ind]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t444EYHjrbrJ"
   },
   "source": [
    "# util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sn72CZjoq-zB",
    "outputId": "1d99cb1d-3857-49c1-aade-71db264155de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/util.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/util.py\n",
    "\n",
    "import json\n",
    "import random\n",
    "import os\n",
    "import logging\n",
    "import pickle\n",
    "import string\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter, OrderedDict, defaultdict as ddict\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from ChunkData import *\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "def save_pickle(obj, path):\n",
    "    \"\"\"\n",
    "    with open(path, 'wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "    \"\"\"\n",
    "    return\n",
    "\n",
    "def visualize(tbx, pred_dict, gold_dict, step, split, num_visuals):\n",
    "    \"\"\"Visualize text examples to TensorBoard.\n",
    "\n",
    "    Args:\n",
    "        tbx (tensorboardX.SummaryWriter): Summary writer.\n",
    "        pred_dict (dict): dict of predictions of the form id -> pred.\n",
    "        step (int): Number of examples seen so far during training.\n",
    "        split (str): Name of data split being visualized.\n",
    "        num_visuals (int): Number of visuals to select at random from preds.\n",
    "    \"\"\"\n",
    "    if num_visuals <= 0:\n",
    "        return\n",
    "    if num_visuals > len(pred_dict):\n",
    "        num_visuals = len(pred_dict)\n",
    "    id2index = {curr_id : idx for idx, curr_id in enumerate(gold_dict['id'])}\n",
    "    visual_ids = np.random.choice(list(pred_dict), size=num_visuals, replace=False)\n",
    "    for i, id_ in enumerate(visual_ids):\n",
    "        pred = pred_dict[id_] or 'N/A'\n",
    "        idx_gold_dict = id2index[id_]\n",
    "        question = gold_dict['question'][idx_gold_dict]\n",
    "        context = gold_dict['context'][idx_gold_dict]\n",
    "        answers = gold_dict['answer'][idx_gold_dict]\n",
    "        gold = answers['text'][0] if answers else 'N/A'\n",
    "        tbl_fmt = (f'- **Question:** {question}\\n'\n",
    "                   + f'- **Context:** {context}\\n'\n",
    "                   + f'- **Answer:** {gold}\\n'\n",
    "                   + f'- **Prediction:** {pred}')\n",
    "        tbx.add_text(tag=f'{split}/{i+1}_of_{num_visuals}',\n",
    "                     text_string=tbl_fmt,\n",
    "                     global_step=step)\n",
    "\n",
    "\n",
    "def get_save_dir(base_dir, name, id_max=100):\n",
    "    for uid in range(1, id_max):\n",
    "        save_dir = os.path.join(base_dir, f'{name}-{uid:02d}')\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "            return save_dir\n",
    "\n",
    "    raise RuntimeError('Too many save directories created with the same name. \\\n",
    "                       Delete old save directories or use another name.')\n",
    "\n",
    "\n",
    "def filter_encodings(encodings):\n",
    "    filter_idx = [idx for idx, val in enumerate(encodings['end_positions'])\n",
    "                 if not val]\n",
    "    filter_idx = set(filter_idx)\n",
    "    encodings_filtered = {key : [] for key in encodings}\n",
    "    sz = len(encodings['input_ids'])\n",
    "    for idx in range(sz):\n",
    "        if idx not in filter_idx:\n",
    "            for key in encodings:\n",
    "                encodings_filtered[key].append(encodings[key][idx])\n",
    "    return encodings_filtered\n",
    "\n",
    "def merge(encodings, new_encoding):\n",
    "    if not encodings:\n",
    "        return new_encoding\n",
    "    else:\n",
    "        for key in new_encoding:\n",
    "            encodings[key] += new_encoding[key]\n",
    "        return encodings\n",
    "\n",
    "def get_logger(log_dir, name):\n",
    "    \"\"\"Get a `logging.Logger` instance that prints to the console\n",
    "    and an auxiliary file.\n",
    "\n",
    "    Args:\n",
    "        log_dir (str): Directory in which to create the log file.\n",
    "        name (str): Name to identify the logs.\n",
    "\n",
    "    Returns:\n",
    "        logger (logging.Logger): Logger instance for logging events.\n",
    "    \"\"\"\n",
    "    class StreamHandlerWithTQDM(logging.Handler):\n",
    "        \"\"\"Let `logging` print without breaking `tqdm` progress bars.\n",
    "\n",
    "        See Also:\n",
    "            > https://stackoverflow.com/questions/38543506\n",
    "        \"\"\"\n",
    "        def emit(self, record):\n",
    "            try:\n",
    "                msg = self.format(record)\n",
    "                tqdm.write(msg)\n",
    "                self.flush()\n",
    "            except (KeyboardInterrupt, SystemExit):\n",
    "                raise\n",
    "            except:\n",
    "                self.handleError(record)\n",
    "\n",
    "    # Create logger\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Log everything (i.e., DEBUG level and above) to a file\n",
    "    log_path = os.path.join(log_dir, f'{name}.txt')\n",
    "    file_handler = logging.FileHandler(log_path)\n",
    "    file_handler.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Log everything except DEBUG level (i.e., INFO level and above) to console\n",
    "    console_handler = StreamHandlerWithTQDM()\n",
    "    console_handler.setLevel(logging.INFO)\n",
    "\n",
    "    # Create format for the logs\n",
    "    file_formatter = logging.Formatter('[%(asctime)s] %(message)s',\n",
    "                                       datefmt='%m.%d.%y %H:%M:%S')\n",
    "    file_handler.setFormatter(file_formatter)\n",
    "    console_formatter = logging.Formatter('[%(asctime)s] %(message)s',\n",
    "                                          datefmt='%m.%d.%y %H:%M:%S')\n",
    "    console_handler.setFormatter(console_formatter)\n",
    "\n",
    "    # add the handlers to the logger\n",
    "    logger.addHandler(file_handler)\n",
    "    logger.addHandler(console_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Keep track of average values over time.\n",
    "\n",
    "    Adapted from:\n",
    "        > https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset meter.\"\"\"\n",
    "        self.__init__()\n",
    "\n",
    "    def update(self, val, num_samples=1):\n",
    "        \"\"\"Update meter with new value `val`, the average of `num` samples.\n",
    "\n",
    "        Args:\n",
    "            val (float): Average value to update the meter with.\n",
    "            num_samples (int): Number of samples that were averaged to\n",
    "                produce `val`.\n",
    "        \"\"\"\n",
    "        self.count += num_samples\n",
    "        self.sum += val * num_samples\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, encodings, train=True):\n",
    "        self.encodings = encodings\n",
    "        self.keys = ['input_ids', 'attention_mask']\n",
    "        if train:\n",
    "            self.keys += ['start_positions', 'end_positions']\n",
    "        assert(all(key in self.encodings for key in self.keys))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key : torch.tensor(self.encodings[key][idx]) for key in self.keys}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "    data_dict = {'question': [], 'context': [], 'id': [], 'answer': []}\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                if len(qa['answers']) == 0:\n",
    "                    data_dict['question'].append(question)\n",
    "                    data_dict['context'].append(context)\n",
    "                    data_dict['id'].append(qa['id'])\n",
    "                else:\n",
    "                    for answer in  qa['answers']:\n",
    "                        data_dict['question'].append(question)\n",
    "                        data_dict['context'].append(context)\n",
    "                        data_dict['id'].append(qa['id'])\n",
    "                        data_dict['answer'].append(answer)\n",
    "    id_map = ddict(list)\n",
    "    for idx, qid in enumerate(data_dict['id']):\n",
    "        id_map[qid].append(idx)\n",
    "\n",
    "    data_dict_collapsed = {'question': [], 'context': [], 'id': []}\n",
    "    if data_dict['answer']:\n",
    "        data_dict_collapsed['answer'] = []\n",
    "    for qid in id_map:\n",
    "        ex_ids = id_map[qid]\n",
    "        data_dict_collapsed['question'].append(data_dict['question'][ex_ids[0]])\n",
    "        data_dict_collapsed['context'].append(data_dict['context'][ex_ids[0]])\n",
    "        data_dict_collapsed['id'].append(qid)\n",
    "        if data_dict['answer']:\n",
    "            all_answers = [data_dict['answer'][idx] for idx in ex_ids]\n",
    "            data_dict_collapsed['answer'].append({'answer_start': [answer['answer_start'] for answer in all_answers],\n",
    "                                                  'text': [answer['text'] for answer in all_answers]})\n",
    "    return data_dict_collapsed\n",
    "\n",
    "def add_token_positions(encodings, answers, tokenizer):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "        # if end position is None, the 'char_to_token' function points to the space before the correct token - > add + 1\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] + 1)\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "\n",
    "def add_end_idx(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # sometimes squad answers are off by a character or two – fix this\n",
    "        if context[start_idx:end_idx] == gold_text:\n",
    "            answer['answer_end'] = end_idx\n",
    "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 1\n",
    "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
    "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
    "            answer['answer_start'] = start_idx - 2\n",
    "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
    "\n",
    "def convert_tokens(eval_dict, qa_id, y_start_list, y_end_list):\n",
    "    \"\"\"Convert predictions to tokens from the context.\n",
    "\n",
    "    Args:\n",
    "        eval_dict (dict): Dictionary with eval info for the dataset. This is\n",
    "            used to perform the mapping from IDs and indices to actual text.\n",
    "        qa_id (int): List of QA example IDs.\n",
    "        y_start_list (list): List of start predictions.\n",
    "        y_end_list (list): List of end predictions.\n",
    "        no_answer (bool): Questions can have no answer. E.g., SQuAD 2.0.\n",
    "\n",
    "    Returns:\n",
    "        pred_dict (dict): Dictionary index IDs -> predicted answer text.\n",
    "        sub_dict (dict): Dictionary UUIDs -> predicted answer text (submission).\n",
    "    \"\"\"\n",
    "    pred_dict = {}\n",
    "    sub_dict = {}\n",
    "    for qid, y_start, y_end in zip(qa_id, y_start_list, y_end_list):\n",
    "        context = eval_dict[str(qid)][\"context\"]\n",
    "        spans = eval_dict[str(qid)][\"spans\"]\n",
    "        uuid = eval_dict[str(qid)][\"uuid\"]\n",
    "        start_idx = spans[y_start][0]\n",
    "        end_idx = spans[y_end][1]\n",
    "        pred_dict[str(qid)] = context[start_idx: end_idx]\n",
    "        sub_dict[uuid] = context[start_idx: end_idx]\n",
    "    return pred_dict, sub_dict\n",
    "\n",
    "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
    "    if not ground_truths:\n",
    "        return metric_fn(prediction, '')\n",
    "    scores_for_ground_truths = []\n",
    "    for ground_truth in ground_truths:\n",
    "        score = metric_fn(prediction, ground_truth)\n",
    "        scores_for_ground_truths.append(score)\n",
    "    return max(scores_for_ground_truths)\n",
    "\n",
    "\n",
    "def eval_dicts(gold_dict, pred_dict):\n",
    "    avna = f1 = em = total = 0\n",
    "    id2index = {curr_id : idx for idx, curr_id in enumerate(gold_dict['id'])}\n",
    "    for curr_id in pred_dict:\n",
    "        total += 1\n",
    "        index = id2index[curr_id]\n",
    "        ground_truths = gold_dict['answer'][index]['text']\n",
    "        prediction = pred_dict[curr_id]\n",
    "        em += metric_max_over_ground_truths(compute_em, prediction, ground_truths)\n",
    "        f1 += metric_max_over_ground_truths(compute_f1, prediction, ground_truths)\n",
    "\n",
    "    eval_dict = {'EM': 100. * em / total,\n",
    "                 'F1': 100. * f1 / total}\n",
    "    return eval_dict\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, predictions,\n",
    "                               n_best_size=20, max_answer_length=30):\n",
    "    all_start_logits, all_end_logits = predictions\n",
    "    # Build a map example to its corresponding features.\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = ddict(list)\n",
    "    for i, feat_id in enumerate(features['id']):\n",
    "        features_per_example[example_id_to_index[feat_id]].append(i)\n",
    "\n",
    "    # The dictionaries we have to fill.\n",
    "    all_predictions = OrderedDict()\n",
    "\n",
    "    # Let's loop over all the examples!\n",
    "    #for example_index in tqdm(range(len(examples['id']))):\n",
    "    for example_index in range(len(examples['id'])):\n",
    "        example = {key : examples[key][example_index] for key in examples}\n",
    "        # Those are the indices of the features associated to the current example.\n",
    "        feature_indices = features_per_example[example_index]\n",
    "        prelim_predictions = []\n",
    "\n",
    "        # Looping through all the features associated to the current example.\n",
    "        for feature_index in feature_indices:\n",
    "            # We grab the predictions of the model for this feature.\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            seq_ids = features.sequence_ids(feature_index)\n",
    "            non_pad_idx = len(seq_ids) - 1\n",
    "            while not seq_ids[non_pad_idx]:\n",
    "                non_pad_idx -= 1\n",
    "            start_logits = start_logits[:non_pad_idx]\n",
    "            end_logits = end_logits[:non_pad_idx]\n",
    "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
    "            # context.\n",
    "            offset_mapping = features[\"offset_mapping\"][feature_index]\n",
    "            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n",
    "            # available in the current feature.\n",
    "            token_is_max_context = features.get(\"token_is_max_context\", None)\n",
    "            if token_is_max_context:\n",
    "                token_is_max_context = token_is_max_context[feature_index]\n",
    "\n",
    "\n",
    "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
    "                    # to part of the input_ids that are not in the context.\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # Don't consider answers with a length that is either = 0 or > max_answer_length.\n",
    "                    if end_index <= start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    # Don't consider answer that don't have the maximum context available (if such information is\n",
    "                    # provided).\n",
    "                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n",
    "                        continue\n",
    "                    prelim_predictions.append(\n",
    "                        {\n",
    "                            \"start_index\": start_index,\n",
    "                            \"end_index\": end_index,\n",
    "                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"start_logit\": start_logits[start_index],\n",
    "                            \"end_logit\": end_logits[end_index],\n",
    "                        }\n",
    "                    )\n",
    "        # Only keep the best `n_best_size` predictions.\n",
    "        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "\n",
    "        # Use the offsets to gather the answer text in the original context.\n",
    "        context = example[\"context\"]\n",
    "        for pred in predictions:\n",
    "            offsets = pred['offsets']\n",
    "            pred[\"text\"] = context[offsets[0] : offsets[1]]\n",
    "\n",
    "        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
    "        # failure.\n",
    "        if len(predictions) == 0:\n",
    "            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n",
    "\n",
    "        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n",
    "        # the LogSumExp trick).\n",
    "        scores = np.array([pred.pop(\"score\") for pred in predictions])\n",
    "        exp_scores = np.exp(scores - np.max(scores))\n",
    "        probs = exp_scores / exp_scores.sum()\n",
    "\n",
    "        # Include the probabilities in our predictions.\n",
    "        for prob, pred in zip(probs, predictions):\n",
    "            pred[\"probability\"] = prob\n",
    "\n",
    "        # need to find the best non-empty prediction.\n",
    "        i = 0\n",
    "        while i < len(predictions):\n",
    "            if predictions[i]['text'] != '':\n",
    "                break\n",
    "            i += 1\n",
    "        if i == len(predictions):\n",
    "            import pdb; pdb.set_trace();\n",
    "\n",
    "        best_non_null_pred = predictions[i]\n",
    "        all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]\n",
    "\n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "\n",
    "# All methods below this line are from the official SQuAD 2.0 eval script\n",
    "# https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Convert to lowercase and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "        return re.sub(regex, ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_em(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = Counter(gold_toks) & Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nU1kuEaIq8O_"
   },
   "source": [
    "# train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BVWkdkWYHgC0",
    "outputId": "d6d80e7a-eb55-4ea3-a5af-bece51e26a9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /tmp/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /tmp/train.py\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import csv\n",
    "import util\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForQuestionAnswering\n",
    "from transformers import DistilBertConfig\n",
    "from transformers import AdamW\n",
    "from tensorboardX import SummaryWriter\n",
    "import pytorch_warmup as warmup\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
    "from args import get_train_test_args\n",
    "\n",
    "from tqdm import tqdm\n",
    "from ChunkData import *\n",
    "\n",
    "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "def prepare_eval_data(dataset_dict, tokenizer):\n",
    "    tokenized_examples = tokenizer(dataset_dict['question'],\n",
    "                                   dataset_dict['context'],\n",
    "                                   truncation=\"only_second\",\n",
    "                                   stride=128,\n",
    "                                   max_length=384,\n",
    "                                   return_overflowing_tokens=True,\n",
    "                                   return_offsets_mapping=True,\n",
    "                                   padding='max_length')\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
    "    # corresponding example_id and we will store the offset mappings.\n",
    "    tokenized_examples[\"id\"] = []\n",
    "    for i in tqdm(range(len(tokenized_examples[\"input_ids\"]))):\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"id\"].append(dataset_dict[\"id\"][sample_index])\n",
    "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
    "        # position is part of the context or not.\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == 1 else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "\n",
    "\n",
    "def prepare_train_data(dataset_dict, tokenizer, remove_data=False):\n",
    "    import gc\n",
    "    import shutil\n",
    "    tokenized_examples = tokenizer(dataset_dict['question'],\n",
    "                                   dataset_dict['context'],\n",
    "                                   truncation=\"only_second\",\n",
    "                                   stride=128,\n",
    "                                   max_length=384,\n",
    "                                   return_overflowing_tokens=True,\n",
    "                                   return_offsets_mapping=True,\n",
    "                                   padding='max_length')\n",
    "    orig_tokenized_examples = tokenized_examples\n",
    "    sample_mapping = orig_tokenized_examples[\"overflow_to_sample_mapping\"]\n",
    "    offset_mapping = orig_tokenized_examples[\"offset_mapping\"]\n",
    "\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    tokenized_examples['id'] = []\n",
    "\n",
    "    temp_tokenized_examples = tokenized_examples\n",
    "    if remove_data:\n",
    "        for c in temp_tokenized_examples.keys():\n",
    "            try:\n",
    "                shutil.rmtree(f\"/tmp/train/{str(c)}\")\n",
    "            except:\n",
    "                print(\"Could not remove \", c)\n",
    "\n",
    "    tokenized_examples = {str(c): ChunkDataWriter(f\"/tmp/train/{str(c)}\") for c in temp_tokenized_examples.keys()}\n",
    "\n",
    "    for key in temp_tokenized_examples.keys():\n",
    "        key = str(key)\n",
    "        if key not in [\"start_positions\", \"end_positions\", \"id\", \"sequence_ids\", \"overflow_to_sample_mapping\", \"offset_mapping\"]:\n",
    "            v_from = temp_tokenized_examples[key]\n",
    "            v_to = tokenized_examples[key]\n",
    "            #print(f\"Copying {key}... {len(v_from)} {type(v_from)}\")\n",
    "            for i in range(len(v_from)):\n",
    "                v_to.append(v_from[i])\n",
    "                if key not in [\"offset_mapping\", \"overflow_mapping\"]:\n",
    "                    v_from[i] = None\n",
    "                    gc.collect(0)\n",
    "                    gc.collect(1)\n",
    "                    gc.collect(2)\n",
    "            temp_tokenized_examples[key] = None\n",
    "            gc.collect(0)\n",
    "            gc.collect(1)\n",
    "            gc.collect(2)\n",
    "            # print(\"done\")\n",
    "            v_to.finalize()\n",
    "\n",
    "    inaccurate = 0\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = orig_tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answer = dataset_dict['answer'][sample_index]\n",
    "        # Start/end character index of the answer in the text.\n",
    "        start_char = answer['answer_start'][0]\n",
    "        end_char = start_char + len(answer['text'][0])\n",
    "        tokenized_examples['id'].append(dataset_dict['id'][sample_index])\n",
    "        # Start token index of the current span in the text.\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "\n",
    "        # End token index of the current span in the text.\n",
    "        token_end_index = len(input_ids) - 1\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "\n",
    "        # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "            # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index += 1\n",
    "            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "            while offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "            # assertion to check if this checks out\n",
    "            context = dataset_dict['context'][sample_index]\n",
    "            offset_st = offsets[tokenized_examples['start_positions'][-1]][0]\n",
    "            offset_en = offsets[tokenized_examples['end_positions'][-1]][1]\n",
    "            if context[offset_st : offset_en] != answer['text'][0]:\n",
    "                inaccurate += 1\n",
    "\n",
    "    total = len(tokenized_examples['id'])\n",
    "    # print(f\"Preprocessing not completely accurate for {inaccurate}/{total} instances\")\n",
    "    # print([key for key in tokenized_examples.keys()])\n",
    "    tokenized_examples = {k: ChunkDataReader(v) for k, v in tokenized_examples.items()}\n",
    "    return tokenized_examples\n",
    "\n",
    "def get_cached_dataset():\n",
    "    import os\n",
    "    import glob\n",
    "    if os.path.exists(\"/tmp/train/input_ids/config.json\"):\n",
    "        g = glob.glob(\"/tmp/train/*\")\n",
    "        g = [x.split('/')[-1] for x in g]\n",
    "        out_dict = {}\n",
    "        for name in g:\n",
    "            val = ChunkDataReader(f\"/tmp/train/{name}\", chunk_size=16)\n",
    "            out_dict[name] = val\n",
    "        return out_dict\n",
    "    return None\n",
    "\n",
    "def read_and_process(args, tokenizer, dataset_dict, dir_name, dataset_name, split):\n",
    "    def get_dataset_slice(dataset_dict, slice_start, slice_length):\n",
    "        out_dict = {k: [] for k in dataset_dict.keys()}\n",
    "        for k in dataset_dict.keys():\n",
    "            arr = dataset_dict[k]\n",
    "            out_dict[k] = arr[slice_start:slice_start+slice_length]\n",
    "        return out_dict\n",
    "\n",
    "    def get_dataset_length(dataset_dict):\n",
    "        column = [c for c in dataset_dict.keys()][0]\n",
    "        return len(dataset_dict[column])\n",
    "\n",
    "    cache_path = f'{dir_name}/{dataset_name}_encodings.pt'\n",
    "    for key in dataset_dict.keys():\n",
    "        dataset_dict[key] = dataset_dict[key]\n",
    "    if split=='train':\n",
    "        cached_dataset = get_cached_dataset()\n",
    "        if not cached_dataset is None:\n",
    "            print(\"USING CACHED DATASET THAT HAS ALREADY BEEN PREPROCESSED\")\n",
    "            return cached_dataset\n",
    "        cleardir = True\n",
    "        #dataset_dict = get_dataset_slice(dataset_dict, 0, 8192)\n",
    "        stride = 512\n",
    "        for i in tqdm(range(0, get_dataset_length(dataset_dict), stride), \"Preparing training data\"):\n",
    "            temp_dict = get_dataset_slice(dataset_dict, i, stride)\n",
    "            tokenized_examples = prepare_train_data(temp_dict, tokenizer, cleardir)\n",
    "            cleardir = False\n",
    "            # for c in tokenized_examples.keys():\n",
    "            #     print(c, len(tokenized_examples[str(c)]))\n",
    "            # print('-' * 80)\n",
    "    else:\n",
    "        if os.path.exists(cache_path) and not args.recompute_features:\n",
    "            tokenized_examples = util.load_pickle(cache_path)\n",
    "            return tokenized_examples\n",
    "        tokenized_examples = prepare_eval_data(dataset_dict, tokenizer)\n",
    "    util.save_pickle(tokenized_examples, cache_path)\n",
    "    return tokenized_examples\n",
    "\n",
    "WARMUP_PERIOD = 1000\n",
    "UNFREEZE_AFTER = 2000\n",
    "class Trainer():\n",
    "    def __init__(self, args, log):\n",
    "        self.lr = args.lr\n",
    "        self.num_epochs = args.num_epochs\n",
    "        self.device = args.device\n",
    "        self.eval_every = args.eval_every\n",
    "        self.path = os.path.join(args.save_dir, 'checkpoint')\n",
    "        self.num_visuals = args.num_visuals\n",
    "        self.save_dir = args.save_dir\n",
    "        self.log = log\n",
    "        self.visualize_predictions = args.visualize_predictions\n",
    "        if not os.path.exists(self.path):\n",
    "            os.makedirs(self.path)\n",
    "        self.optimizer = None\n",
    "\n",
    "    def save(self, model):\n",
    "        model.save_pretrained(self.path)\n",
    "\n",
    "    def evaluate(self, model, data_loader, data_dict, return_preds=False, split='validation'):\n",
    "        device = self.device\n",
    "\n",
    "        # print(\"\\nEvaluating\")\n",
    "        model.eval()\n",
    "        pred_dict = {}\n",
    "        all_start_logits = []\n",
    "        all_end_logits = []\n",
    "        with torch.no_grad(): #, \\\n",
    "                #tqdm(total=len(data_loader.dataset)) as progress_bar:\n",
    "            for batch in data_loader:\n",
    "                # Setup for forward\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                batch_size = len(input_ids)\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                # Forward\n",
    "                start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
    "\n",
    "                all_start_logits.append(start_logits)\n",
    "                all_end_logits.append(end_logits)\n",
    "                #progress_bar.update(batch_size)\n",
    "\n",
    "        # Get F1 and EM scores\n",
    "        start_logits = torch.cat(all_start_logits).cpu().numpy()\n",
    "        end_logits = torch.cat(all_end_logits).cpu().numpy()\n",
    "        preds = util.postprocess_qa_predictions(data_dict,\n",
    "                                                 data_loader.dataset.encodings,\n",
    "                                                 (start_logits, end_logits))\n",
    "        if split == 'validation':\n",
    "            results = util.eval_dicts(data_dict, preds)\n",
    "            results_list = [('F1', results['F1']),\n",
    "                            ('EM', results['EM'])]\n",
    "        else:\n",
    "            results_list = [('F1', -1.0),\n",
    "                            ('EM', -1.0)]\n",
    "        results = OrderedDict(results_list)\n",
    "        if return_preds:\n",
    "            return preds, results\n",
    "        return results\n",
    "\n",
    "    def gradual_unfreeze(self, model, iter):\n",
    "        \"\"\"\n",
    "        Gradually unfreeze layers, rather than training them all at once\n",
    "        \"\"\"\n",
    "        unfreeze_after_iter = [\n",
    "            (0, \"distilbert.transformer.layer.5.attention.q_lin.weight\"),\n",
    "            (WARMUP_PERIOD, \"distilbert.transformer.layer.4.attention.q_lin.weight\"),\n",
    "            (2 * UNFREEZE_AFTER, \"distilbert.transformer.layer.3.attention.q_lin.weight\"),\n",
    "            (3 * UNFREEZE_AFTER, \"distilbert.transformer.layer.2.attention.q_lin.weight\"),\n",
    "            (4 * UNFREEZE_AFTER, \"distilbert.transformer.layer.1.attention.q_lin.weight\"),\n",
    "            (5 * UNFREEZE_AFTER, \"distilbert.transformer.layer.0.attention.q_lin.weight\"),\n",
    "            (6 * UNFREEZE_AFTER, \"all\")\n",
    "        ]\n",
    "\n",
    "        should_unfreeze = False\n",
    "        unfreeze_from = None\n",
    "\n",
    "        for i, name in unfreeze_after_iter:\n",
    "            if i == iter:\n",
    "                should_unfreeze = True\n",
    "                unfreeze_from = name\n",
    "\n",
    "        if not should_unfreeze:\n",
    "            return False \n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        start_unfreezing = False\n",
    "\n",
    "        printstring = \"\"\n",
    "        if unfreeze_from == \"all\":\n",
    "            printstring += f\"\\nIteration: {iter:>2d} unfreezing all all\"\n",
    "            start_unfreezing = True\n",
    "\n",
    "        did_unfreeze = False\n",
    "        unfrozen_count = 0\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if not start_unfreezing:\n",
    "                if unfreeze_from in name or unfreeze_from == name:\n",
    "                    start_unfreezing = True\n",
    "                    printstring += f\"\\nIteration: {iter:>2d} unfreezing from {unfreeze_from}\"\n",
    "            if start_unfreezing:\n",
    "                param.requires_grad = True\n",
    "                did_unfreeze = True\n",
    "                unfrozen_count += 1\n",
    "\n",
    "        if not did_unfreeze:\n",
    "            # If we didn't unfreeze any layer, unfreeze all\n",
    "            for name, param in model.named_parameters():\n",
    "                param.requires_grad = True\n",
    "                unfrozen_count += 1\n",
    "\n",
    "        print(f\"{printstring} Unfroze {unfrozen_count} layers\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    def get_optimizer(self, model, num_steps, learning_rate, batch_size):\n",
    "        optim = AdamW(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), weight_decay=0.01)\n",
    "\n",
    "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=num_steps)\n",
    "        #lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, factor=0.8, patience=1000)\n",
    "\n",
    "        #warmup_scheduler = warmup.UntunedLinearWarmup(optim)\n",
    "        #warmup_scheduler = warmup.base.LinearWarmup(optim, warmup_period=20000)\n",
    "        #warmup_scheduler = warmup.base.ExponentialWarmup(optim, WARMUP_PERIOD)\n",
    "        warmup_scheduler = warmup.base.LinearWarmup(optim, WARMUP_PERIOD)\n",
    "        print(f\"Using warmup_period={WARMUP_PERIOD}\")\n",
    "\n",
    "        return optim, lr_scheduler, warmup_scheduler\n",
    "\n",
    "\n",
    "    def train(self, model, train_dataloader, eval_dataloader, val_dict):\n",
    "        # TODO: RAJBIR: Change to True\n",
    "        should_eval_every = True\n",
    "\n",
    "        device = self.device\n",
    "        model.to(device)\n",
    "        global_idx = 0\n",
    "        best_scores = {'F1': -1.0, 'EM': -1.0}\n",
    "        tbx = SummaryWriter(self.save_dir)\n",
    "\n",
    "        args = get_train_test_args()\n",
    "        num_steps = len(train_dataloader) * self.num_epochs \n",
    "        #optim, lr_scheduler, warmup_scheduler = self.get_optimizer(model, num_steps, self.lr, args.batch_size)\n",
    "        \n",
    "        #print(\"num_steps/length of dataloader\", num_steps, len(train_dataloader))\n",
    "        optim =  AdamW(model.parameters(), self.lr)\n",
    "        lr_scheduler = None\n",
    "        warmup_scheduler = None\n",
    "\n",
    "        for epoch_num in range(self.num_epochs):\n",
    "            self.log.info(f'Epoch: {epoch_num}')\n",
    "            new_epoch = True\n",
    "\n",
    "            with torch.enable_grad(), tqdm(total=len(train_dataloader.dataset)) as progress_bar:\n",
    "                for batch in train_dataloader:\n",
    "                    self.gradual_unfreeze(model, global_idx)\n",
    "                    #if new_epoch or self.gradual_unfreeze(model, global_idx):\n",
    "                    #    new_epoch = False\n",
    "                    #    optim, lr_scheduler, warmup_scheduler = self.get_optimizer(model, num_steps, self.lr, args.batch_size)\n",
    "                    optim.zero_grad()\n",
    "                    model.train()\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    start_positions = batch['start_positions'].to(device)\n",
    "                    end_positions = batch['end_positions'].to(device)\n",
    "                    outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                                    start_positions=start_positions,\n",
    "                                    end_positions=end_positions)\n",
    "                    loss = outputs[0]\n",
    "                    loss.backward()\n",
    "                    optim.step()\n",
    "                    if lr_scheduler != None:\n",
    "                        lr_scheduler.step()\n",
    "                    if warmup_scheduler != None:\n",
    "                        warmup_scheduler.dampen()\n",
    "\n",
    "                    progress_bar.update(len(input_ids))\n",
    "                    progress_bar.set_postfix(epoch=epoch_num, NLL=loss.item())\n",
    "                    tbx.add_scalar('train/NLL', loss.item(), global_idx)\n",
    "                    if should_eval_every and global_idx % self.eval_every == 0:\n",
    "                        #self.log.info(f'Evaluating at step {global_idx}...')\n",
    "                        preds, curr_score = self.evaluate(model, eval_dataloader, val_dict, return_preds=True)\n",
    "                        results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in curr_score.items())\n",
    "                        #self.log.info('Visualizing in TensorBoard...')\n",
    "                        for k, v in curr_score.items():\n",
    "                            tbx.add_scalar(f'val/{k}', v, global_idx)\n",
    "                        self.log.info(f'Eval {results_str}')\n",
    "                        if self.visualize_predictions:\n",
    "                            util.visualize(tbx,\n",
    "                                           pred_dict=preds,\n",
    "                                           gold_dict=val_dict,\n",
    "                                           step=global_idx,\n",
    "                                           split='val',\n",
    "                                           num_visuals=self.num_visuals)\n",
    "                        if curr_score['F1'] >= best_scores['F1']:\n",
    "                            best_scores = curr_score\n",
    "                            self.save(model)\n",
    "                    global_idx += 1\n",
    "        if should_eval_every:\n",
    "            preds, curr_score = self.evaluate(model, eval_dataloader, val_dict, return_preds=True)\n",
    "            if curr_score['F1'] >= best_scores['F1']:\n",
    "                best_scores = curr_score\n",
    "                self.save(model)\n",
    "            results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in curr_score.items())\n",
    "            self.log.info(f'Final eval results after epoch: {results_str}')\n",
    "            self.log.info(f'After epoch, best scores so far: {best_scores}')\n",
    "        return best_scores\n",
    "\n",
    "def get_dataset(args, datasets, data_dir, tokenizer, split_name):\n",
    "    datasets = datasets.split(',')\n",
    "    dataset_dict = None\n",
    "    dataset_name=''\n",
    "    for dataset in datasets:\n",
    "        dataset_name += f'_{dataset}'\n",
    "        dataset_dict_curr = util.read_squad(f'{data_dir}/{dataset}')\n",
    "        dataset_dict = util.merge(dataset_dict, dataset_dict_curr)\n",
    "    data_encodings = read_and_process(args, tokenizer, dataset_dict, data_dir, dataset_name, split_name)\n",
    "    return util.QADataset(data_encodings, train=(split_name=='train')), dataset_dict\n",
    "\n",
    "#import torch_xla.core.xla_model as xm\n",
    "def main():\n",
    "    # define parser and arguments\n",
    "    args = get_train_test_args()\n",
    "\n",
    "    util.set_seed(args.seed)\n",
    "    config = DistilBertConfig(dropout=0.2,  attention_dropout=0.2,  output_hidden_states=True)\n",
    "    model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\", config=config)\n",
    "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    if args.do_train:\n",
    "        if not os.path.exists(args.save_dir):\n",
    "            os.makedirs(args.save_dir)\n",
    "        args.save_dir = util.get_save_dir(args.save_dir, args.run_name)\n",
    "        log = util.get_logger(args.save_dir, 'log_train')\n",
    "        log.info(f'Args: {json.dumps(vars(args), indent=4, sort_keys=True)}')\n",
    "        log.info(\"Preparing Training Data...\")\n",
    "        args.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"USING CUDA !!!!\")\n",
    "        else:\n",
    "            #print(xm.xla_device())\n",
    "            #if xm.xla_device() != None:\n",
    "            if False:\n",
    "                args.device = xm.xla_device()\n",
    "                print(\"USING TPU!!! \")\n",
    "            else:\n",
    "                printt(\"USING CPU !!!\")\n",
    "        trainer = Trainer(args, log)\n",
    "        train_dataset, _ = get_dataset(args, args.train_datasets, args.train_dir, tokenizer, 'train')\n",
    "        log.info(\"Preparing Validation Data...\")\n",
    "        val_dataset, val_dict = get_dataset(args, args.train_datasets, args.val_dir, tokenizer, 'val')\n",
    "        train_loader = DataLoader(train_dataset,\n",
    "                        batch_size=args.batch_size,\n",
    "                        sampler=RandomSampler(train_dataset))\n",
    "        val_loader = DataLoader(val_dataset,\n",
    "                                batch_size=args.batch_size,\n",
    "                                sampler=SequentialSampler(val_dataset))\n",
    "        best_scores = trainer.train(model, train_loader, val_loader, val_dict)\n",
    "    if args.do_eval:\n",
    "        args.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        split_name = 'test' if 'test' in args.eval_dir else 'validation'\n",
    "        log = util.get_logger(args.save_dir, f'log_{split_name}')\n",
    "        trainer = Trainer(args, log)\n",
    "        checkpoint_path = os.path.join(args.save_dir, 'checkpoint')\n",
    "        model = DistilBertForQuestionAnswering.from_pretrained(checkpoint_path)\n",
    "        model.to(args.device)\n",
    "        eval_dataset, eval_dict = get_dataset(args, args.eval_datasets, args.eval_dir, tokenizer, split_name)\n",
    "        eval_loader = DataLoader(eval_dataset,\n",
    "                                 batch_size=args.batch_size,\n",
    "                                 sampler=SequentialSampler(eval_dataset))\n",
    "        eval_preds, eval_scores = trainer.evaluate(model, eval_loader,\n",
    "                                                   eval_dict, return_preds=True,\n",
    "                                                   split=split_name)\n",
    "        results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in eval_scores.items())\n",
    "        log.info(f'Eval {results_str}')\n",
    "        # Write submission file\n",
    "        sub_path = os.path.join(args.save_dir, split_name + '_' + args.sub_file)\n",
    "        log.info(f'Writing submission file to {sub_path}...')\n",
    "        with open(sub_path, 'w', newline='', encoding='utf-8') as csv_fh:\n",
    "            csv_writer = csv.writer(csv_fh, delimiter=',')\n",
    "            csv_writer.writerow(['Id', 'Predicted'])\n",
    "            for uuid in sorted(eval_preds):\n",
    "                csv_writer.writerow([uuid, eval_preds[uuid]])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "H5FnNtmkM0Om"
   },
   "outputs": [],
   "source": [
    "#!find / -name baseline-01\n",
    "#!rm -rf /content/mtu-nlp-assignment/assignment2/robustqa/save/baseline-01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STjJF5WnEmq3"
   },
   "source": [
    "# run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZQpzgaT95wt",
    "outputId": "38a6da78-b33e-4a43-f0eb-bdb1dada20c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# Modify this if python version is different\n",
    "export PATH=$PATH:/usr/lib/python3.7/venv/scripts/common/:/usr/lib/python3.6/venv/scripts/common/activate\n",
    "cd mtu-nlp-assignment/assignment2/\n",
    "\n",
    "rm -rf robustqa\n",
    "unzip -o robustqa.zip\n",
    "cd robustqa\n",
    "mv datasets_50k.tar.gz  datasets_50k.tar\n",
    "tar -xf datasets_50k.tar\n",
    "\n",
    "source activate robustqa\n",
    "cp -f /tmp/args.py ./args.py\n",
    "cp -f /tmp/train.py ./train.py\n",
    "cp -f /tmp/util.py ./util.py\n",
    "cp -f /tmp/ChunkData.py ./ChunkData.py\n",
    "grep batch.size args.py\n",
    "python train.py --do-train --run-name baseline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvxLkNEWEqsb"
   },
   "source": [
    "# Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V7BYx8jAEQCj",
    "outputId": "641b3260-f695-47ef-bf42-34332a261cd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  robustqa.zip\n",
      "   creating: robustqa/\n",
      "  inflating: robustqa/datasets_50k.tar.gz  \n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/robustqa/\n",
      "  inflating: __MACOSX/robustqa/._datasets_50k.tar.gz  \n",
      "  inflating: robustqa/convert_to_squad.py  \n",
      "  inflating: robustqa/environment.yml  \n",
      "  inflating: robustqa/util.py        \n",
      "  inflating: robustqa/README.md      \n",
      "  inflating: __MACOSX/robustqa/._README.md  \n",
      "  inflating: robustqa/train.py       \n",
      "  inflating: robustqa/args.py        \n",
      "    parser.add_argument('--batch-size', type=int, default=32)\n",
      "Downloading: 100% 268M/268M [00:05<00:00, 48.8MB/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Downloading: 100% 232k/232k [00:00<00:00, 902kB/s]\n",
      "Downloading: 100% 466k/466k [00:00<00:00, 1.49MB/s]\n",
      "[01.01.22 12:55:23] Args: {\n",
      "    \"batch_size\": 32,\n",
      "    \"do_eval\": false,\n",
      "    \"do_train\": true,\n",
      "    \"eval\": false,\n",
      "    \"eval_datasets\": \"race,relation_extraction,duorc\",\n",
      "    \"eval_dir\": \"datasets/oodomain_test\",\n",
      "    \"eval_every\": 2500,\n",
      "    \"lr\": 3e-05,\n",
      "    \"num_epochs\": 4,\n",
      "    \"num_visuals\": 10,\n",
      "    \"recompute_features\": false,\n",
      "    \"run_name\": \"baseline\",\n",
      "    \"save_dir\": \"save/baseline-01\",\n",
      "    \"seed\": 42,\n",
      "    \"sub_file\": \"submission_file\",\n",
      "    \"train\": false,\n",
      "    \"train_datasets\": \"squad,nat_questions,newsqa\",\n",
      "    \"train_dir\": \"datasets/indomain_train\",\n",
      "    \"val_dir\": \"datasets/indomain_val\",\n",
      "    \"visualize_predictions\": false\n",
      "}\n",
      "[01.01.22 12:55:23] Preparing Training Data...\n",
      "USING CUDA !!!!\n",
      "USING CACHED DATASET THAT HAS ALREADY BEEN PREPROCESSED\n",
      "[01.01.22 12:55:27] Preparing Validation Data...\n",
      "100% 38888/38888 [00:02<00:00, 15554.94it/s]\n",
      "[01.01.22 12:55:48] Epoch: 0\n",
      "  0% 0/488383 [00:00<?, ?it/s]\n",
      "Iteration:  0 unfreezing from distilbert.transformer.layer.5.attention.q_lin.weight Unfroze 18 layers\n",
      "[01.01.22 13:00:05] Eval F1: 07.54, EM: 00.06\n",
      "  7% 32000/488383 [08:59<1:06:58, 113.57it/s, NLL=2.06, epoch=0]\n",
      "Iteration: 1000 unfreezing from distilbert.transformer.layer.4.attention.q_lin.weight Unfroze 34 layers\n",
      "[01.01.22 13:17:43] Eval F1: 41.07, EM: 26.54\n",
      " 26% 128000/488383 [30:34<1:04:54, 92.53it/s, NLL=1.41, epoch=0]\n",
      "Iteration: 4000 unfreezing from distilbert.transformer.layer.3.attention.q_lin.weight Unfroze 50 layers\n",
      "[01.01.22 13:37:31] Eval F1: 52.52, EM: 36.42\n",
      " 39% 192000/488383 [48:34<1:03:21, 77.97it/s, NLL=1.54, epoch=0]\n",
      "Iteration: 6000 unfreezing from distilbert.transformer.layer.2.attention.q_lin.weight Unfroze 66 layers\n",
      "[01.01.22 14:00:36] Eval F1: 60.56, EM: 43.99\n",
      " 52% 256000/488383 [1:08:46<57:48, 67.00it/s, NLL=1.21, epoch=0]\n",
      "Iteration: 8000 unfreezing from distilbert.transformer.layer.1.attention.q_lin.weight Unfroze 82 layers\n",
      " 66% 320000/488383 [1:26:51<47:26, 59.15it/s, NLL=1.1, epoch=0] \n",
      "Iteration: 10000 unfreezing from distilbert.transformer.layer.0.attention.q_lin.weight Unfroze 98 layers\n",
      "[01.01.22 14:26:57] Eval F1: 64.60, EM: 48.34\n",
      " 79% 384000/488383 [1:51:23<33:03, 52.63it/s, NLL=0.957, epoch=0]\n",
      "Iteration: 12000 unfreezing all all Unfroze 102 layers\n",
      "[01.01.22 14:56:43] Eval F1: 66.22, EM: 49.77\n",
      "[01.01.22 15:27:02] Eval F1: 67.90, EM: 51.53\n",
      "100% 488383/488383 [2:33:57<00:00, 52.87it/s, NLL=0.768, epoch=0]\n",
      "[01.01.22 15:29:46] Epoch: 1\n",
      "[01.01.22 15:57:37] Eval F1: 68.29, EM: 52.15\n",
      "[01.01.22 16:28:15] Eval F1: 69.08, EM: 52.55\n",
      "[01.01.22 16:58:50] Eval F1: 69.41, EM: 53.13\n",
      "[01.01.22 17:29:26] Eval F1: 69.75, EM: 53.50\n",
      "[01.01.22 18:00:02] Eval F1: 70.31, EM: 53.87\n",
      "[01.01.22 18:30:37] Eval F1: 69.98, EM: 54.04\n",
      "100% 488383/488383 [3:06:21<00:00, 43.68it/s, NLL=0.944, epoch=1]\n",
      "[01.01.22 18:36:07] Epoch: 2\n",
      "[01.01.22 19:01:09] Eval F1: 70.46, EM: 54.34\n",
      "[01.01.22 19:31:42] Eval F1: 70.74, EM: 54.59\n",
      "[01.01.22 20:02:16] Eval F1: 70.20, EM: 54.07\n",
      "[01.01.22 20:32:49] Eval F1: 70.64, EM: 54.50\n",
      "[01.01.22 21:03:26] Eval F1: 70.69, EM: 54.70\n",
      "[01.01.22 21:34:03] Eval F1: 71.30, EM: 54.89\n",
      "100% 488383/488383 [3:06:12<00:00, 43.71it/s, NLL=0.755, epoch=2]\n",
      "[01.01.22 21:42:20] Epoch: 3\n",
      "[01.01.22 22:04:40] Eval F1: 71.29, EM: 54.88\n",
      "[01.01.22 22:35:15] Eval F1: 71.29, EM: 55.02\n",
      "[01.01.22 23:05:48] Eval F1: 71.26, EM: 55.24\n",
      "[01.01.22 23:36:22] Eval F1: 70.89, EM: 54.71\n",
      "[01.02.22 00:06:54] Eval F1: 71.54, EM: 54.97\n",
      "[01.02.22 00:37:27] Eval F1: 71.43, EM: 55.04\n",
      "100% 488383/488383 [3:06:06<00:00, 43.74it/s, NLL=0.236, epoch=3]\n",
      "[01.02.22 00:52:44] Final eval results after epoch: F1: 71.63, EM: 55.32\n",
      "[01.02.22 00:52:44] After epoch, best scores so far: OrderedDict([('F1', 71.54445272557282), ('EM', 54.967774639727715)])\n"
     ]
    }
   ],
   "source": [
    "!bash ./run.sh "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mjcg_tNu8RHB"
   },
   "source": [
    "# Save pre-processed dataset\n",
    "Save the tokenized and pre-processed dataset for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CglTX0qbLJuF",
    "outputId": "c8c0b79a-dd26-4321-a6da-afb00a5d2152"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing save_train.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile save_train.sh\n",
    "#!/bin/bash\n",
    "THEDATE=`date +%Y-%m-%d_%H-%M-%S`\n",
    "\n",
    "echo \"COPYING SAVE\"\n",
    "echo \"------------\"\n",
    "THEDATE=`date +%Y-%m-%d_%H-%M-%S`\n",
    "cd mtu-nlp-assignment/assignment2/robustqa\n",
    "tar -cf \"save-${THEDATE}.tar\" save\n",
    "gzip \"save-${THEDATE}.tar\"\n",
    "cp \"save-${THEDATE}.tar.gz\" /content/gdrive/MyDrive/NLP/NLP-Save-Train\n",
    "du -sh \"/content/gdrive/MyDrive/NLP/NLP-Save-Train/save-${THEDATE}.tar.gz\"\n",
    "\n",
    "echo \"----------------------------------------------------------\"\n",
    "\n",
    "if [[  -f /tmp/train/input_ids/config.json ]]\n",
    "then\n",
    "    echo \"COPYING PREPROCESSED TRAINING DATA\"\n",
    "    echo \"----------------------------------\"\n",
    "    cd /tmp\n",
    "    rm -f train.tar train.tar.gz\n",
    "    tar -cf \"train-${THEDATE}.tar\" train\n",
    "    gzip \"train-${THEDATE}.tar\"\n",
    "    mkdir -p /content/gdrive/MyDrive/NLP/NLP-Save-Train\n",
    "    cp \"train-${THEDATE}.tar.gz\" \"/content/gdrive/MyDrive/NLP/NLP-Save-Train/train-${THEDATE}.tar.gz\"\n",
    "    du -sh \"/content/gdrive/MyDrive/NLP/NLP-Save-Train/train-${THEDATE}.tar.gz\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1QJ8T1yGbX23",
    "outputId": "5cd02d24-027b-4535-e6e0-3975ee4bd89c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPYING SAVE\n",
      "------------\n",
      "235M\t/content/gdrive/MyDrive/NLP/NLP-Save-Train/save-2022-01-02_00-56-46.tar.gz\n",
      "----------------------------------------------------------\n",
      "COPYING PREPROCESSED TRAINING DATA\n",
      "----------------------------------\n",
      "99M\t/content/gdrive/MyDrive/NLP/NLP-Save-Train/train-2022-01-02_00-56-46.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!/bin/bash save_train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7y97rZCDwvcN",
    "outputId": "2fa746d5-379e-4553-a4f6-3f56a9511e5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/train/end_positions/config.json\n",
      "{\"length\": 488383}\n",
      "\n",
      "/tmp/train/overflow_to_sample_mapping/config.json\n",
      "{\"length\": 0}\n",
      "\n",
      "/tmp/train/id/config.json\n",
      "{\"length\": 488383}\n",
      "\n",
      "/tmp/train/attention_mask/config.json\n",
      "{\"length\": 488383}\n",
      "\n",
      "/tmp/train/start_positions/config.json\n",
      "{\"length\": 488383}\n",
      "\n",
      "/tmp/train/input_ids/config.json\n",
      "{\"length\": 488383}\n",
      "\n",
      "/tmp/train/offset_mapping/config.json\n",
      "{\"length\": 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!find /tmp/train -name config.json -exec ls {} \\; -exec cat {} \\; -exec echo \\; -exec echo \\;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lP3-yRmeGoSg"
   },
   "source": [
    "## Create submissions file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYbmQ11dxpxz",
    "outputId": "d055cf10-401d-4d07-a295-139289d2b013"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# Modify this if python version is different\n",
    "export PATH=$PATH:/usr/lib/python3.7/venv/scripts/common/:/usr/lib/python3.6/venv/scripts/common/activate\n",
    "cd /content/mtu-nlp-assignment/assignment2/robustqa/\n",
    "source activate robustqa\n",
    "python train.py --do-eval --sub-file mtl_submission.csv --save-dir save/baseline-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eHFVpz9HLcGA",
    "outputId": "d9b4ef3d-4612-457d-eb28-568c439bff67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100% 7567/7567 [00:00<00:00, 19168.01it/s]\n",
      "[01.02.22 01:00:57] Eval F1: -1.00, EM: -1.00\n",
      "[01.02.22 01:00:57] Writing submission file to save/baseline-01/test_mtl_submission.csv...\n",
      "/content/gdrive/MyDrive/NLP/NLP-Save-Train/test-mtl-submission-2022-01-01_00-51-34.csv\n",
      "\n",
      "/content/gdrive/MyDrive/NLP/NLP-Save-Train/run1-augdata-2022-01-01_00-51-34:\n",
      " save-2022-01-01_00-51-34.tar.gz\n",
      "'test-mtl-submission-2022-01-01_00-51-34 (1).csv'\n",
      " test-mtl-submission-2022-01-01_00-51-34.csv\n",
      " train-2022-01-01_00-51-34.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!bash run.sh\n",
    "!cp ./mtu-nlp-assignment/assignment2/robustqa/save/baseline-01/test_mtl_submission.csv test-mtl-submission-2022-01-02_00-56-46.csv\n",
    "!cp test-mtl-submission-2022-01-02_00-56-46.csv /content/gdrive/MyDrive/NLP/NLP-Save-Train/\n",
    "!ls /content/gdrive/MyDrive/NLP/NLP-Save-Train/*51-34*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEUOTZlSTNcH"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "MTU-NLP-Assignment-2-QA-augmented-gradual-unfreezing_wip",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
