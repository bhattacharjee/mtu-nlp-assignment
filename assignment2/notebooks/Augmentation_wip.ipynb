{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Augmentation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment2/notebooks/Augmentation_wip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f79e967-c049-4e56-a2a7-b6465d9c3244",
        "id": "gwD52NTcePCl"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 7.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 7.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 6.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 69 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 7.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 69.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 50.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 124 kB 8.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 8.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 461 kB 69.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 8.1 MB/s \n",
            "\u001b[?25h  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Cloning into 'mtu-nlp-assignment'...\n",
            "remote: Enumerating objects: 1076, done.\u001b[K\n",
            "remote: Counting objects: 100% (1076/1076), done.\u001b[K\n",
            "remote: Compressing objects: 100% (935/935), done.\u001b[K\n",
            "remote: Total 1076 (delta 542), reused 395 (delta 137), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1076/1076), 96.13 MiB | 29.45 MiB/s, done.\n",
            "Resolving deltas: 100% (542/542), done.\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm\n",
        "!pip install torchvision==0.8.2  -q -q -q\n",
        "!pip install torchtext==0.8.1    -q -q -q\n",
        "!pip install torchaudio==0.7.2   -q -q -q\n",
        "!pip install torch==1.7.1        -q -q -q\n",
        "!pip install tqdm==4.49.0        -q -q -q\n",
        "!pip install transformers==4.2.2 -q -q -q\n",
        "!pip install tensorflow          -q -q -q\n",
        "!pip install tensorboard         -q -q -q\n",
        "!pip install tensorboardX        -q -q -q\n",
        "!pip install --upgrade virtualenv -q -q -q\n",
        "!pip install sentencepiece==0.1.94       -q -q -q\n",
        "!pip install mosestokenizer==1.1.0 -q -q -q\n",
        "\n",
        "!rm -rf mtu-nlp-assignment\n",
        "!git clone https://bhattacharjee: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@github.com/bhattacharjee/mtu-nlp-assignment.git\n",
        "!cp mtu-nlp-assignment/work/augment_data/convert_to_squad.py .\n",
        "!cp mtu-nlp-assignment/work/augment_data/util.py .\n",
        "!cp mtu-nlp-assignment/work/augment_data/args.py .\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile copy_train.sh\n",
        "#!/bin/bash\n",
        "cp /content/gdrive/MyDrive/NLP/NLP-Save-Train/train.tar.gz /tmp\n",
        "cd /tmp\n",
        "gunzip train.tar.gz\n",
        "tar -xf train.tar\n",
        "ls /tmp/train"
      ],
      "metadata": {
        "id": "tXDJM3pMquXd",
        "outputId": "acbc0c71-df4f-4a69-89ca-bd2196e861bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing copy_train.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/bin/bash copy_train.sh"
      ],
      "metadata": {
        "id": "XRjVoCQzqysT",
        "outputId": "5a45dbdb-52f5-4f81-cc9c-9d447aa7b0e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_mask\tid\t   offset_mapping\t       start_positions\n",
            "end_positions\tinput_ids  overflow_to_sample_mapping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip mtu-nlp-assignment/assignment2/robustqa.zip\n",
        "!mv robustqa/datasets_50k.tar.gz datasets_50k.tar\n",
        "!tar -xf datasets_50k.tar"
      ],
      "metadata": {
        "id": "MLkhLTlbst0N",
        "outputId": "488d187d-fd59-4b3f-bfda-909bfc3e7890",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  mtu-nlp-assignment/assignment2/robustqa.zip\n",
            "replace robustqa/datasets_50k.tar.gz? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: robustqa/datasets_50k.tar.gz  \n",
            "  inflating: __MACOSX/robustqa/._datasets_50k.tar.gz  \n",
            "  inflating: robustqa/convert_to_squad.py  \n",
            "  inflating: robustqa/environment.yml  \n",
            "  inflating: robustqa/util.py        \n",
            "  inflating: robustqa/README.md      \n",
            "  inflating: __MACOSX/robustqa/._README.md  \n",
            "  inflating: robustqa/train.py       \n",
            "  inflating: robustqa/args.py        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BackTranslation Implementation"
      ],
      "metadata": {
        "id": "VOLlEKN8eZ3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using cuda\")\n",
        "    torch_device = 'cuda'\n",
        "    cuda = torch.device('cuda')\n",
        "    print(cuda)\n",
        "    #torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "else:\n",
        "    torch_device = 'cpu'\n",
        "\n",
        "target_model_name = 'Helsinki-NLP/opus-mt-en-ROMANCE'\n",
        "target_tokenizer = MarianTokenizer.from_pretrained(target_model_name)\n",
        "target_model = MarianMTModel.from_pretrained(target_model_name)\n",
        "\n",
        "en_model_name = 'Helsinki-NLP/opus-mt-ROMANCE-en'\n",
        "en_tokenizer = MarianTokenizer.from_pretrained(en_model_name)\n",
        "en_model = MarianMTModel.from_pretrained(en_model_name)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    target_model = target_model.to(torch_device)\n",
        "    en_model = en_model.to(torch_device)\n",
        "\n",
        "def translate(textarray, model, tokenizer, lang=\"fr\"):\n",
        "    # Prepare the text data into appropriate format for the model\n",
        "    def templatize(text):\n",
        "        if lang== 'en':\n",
        "            return text\n",
        "        else:\n",
        "            return f\">>{lang}<< {text}\"\n",
        "    textarray = [templatize(t) for t in textarray]\n",
        "    encoded = tokenizer.prepare_seq2seq_batch(textarray, return_tensors='pt').to(torch_device)\n",
        "    translated = model.generate(**encoded).to(torch_device)\n",
        "    translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "    return translated_texts\n",
        "\n",
        "def back_translate(texts, source_lang=\"en\", target_lang=\"fr\"):\n",
        "    # Translate from source to target language\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    fr_texts = translate(texts, target_model, target_tokenizer, \n",
        "                         lang=target_lang)\n",
        "    # Translate from target language back to source language\n",
        "    back_translated_texts = translate(fr_texts, en_model, en_tokenizer, \n",
        "                                      lang=source_lang)\n",
        "    return back_translated_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad405c8a-6a1e-4163-fcff-924117a4853e",
        "id": "uWVUx0bueTI2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read squad and translate"
      ],
      "metadata": {
        "id": "ktHHWQhglvf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#/usr/bin/env python3\n",
        "import json\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import csv\n",
        "import util\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import DistilBertForQuestionAnswering\n",
        "from transformers import AdamW\n",
        "from args import *\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
        "from args import get_train_test_args\n",
        "\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "def get_dataset(args, datasets, data_dir, tokenizer, split_name):\n",
        "    datasets = datasets.split(',')\n",
        "    dataset_dict = None\n",
        "    dataset_name=''\n",
        "    for dataset in datasets:\n",
        "        dataset_name += f'_{dataset}'\n",
        "        dataset_dict_curr = util.read_squad(f'{data_dir}/{dataset}')\n",
        "        dataset_dict = util.merge(dataset_dict, dataset_dict_curr)\n",
        "    return dataset_dict\n",
        "\n",
        "def print_question(question, context, ind, answer):\n",
        "    print(f\"ID:            {ind}\")\n",
        "    print(f\"CONTEXT:       {context}\")\n",
        "    print(f\"QUESTOIN:      {context}\")\n",
        "    print(f\"ANSWER:        {answer}\")\n",
        "    anslen = len(ans)\n",
        "\n",
        "def get_new_context(context:str, answer:str)->tuple:\n",
        "    \"\"\"\n",
        "    Replace the answer in the context with a number\n",
        "    so that it doesn't backtranslate\n",
        "\n",
        "    Return a tuple:\n",
        "        1. string that was replaced, handy while restoring\n",
        "           the answer in the translated string\n",
        "        2. translated question\n",
        "    \"\"\"\n",
        "    replaced_str = \"\"\n",
        "    start_ind = context.index(answer)\n",
        "    orig_ans = answer\n",
        "    len_orig_ans = len(orig_ans)\n",
        "    context = list(context)\n",
        "\n",
        "    for i in range(len_orig_ans):\n",
        "        replaced_str +=  '1'\n",
        "        context[i + start_ind] = '1'\n",
        "\n",
        "    print('-' * 80)\n",
        "    print(replaced_str)\n",
        "    print('-' * 80)\n",
        "    return \"\".join(context), replaced_str\n",
        "\n",
        "def get_start_end_index(context, replaced):\n",
        "    with open(\"save.txt\", \"a\") as f:\n",
        "        print(context, file=f)\n",
        "        print(replaced, file=f)\n",
        "\n",
        "    try:\n",
        "        start_ind = context.index(replaced)\n",
        "    except:\n",
        "        # Sometimes the replaced string is truncated by the\n",
        "        # translation, try again with half the string to see\n",
        "        # if there is a match\n",
        "        if len(replaced) < 4:\n",
        "            return -1, -1\n",
        "        try:\n",
        "            start_ind = context.index(replaced[:len(replaced) // 2])\n",
        "        except:\n",
        "            if len(replaced) < 8:\n",
        "                return -1, -1\n",
        "            try:\n",
        "                start_ind = context.index(replaced[len(replaced) // 4])\n",
        "            except:\n",
        "                return -1, -1\n",
        "\n",
        "    start_char = replaced[0]\n",
        "    end_ind = start_ind\n",
        "    while end_ind < len(context) and context[end_ind] == start_char:\n",
        "        end_ind += 1\n",
        "    return start_ind, end_ind\n",
        "\n",
        "\n",
        "    \n",
        "def reconstruct_context(context:str, replaced:str, origans:str)->str:\n",
        "    \"\"\"\n",
        "    Take the backtranslated context, and replace the placeholder\n",
        "    for the answer with the actual answer\n",
        "    \"\"\"\n",
        "    start_index, end_index = get_start_end_index(context, replaced)\n",
        "    str1 = context[:start_index]\n",
        "    str2 = context[end_index:]\n",
        "    return str1 + origans + str2\n",
        "\n",
        "def back_translate_context(context, answer):\n",
        "    temp_ctx, replaced = get_new_context(context, answer)\n",
        "    trns_ctx = back_translate(temp_ctx)[0]\n",
        "    new_ctx = reconstruct_context(trns_ctx, replaced, answer)\n",
        "    return new_ctx\n",
        "\n",
        "\n",
        "def main():\n",
        "    # define parser and arguments\n",
        "    import sys\n",
        "    sys.argv = ['']\n",
        "    args = get_train_test_args()\n",
        "\n",
        "    util.set_seed(args.seed)\n",
        "    print(args)\n",
        "\n",
        "    if not os.path.exists(args.save_dir):\n",
        "        os.makedirs(args.save_dir)\n",
        "    args.save_dir = util.get_save_dir(args.save_dir, args.run_name)\n",
        "    log = util.get_logger(args.save_dir, 'log_train')\n",
        "    train_dataset = get_dataset(args, args.train_datasets, args.train_dir, None, 'train')\n",
        "\n",
        "    print(train_dataset.keys())\n",
        "\n",
        "    forig = open(\"orig.txt\", \"w\")\n",
        "    fnew = open(\"new.txt\", \"w\")\n",
        "\n",
        "    questions = train_dataset['question']\n",
        "    contexts = train_dataset['context']\n",
        "    ids = train_dataset['id']\n",
        "    answers = train_dataset['answer']\n",
        "    j = 0\n",
        "    for q, c, i, a in zip(questions, contexts, ids, answers):\n",
        "        if j == 4:\n",
        "            answer = a['text'][0]\n",
        "            nc = back_translate_context(c, answer)\n",
        "            nq = back_translate(q)\n",
        "\n",
        "            print(c, file=forig)\n",
        "            print(q, file=forig)\n",
        "            print(answer, file=forig)\n",
        "            print(\"\", file=forig)\n",
        "\n",
        "            print(nc, file=fnew)\n",
        "            print(nq, file=fnew)\n",
        "            print(answer, file=fnew)\n",
        "            print(\"\", file=fnew)\n",
        "\n",
        "        j += 1\n",
        "        if j > 10: break\n",
        "\n",
        "    forig.close()\n",
        "    fnew.close()\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "fvfeN9ukg8Vr",
        "outputId": "e7fe6444-580d-4bc7-c091-b354cae90fd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=16, do_eval=False, do_train=False, eval=False, eval_datasets='race,relation_extraction,duorc', eval_dir='datasets/oodomain_test', eval_every=5000, lr=3e-05, num_epochs=3, num_visuals=10, recompute_features=False, run_name='multitask_distilbert', save_dir='save/', seed=42, sub_file='', train=False, train_datasets='squad,nat_questions,newsqa', train_dir='datasets/indomain_train', val_dir='datasets/indomain_val', visualize_predictions=False)\n",
            "dict_keys(['question', 'context', 'id', 'answer'])\n",
            "--------------------------------------------------------------------------------\n",
            "111111111\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}