{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Augmentation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment2/notebooks/Augmentation_wip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e86131-d2cb-432f-8feb-27ce4e023512",
        "id": "gwD52NTcePCl"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.49.0)\n",
            "Cloning into 'mtu-nlp-assignment'...\n",
            "remote: Enumerating objects: 1027, done.\u001b[K\n",
            "remote: Counting objects: 100% (1027/1027), done.\u001b[K\n",
            "remote: Compressing objects: 100% (892/892), done.\u001b[K\n",
            "remote: Total 1027 (delta 508), reused 383 (delta 131), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1027/1027), 96.10 MiB | 34.04 MiB/s, done.\n",
            "Resolving deltas: 100% (508/508), done.\n",
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm\n",
        "!pip install torchvision==0.8.2  -q -q -q\n",
        "!pip install torchtext==0.8.1    -q -q -q\n",
        "!pip install torchaudio==0.7.2   -q -q -q\n",
        "!pip install torch==1.7.1        -q -q -q\n",
        "!pip install tqdm==4.49.0        -q -q -q\n",
        "!pip install transformers==4.2.2 -q -q -q\n",
        "!pip install tensorflow          -q -q -q\n",
        "!pip install tensorboard         -q -q -q\n",
        "!pip install tensorboardX        -q -q -q\n",
        "!pip install --upgrade virtualenv -q -q -q\n",
        "!pip install sentencepiece==0.1.94       -q -q -q\n",
        "!pip install mosestokenizer==1.1.0 -q -q -q\n",
        "\n",
        "!rm -rf mtu-nlp-assignment\n",
        "!git clone https://github.com/bhattacharjee/mtu-nlp-assignment.git\n",
        "!cp mtu-nlp-assignment/work/augment_data/convert_to_squad.py .\n",
        "!cp mtu-nlp-assignment/work/augment_data/util.py .\n",
        "!cp mtu-nlp-assignment/work/augment_data/args.py .\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile copy_train.sh\n",
        "#!/bin/bash\n",
        "cp /content/gdrive/MyDrive/NLP/NLP-Save-Train/train.tar.gz /tmp\n",
        "cd /tmp\n",
        "gunzip train.tar.gz\n",
        "tar -xf train.tar\n",
        "ls /tmp/train"
      ],
      "metadata": {
        "id": "tXDJM3pMquXd",
        "outputId": "acbc0c71-df4f-4a69-89ca-bd2196e861bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing copy_train.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/bin/bash copy_train.sh"
      ],
      "metadata": {
        "id": "XRjVoCQzqysT",
        "outputId": "5a45dbdb-52f5-4f81-cc9c-9d447aa7b0e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "attention_mask\tid\t   offset_mapping\t       start_positions\n",
            "end_positions\tinput_ids  overflow_to_sample_mapping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip mtu-nlp-assignment/assignment2/robustqa.zip\n",
        "!mv robustqa/datasets_50k.tar.gz datasets_50k.tar\n",
        "!tar -xf datasets_50k.tar"
      ],
      "metadata": {
        "id": "MLkhLTlbst0N",
        "outputId": "488d187d-fd59-4b3f-bfda-909bfc3e7890",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  mtu-nlp-assignment/assignment2/robustqa.zip\n",
            "replace robustqa/datasets_50k.tar.gz? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: robustqa/datasets_50k.tar.gz  \n",
            "  inflating: __MACOSX/robustqa/._datasets_50k.tar.gz  \n",
            "  inflating: robustqa/convert_to_squad.py  \n",
            "  inflating: robustqa/environment.yml  \n",
            "  inflating: robustqa/util.py        \n",
            "  inflating: robustqa/README.md      \n",
            "  inflating: __MACOSX/robustqa/._README.md  \n",
            "  inflating: robustqa/train.py       \n",
            "  inflating: robustqa/args.py        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BackTranslation Implementation"
      ],
      "metadata": {
        "id": "VOLlEKN8eZ3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Using cuda\")\n",
        "    torch_device = 'cuda'\n",
        "    cuda = torch.device('cuda')\n",
        "    print(cuda)\n",
        "    #torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "else:\n",
        "    torch_device = 'cpu'\n",
        "\n",
        "target_model_name = 'Helsinki-NLP/opus-mt-en-ROMANCE'\n",
        "target_tokenizer = MarianTokenizer.from_pretrained(target_model_name)\n",
        "target_model = MarianMTModel.from_pretrained(target_model_name)\n",
        "\n",
        "en_model_name = 'Helsinki-NLP/opus-mt-ROMANCE-en'\n",
        "en_tokenizer = MarianTokenizer.from_pretrained(en_model_name)\n",
        "en_model = MarianMTModel.from_pretrained(en_model_name)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    target_model = target_model.to(torch_device)\n",
        "    en_model = en_model.to(torch_device)\n",
        "\n",
        "def translate(textarray, model, tokenizer, lang=\"fr\"):\n",
        "    # Prepare the text data into appropriate format for the model\n",
        "    def templatize(text):\n",
        "        if lang== 'en':\n",
        "            return text\n",
        "        else:\n",
        "            return f\">>{lang}<< {text}\"\n",
        "    textarray = [templatize(t) for t in textarray]\n",
        "    encoded = tokenizer.prepare_seq2seq_batch(textarray, return_tensors='pt').to(torch_device)\n",
        "    translated = model.generate(**encoded).to(torch_device)\n",
        "    translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
        "    return translated_texts\n",
        "\n",
        "def back_translate(texts, source_lang=\"en\", target_lang=\"fr\"):\n",
        "    # Translate from source to target language\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    fr_texts = translate(texts, target_model, target_tokenizer, \n",
        "                         lang=target_lang)\n",
        "    # Translate from target language back to source language\n",
        "    back_translated_texts = translate(fr_texts, en_model, en_tokenizer, \n",
        "                                      lang=source_lang)\n",
        "    return back_translated_texts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad405c8a-6a1e-4163-fcff-924117a4853e",
        "id": "uWVUx0bueTI2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read squad and translate"
      ],
      "metadata": {
        "id": "ktHHWQhglvf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#/usr/bin/env python3\n",
        "import json\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import csv\n",
        "import util\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import DistilBertForQuestionAnswering\n",
        "from transformers import AdamW\n",
        "from args import *\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
        "from args import get_train_test_args\n",
        "\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "\n",
        "def get_dataset(args, datasets, data_dir, tokenizer, split_name):\n",
        "    datasets = datasets.split(',')\n",
        "    dataset_dict = None\n",
        "    dataset_name=''\n",
        "    for dataset in datasets:\n",
        "        dataset_name += f'_{dataset}'\n",
        "        dataset_dict_curr = util.read_squad(f'{data_dir}/{dataset}')\n",
        "        dataset_dict = util.merge(dataset_dict, dataset_dict_curr)\n",
        "    return dataset_dict\n",
        "\n",
        "def print_question(question, context, ind, answer):\n",
        "    print(f\"ID:            {ind}\")\n",
        "    print(f\"CONTEXT:       {context}\")\n",
        "    print(f\"QUESTOIN:      {context}\")\n",
        "    print(f\"ANSWER:        {answer}\")\n",
        "    anslen = len(ans)\n",
        "\n",
        "def get_new_context(context:str, answer:str)->tuple:\n",
        "    \"\"\"\n",
        "    Replace the answer in the context with a number\n",
        "    so that it doesn't backtranslate\n",
        "\n",
        "    Return a tuple:\n",
        "        1. string that was replaced, handy while restoring\n",
        "           the answer in the translated string\n",
        "        2. translated question\n",
        "    \"\"\"\n",
        "    replaced_str = \"\"\n",
        "    start_ind = context.index(answer)\n",
        "    orig_ans = answer\n",
        "    len_orig_ans = len(orig_ans)\n",
        "    context = list(context)\n",
        "\n",
        "    for i in range(len_orig_ans):\n",
        "        replaced_str +=  '1'\n",
        "        context[i + start_ind] = '1'\n",
        "\n",
        "    print('-' * 80)\n",
        "    print(replaced_str)\n",
        "    print('-' * 80)\n",
        "    return \"\".join(context), replaced_str\n",
        "\n",
        "def get_start_end_index(context, replaced):\n",
        "    with open(\"save.txt\", \"a\") as f:\n",
        "        print(context, file=f)\n",
        "        print(replaced, file=f)\n",
        "\n",
        "    try:\n",
        "        start_ind = context.index(replaced)\n",
        "    except:\n",
        "        # Sometimes the replaced string is truncated by the\n",
        "        # translation, try again with half the string to see\n",
        "        # if there is a match\n",
        "        if len(replaced) < 4:\n",
        "            return -1, -1\n",
        "        try:\n",
        "            start_ind = context.index(replaced[:len(replaced) // 2])\n",
        "        except:\n",
        "            if len(replaced) < 8:\n",
        "                return -1, -1\n",
        "            try:\n",
        "                start_ind = context.index(replaced[len(replaced) // 4])\n",
        "            except:\n",
        "                return -1, -1\n",
        "\n",
        "    start_char = replaced[0]\n",
        "    end_ind = start_ind\n",
        "    while end_ind < len(context) and context[end_ind] == start_char:\n",
        "        end_ind += 1\n",
        "    return start_ind, end_ind\n",
        "\n",
        "\n",
        "    \n",
        "def reconstruct_context(context:str, replaced:str, origans:str)->str:\n",
        "    \"\"\"\n",
        "    Take the backtranslated context, and replace the placeholder\n",
        "    for the answer with the actual answer\n",
        "    \"\"\"\n",
        "    start_index, end_index = get_start_end_index(context, replaced)\n",
        "    str1 = context[:start_index]\n",
        "    str2 = context[end_index:]\n",
        "    return str1 + origans + str2\n",
        "\n",
        "def back_translate_context(context, answer):\n",
        "    temp_ctx, replaced = get_new_context(context, answer)\n",
        "    trns_ctx = back_translate(temp_ctx)[0]\n",
        "    new_ctx = reconstruct_context(trns_ctx, replaced, answer)\n",
        "    return new_ctx\n",
        "\n",
        "\n",
        "def main():\n",
        "    # define parser and arguments\n",
        "    import sys\n",
        "    sys.argv = ['']\n",
        "    args = get_train_test_args()\n",
        "\n",
        "    util.set_seed(args.seed)\n",
        "    print(args)\n",
        "\n",
        "    if not os.path.exists(args.save_dir):\n",
        "        os.makedirs(args.save_dir)\n",
        "    args.save_dir = util.get_save_dir(args.save_dir, args.run_name)\n",
        "    log = util.get_logger(args.save_dir, 'log_train')\n",
        "    train_dataset = get_dataset(args, args.train_datasets, args.train_dir, None, 'train')\n",
        "\n",
        "    print(train_dataset.keys())\n",
        "\n",
        "    forig = open(\"orig.txt\", \"w\")\n",
        "    fnew = open(\"new.txt\", \"w\")\n",
        "\n",
        "    questions = train_dataset['question']\n",
        "    contexts = train_dataset['context']\n",
        "    ids = train_dataset['id']\n",
        "    answers = train_dataset['answer']\n",
        "    j = 0\n",
        "    for q, c, i, a in zip(questions, contexts, ids, answers):\n",
        "        if j == 4:\n",
        "            answer = a['text'][0]\n",
        "            nc = back_translate_context(c, answer)\n",
        "            nq = back_translate(q)\n",
        "\n",
        "            print(c, file=forig)\n",
        "            print(q, file=forig)\n",
        "            print(answer, file=forig)\n",
        "            print(\"\", file=forig)\n",
        "\n",
        "            print(nc, file=fnew)\n",
        "            print(nq, file=fnew)\n",
        "            print(answer, file=fnew)\n",
        "            print(\"\", file=fnew)\n",
        "\n",
        "        j += 1\n",
        "        if j > 10: break\n",
        "\n",
        "    forig.close()\n",
        "    fnew.close()\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "fvfeN9ukg8Vr",
        "outputId": "e7fe6444-580d-4bc7-c091-b354cae90fd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(batch_size=16, do_eval=False, do_train=False, eval=False, eval_datasets='race,relation_extraction,duorc', eval_dir='datasets/oodomain_test', eval_every=5000, lr=3e-05, num_epochs=3, num_visuals=10, recompute_features=False, run_name='multitask_distilbert', save_dir='save/', seed=42, sub_file='', train=False, train_datasets='squad,nat_questions,newsqa', train_dir='datasets/indomain_train', val_dir='datasets/indomain_val', visualize_predictions=False)\n",
            "dict_keys(['question', 'context', 'id', 'answer'])\n",
            "--------------------------------------------------------------------------------\n",
            "111111111\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%tb"
      ],
      "metadata": {
        "id": "-HKWL1Pbm9dy",
        "outputId": "c9046c23-0b17-4988-a30e-cbbb5384feda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-aba7eec94dc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-aba7eec94dc7>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;31m# define parser and arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_test_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/args.py\u001b[0m in \u001b[0;36mget_train_test_args\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--visualize-predictions'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'store_true'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_argument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--eval-every'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mparse_args\u001b[0;34m(self, args, namespace)\u001b[0m\n\u001b[1;32m   1765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized arguments: %s'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1767\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1768\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1769\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m   2515\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2516\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'prog'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'message'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2517\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%(prog)s: error: %(message)s\\n'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.7/argparse.py\u001b[0m in \u001b[0;36mexit\u001b[0;34m(self, status, message)\u001b[0m\n\u001b[1;32m   2502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2503\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2504\u001b[0;31m         \u001b[0m_sys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemExit\u001b[0m: 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iNqTHMzMnT6F"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}