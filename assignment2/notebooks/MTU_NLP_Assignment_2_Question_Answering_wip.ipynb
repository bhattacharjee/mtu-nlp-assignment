{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MTU-NLP-Assignment-2-Question-Answering-wip",
      "provenance": [],
      "machine_shape": "hm",
      "toc_visible": true,
      "authorship_tag": "ABX9TyPhxcJVKWm5tgz9ASSOIcwp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment2/notebooks/MTU_NLP_Assignment_2_Question_Answering_wip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2nEpGzfbvwj-",
        "outputId": "0511e0cb-7d50-4e7f-987d-24d6baa9cedc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 12.8 MB 4.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 14 kB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 4.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 4.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 69 kB 3.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 70.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 75.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 124 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 461 kB 80.7 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install torchvision==0.8.2  -q -q -q\n",
        "!pip install torchtext==0.8.1    -q -q -q\n",
        "!pip install torchaudio==0.7.2   -q -q -q\n",
        "!pip install torch==1.7.1        -q -q -q\n",
        "!pip install tqdm==4.49.0        -q -q -q\n",
        "!pip install transformers==4.2.2 -q -q -q\n",
        "!pip install tensorflow          -q -q -q\n",
        "!pip install tensorboard         -q -q -q\n",
        "!pip install tensorboardX        -q -q -q\n",
        "!pip install --upgrade virtualenv -q -q -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# import shutil\n",
        "# drive.mount('/content/gdrive')\n",
        "# shutil.copy('/content/gdrive/MyDrive/MTUNLPA2/saved_pickle.tar.gz', '/tmp/saved_pickle.tar.gz')"
      ],
      "metadata": {
        "id": "daoDMfvJpfqa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!tar -xvzf /tmp/saved_pickle.tar.gz\n",
        "#!rm -f /tmp/saved_pickle.tar.gz"
      ],
      "metadata": {
        "id": "4jqbwyEls1AS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls saved_pickle\n",
        "#!du -sh saved_pickle\n",
        "#!pwd\n"
      ],
      "metadata": {
        "id": "95IyWg1-ujLC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bhattacharjee/mtu-nlp-assignment.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOK9thHC0rC1",
        "outputId": "5ca96265-3756-4c84-e78b-45f13b150678"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mtu-nlp-assignment'...\n",
            "remote: Enumerating objects: 869, done.\u001b[K\n",
            "remote: Counting objects: 100% (869/869), done.\u001b[K\n",
            "remote: Compressing objects: 100% (748/748), done.\u001b[K\n",
            "remote: Total 869 (delta 398), reused 343 (delta 117), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (869/869), 95.99 MiB | 60.79 MiB/s, done.\n",
            "Resolving deltas: 100% (398/398), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# args.py"
      ],
      "metadata": {
        "id": "454WJr-Rq5rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /tmp/args.py\n",
        "import argparse\n",
        "\n",
        "def get_train_test_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--batch-size', type=int, default=4)\n",
        "    parser.add_argument('--num-epochs', type=int, default=3)\n",
        "    parser.add_argument('--lr', type=float, default=3e-5)\n",
        "    parser.add_argument('--num-visuals', type=int, default=10)\n",
        "    parser.add_argument('--seed', type=int, default=42)\n",
        "    parser.add_argument('--save-dir', type=str, default='save/')\n",
        "    parser.add_argument('--train', action='store_true')\n",
        "    parser.add_argument('--eval', action='store_true')\n",
        "    parser.add_argument('--train-datasets', type=str, default='squad,nat_questions,newsqa')\n",
        "    parser.add_argument('--run-name', type=str, default='multitask_distilbert')\n",
        "    parser.add_argument('--recompute-features', action='store_true')\n",
        "    parser.add_argument('--train-dir', type=str, default='datasets/indomain_train')\n",
        "    parser.add_argument('--val-dir', type=str, default='datasets/indomain_val')\n",
        "    parser.add_argument('--eval-dir', type=str, default='datasets/oodomain_test')\n",
        "    parser.add_argument('--eval-datasets', type=str, default='race,relation_extraction,duorc')\n",
        "    parser.add_argument('--do-train', action='store_true')\n",
        "    parser.add_argument('--do-eval', action='store_true')\n",
        "    parser.add_argument('--sub-file', type=str, default='')\n",
        "    parser.add_argument('--visualize-predictions', action='store_true')\n",
        "    parser.add_argument('--eval-every', type=int, default=5000)\n",
        "    args = parser.parse_args()\n",
        "    return args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr1LRFt1GAUJ",
        "outputId": "dc54b98d-530d-4f70-887f-159a8f6bf6e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /tmp/args.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# chunkdata.py"
      ],
      "metadata": {
        "id": "j_3XY82PI7uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /tmp/ChunkData.py\n",
        "\n",
        "#!/usr/bin/env python3\n",
        "\n",
        "# Copyright 2021 Rajbir Bhattacharjee\n",
        "#\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "# \n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "#\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "# SOFTWARE.\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import pathlib\n",
        "import pickle\n",
        "import json\n",
        "\n",
        "import threading\n",
        "\n",
        "class ChunkDataCommon():\n",
        "    def __init__(self, directory:str, chunk_size=16):\n",
        "        self.directory = directory\n",
        "        self.chunk_size = chunk_size\n",
        "\n",
        "    def get_pickle_file_name(self, st, en):\n",
        "        return f\"{self.directory}/array_chunk-{st:08d}-{en:08d}.pickle\"\n",
        "\n",
        "    def get_conf_file_name(self):\n",
        "        with self.lock:\n",
        "            return f\"{self.directory}/config.json\"\n",
        "\n",
        "    def get_slice_start_for_index(self, ind):\n",
        "        return (ind // self.chunk_size) * self.chunk_size\n",
        "\n",
        "    def load_slice(self, ind_start):\n",
        "        with self.lock:\n",
        "            ind_end = ind_start + self.chunk_size - 1\n",
        "            filename =self.get_pickle_file_name(ind_start, ind_end)\n",
        "            with open(filename, \"rb\") as f:\n",
        "                self.current_slice = pickle.load(f)\n",
        "                self.current_slice_start = ind_start\n",
        "                return\n",
        "            self.current_slice = None\n",
        "            self.current_slice_start = -1\n",
        "            raise IndexError\n",
        "\n",
        "\n",
        "class ChunkDataWriter(ChunkDataCommon):\n",
        "    \"\"\"Uses pickle, but splits into multiple files.\n",
        "    behaves like an array and supports the append() method,\n",
        "    and finalize method()\"\"\"\n",
        "    def __init__(self, directory:str, chunk_size=16):\n",
        "        super(self.__class__, self).__init__(directory, chunk_size)\n",
        "        self.lock = threading.RLock()\n",
        "\n",
        "        # the current slice of the array\n",
        "        self.current_slice = list()\n",
        "\n",
        "        # Length of all items\n",
        "        self.length = 0\n",
        "\n",
        "        # Where does the current slice start from\n",
        "        self.current_slice_start = -1\n",
        "\n",
        "\n",
        "        if not os.path.exists(self.directory):\n",
        "            pathlib.Path(self.directory).mkdir(parents=True, exist_ok=False)\n",
        "\n",
        "        if os.path.exists(self.get_conf_file_name()):\n",
        "            self.load_existing()\n",
        "        else:\n",
        "            self.write_conf()\n",
        "\n",
        "    def load_existing(self):\n",
        "        with open(self.get_conf_file_name(), \"r\") as f:\n",
        "            d = json.load(f)\n",
        "            self.length = d['length']\n",
        "            self.length = self.length if self.length > 0 else 0\n",
        "            if self.length > 0:\n",
        "                ind_start = self.get_slice_start_for_index(self.length - 1)\n",
        "                self.load_slice(ind_start)\n",
        "                self.current_slice_start = ind_start\n",
        "\n",
        "    def finalize(self):\n",
        "        #print(f\"Finalize: {self.directory} - Length: {self.length}\")\n",
        "        self.write_conf()\n",
        "\n",
        "    def get_current_slice_max(self):\n",
        "        # Return the index of the last element that can\n",
        "        # be stored in the current slice\n",
        "        with self.lock:\n",
        "            if self.current_slice_start == -1:\n",
        "                return -1\n",
        "            else:\n",
        "                return self.current_slice_start + self.chunk_size - 1\n",
        "\n",
        "    def get_current_slice_end(self):\n",
        "        # Return the end of the current slice\n",
        "        with self.lock:\n",
        "            if self.current_slice_start == -1:\n",
        "                return -1\n",
        "            else:\n",
        "                return self.current_slice_start + len(self.current_slice)\n",
        "\n",
        "    def get_current_slice_size(self):\n",
        "        # return the size of the current slice\n",
        "        with self.lock:\n",
        "            return len(self.current_slice)\n",
        "\n",
        "    def get_current_slice_offsets(self):\n",
        "        with self.lock:\n",
        "            return self.current_slice_start, \\\n",
        "                self.get_current_slice_end(), \\\n",
        "                self.get_current_slice_max()\n",
        "\n",
        "    def get_current_file_name(self):\n",
        "        with self.lock:\n",
        "            if -1 == self.current_slice_start:\n",
        "                return None\n",
        "            st = self.current_slice_start\n",
        "            en = st + self.chunk_size - 1\n",
        "            return self.get_pickle_file_name(st, en)\n",
        "\n",
        "    def write_chunk(self):\n",
        "        with self.lock:\n",
        "            with open(self.get_current_file_name(), \"wb\") as f:\n",
        "                pickle.dump(self.current_slice, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "\n",
        "    def write_conf(self):\n",
        "        with self.lock:\n",
        "            thedict = {}\n",
        "            thedict[\"length\"] = self.length\n",
        "            #print(\"Exists?\", os.path.exists(self.directory))\n",
        "            with open(self.get_conf_file_name(), \"w\") as f:\n",
        "                json.dump(thedict, f)\n",
        "\n",
        "    def append(self, x):\n",
        "        with self.lock:\n",
        "            if 0 == len(self.current_slice) % self.chunk_size:\n",
        "                self.current_slice = list()\n",
        "                if self.current_slice_start != -1:\n",
        "                    self.current_slice_start += self.chunk_size\n",
        "            if self.current_slice_start == -1:\n",
        "                self.current_slice_start = 0\n",
        "            self.length += 1\n",
        "            self.current_slice.append(x)\n",
        "            self.write_chunk()\n",
        "            self.write_conf()\n",
        "        pass\n",
        "\n",
        "    def __del__(self):\n",
        "        with self.lock:\n",
        "            self.write_conf()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length if self.length >= 0 else 0\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        try:\n",
        "            reader = ChunkDataReader(self)\n",
        "        except Exception as e:\n",
        "            raise IndexError\n",
        "        return reader[ind]\n",
        "\n",
        "\n",
        "\n",
        "class ChunkDataReader(ChunkDataCommon):\n",
        "    def __init__(self, d, chunk_size=16):\n",
        "        super(self.__class__, self).__init__(d, chunk_size=None)\n",
        "\n",
        "        self.lock = threading.RLock()\n",
        "        self.length = -1\n",
        "\n",
        "        if isinstance(d, str):\n",
        "            directory = d\n",
        "        elif isinstance(d, ChunkDataReader) or isinstance(d, ChunkDataWriter):\n",
        "            directory = d.directory\n",
        "            chunk_size = d.chunk_size\n",
        "\n",
        "        self.current_slice = None\n",
        "        self.current_slice_start = -1\n",
        "        self.chunk_size = chunk_size\n",
        "        self.directory = directory\n",
        "\n",
        "        with open(self.get_conf_file_name(), \"r\") as f:\n",
        "            conf = json.load(f)\n",
        "            length = conf['length']\n",
        "            self.length = length if length > 0 else -1\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length if self.length > 0 else 0\n",
        "\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "        if ind >= self.length:\n",
        "            raise IndexError\n",
        "        if (ind < 0):\n",
        "            ind = self.length + ind \n",
        "        with self.lock:\n",
        "            ind_start = self.get_slice_start_for_index(ind)\n",
        "            if ind_start != self.current_slice_start:\n",
        "                self.load_slice(ind_start)\n",
        "            ind = ind - self.current_slice_start\n",
        "            return self.current_slice[ind]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzaXb5IjI5sB",
        "outputId": "0a183545-afa7-4337-83f5-e1a56bf7e0ca"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /tmp/ChunkData.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# util.py"
      ],
      "metadata": {
        "id": "t444EYHjrbrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /tmp/util.py\n",
        "\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "import logging\n",
        "import pickle\n",
        "import string\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import Counter, OrderedDict, defaultdict as ddict\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "from ChunkData import *\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def load_pickle(path):\n",
        "    with open(path, 'rb') as f:\n",
        "        obj = pickle.load(f)\n",
        "    return obj\n",
        "\n",
        "def save_pickle(obj, path):\n",
        "    \"\"\"\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "    \"\"\"\n",
        "    return\n",
        "\n",
        "def visualize(tbx, pred_dict, gold_dict, step, split, num_visuals):\n",
        "    \"\"\"Visualize text examples to TensorBoard.\n",
        "\n",
        "    Args:\n",
        "        tbx (tensorboardX.SummaryWriter): Summary writer.\n",
        "        pred_dict (dict): dict of predictions of the form id -> pred.\n",
        "        step (int): Number of examples seen so far during training.\n",
        "        split (str): Name of data split being visualized.\n",
        "        num_visuals (int): Number of visuals to select at random from preds.\n",
        "    \"\"\"\n",
        "    if num_visuals <= 0:\n",
        "        return\n",
        "    if num_visuals > len(pred_dict):\n",
        "        num_visuals = len(pred_dict)\n",
        "    id2index = {curr_id : idx for idx, curr_id in enumerate(gold_dict['id'])}\n",
        "    visual_ids = np.random.choice(list(pred_dict), size=num_visuals, replace=False)\n",
        "    for i, id_ in enumerate(visual_ids):\n",
        "        pred = pred_dict[id_] or 'N/A'\n",
        "        idx_gold_dict = id2index[id_]\n",
        "        question = gold_dict['question'][idx_gold_dict]\n",
        "        context = gold_dict['context'][idx_gold_dict]\n",
        "        answers = gold_dict['answer'][idx_gold_dict]\n",
        "        gold = answers['text'][0] if answers else 'N/A'\n",
        "        tbl_fmt = (f'- **Question:** {question}\\n'\n",
        "                   + f'- **Context:** {context}\\n'\n",
        "                   + f'- **Answer:** {gold}\\n'\n",
        "                   + f'- **Prediction:** {pred}')\n",
        "        tbx.add_text(tag=f'{split}/{i+1}_of_{num_visuals}',\n",
        "                     text_string=tbl_fmt,\n",
        "                     global_step=step)\n",
        "\n",
        "\n",
        "def get_save_dir(base_dir, name, id_max=100):\n",
        "    for uid in range(1, id_max):\n",
        "        save_dir = os.path.join(base_dir, f'{name}-{uid:02d}')\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "            return save_dir\n",
        "\n",
        "    raise RuntimeError('Too many save directories created with the same name. \\\n",
        "                       Delete old save directories or use another name.')\n",
        "\n",
        "\n",
        "def filter_encodings(encodings):\n",
        "    filter_idx = [idx for idx, val in enumerate(encodings['end_positions'])\n",
        "                 if not val]\n",
        "    filter_idx = set(filter_idx)\n",
        "    encodings_filtered = {key : [] for key in encodings}\n",
        "    sz = len(encodings['input_ids'])\n",
        "    for idx in range(sz):\n",
        "        if idx not in filter_idx:\n",
        "            for key in encodings:\n",
        "                encodings_filtered[key].append(encodings[key][idx])\n",
        "    return encodings_filtered\n",
        "\n",
        "def merge(encodings, new_encoding):\n",
        "    if not encodings:\n",
        "        return new_encoding\n",
        "    else:\n",
        "        for key in new_encoding:\n",
        "            encodings[key] += new_encoding[key]\n",
        "        return encodings\n",
        "\n",
        "def get_logger(log_dir, name):\n",
        "    \"\"\"Get a `logging.Logger` instance that prints to the console\n",
        "    and an auxiliary file.\n",
        "\n",
        "    Args:\n",
        "        log_dir (str): Directory in which to create the log file.\n",
        "        name (str): Name to identify the logs.\n",
        "\n",
        "    Returns:\n",
        "        logger (logging.Logger): Logger instance for logging events.\n",
        "    \"\"\"\n",
        "    class StreamHandlerWithTQDM(logging.Handler):\n",
        "        \"\"\"Let `logging` print without breaking `tqdm` progress bars.\n",
        "\n",
        "        See Also:\n",
        "            > https://stackoverflow.com/questions/38543506\n",
        "        \"\"\"\n",
        "        def emit(self, record):\n",
        "            try:\n",
        "                msg = self.format(record)\n",
        "                tqdm.write(msg)\n",
        "                self.flush()\n",
        "            except (KeyboardInterrupt, SystemExit):\n",
        "                raise\n",
        "            except:\n",
        "                self.handleError(record)\n",
        "\n",
        "    # Create logger\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "\n",
        "    # Log everything (i.e., DEBUG level and above) to a file\n",
        "    log_path = os.path.join(log_dir, f'{name}.txt')\n",
        "    file_handler = logging.FileHandler(log_path)\n",
        "    file_handler.setLevel(logging.DEBUG)\n",
        "\n",
        "    # Log everything except DEBUG level (i.e., INFO level and above) to console\n",
        "    console_handler = StreamHandlerWithTQDM()\n",
        "    console_handler.setLevel(logging.INFO)\n",
        "\n",
        "    # Create format for the logs\n",
        "    file_formatter = logging.Formatter('[%(asctime)s] %(message)s',\n",
        "                                       datefmt='%m.%d.%y %H:%M:%S')\n",
        "    file_handler.setFormatter(file_formatter)\n",
        "    console_formatter = logging.Formatter('[%(asctime)s] %(message)s',\n",
        "                                          datefmt='%m.%d.%y %H:%M:%S')\n",
        "    console_handler.setFormatter(console_formatter)\n",
        "\n",
        "    # add the handlers to the logger\n",
        "    logger.addHandler(file_handler)\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"Keep track of average values over time.\n",
        "\n",
        "    Adapted from:\n",
        "        > https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset meter.\"\"\"\n",
        "        self.__init__()\n",
        "\n",
        "    def update(self, val, num_samples=1):\n",
        "        \"\"\"Update meter with new value `val`, the average of `num` samples.\n",
        "\n",
        "        Args:\n",
        "            val (float): Average value to update the meter with.\n",
        "            num_samples (int): Number of samples that were averaged to\n",
        "                produce `val`.\n",
        "        \"\"\"\n",
        "        self.count += num_samples\n",
        "        self.sum += val * num_samples\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, encodings, train=True):\n",
        "        self.encodings = encodings\n",
        "        self.keys = ['input_ids', 'attention_mask']\n",
        "        if train:\n",
        "            self.keys += ['start_positions', 'end_positions']\n",
        "        assert(all(key in self.encodings for key in self.keys))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key : torch.tensor(self.encodings[key][idx]) for key in self.keys}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "def read_squad(path):\n",
        "    path = Path(path)\n",
        "    with open(path, 'rb') as f:\n",
        "        squad_dict = json.load(f)\n",
        "    data_dict = {'question': [], 'context': [], 'id': [], 'answer': []}\n",
        "    for group in squad_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                if len(qa['answers']) == 0:\n",
        "                    data_dict['question'].append(question)\n",
        "                    data_dict['context'].append(context)\n",
        "                    data_dict['id'].append(qa['id'])\n",
        "                else:\n",
        "                    for answer in  qa['answers']:\n",
        "                        data_dict['question'].append(question)\n",
        "                        data_dict['context'].append(context)\n",
        "                        data_dict['id'].append(qa['id'])\n",
        "                        data_dict['answer'].append(answer)\n",
        "    id_map = ddict(list)\n",
        "    for idx, qid in enumerate(data_dict['id']):\n",
        "        id_map[qid].append(idx)\n",
        "\n",
        "    data_dict_collapsed = {'question': [], 'context': [], 'id': []}\n",
        "    if data_dict['answer']:\n",
        "        data_dict_collapsed['answer'] = []\n",
        "    for qid in id_map:\n",
        "        ex_ids = id_map[qid]\n",
        "        data_dict_collapsed['question'].append(data_dict['question'][ex_ids[0]])\n",
        "        data_dict_collapsed['context'].append(data_dict['context'][ex_ids[0]])\n",
        "        data_dict_collapsed['id'].append(qid)\n",
        "        if data_dict['answer']:\n",
        "            all_answers = [data_dict['answer'][idx] for idx in ex_ids]\n",
        "            data_dict_collapsed['answer'].append({'answer_start': [answer['answer_start'] for answer in all_answers],\n",
        "                                                  'text': [answer['text'] for answer in all_answers]})\n",
        "    return data_dict_collapsed\n",
        "\n",
        "def add_token_positions(encodings, answers, tokenizer):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
        "\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "        # if end position is None, the 'char_to_token' function points to the space before the correct token - > add + 1\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] + 1)\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "\n",
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        gold_text = answer['text']\n",
        "        start_idx = answer['answer_start']\n",
        "        end_idx = start_idx + len(gold_text)\n",
        "\n",
        "        # sometimes squad answers are off by a character or two – fix this\n",
        "        if context[start_idx:end_idx] == gold_text:\n",
        "            answer['answer_end'] = end_idx\n",
        "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 1\n",
        "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
        "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 2\n",
        "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
        "\n",
        "def convert_tokens(eval_dict, qa_id, y_start_list, y_end_list):\n",
        "    \"\"\"Convert predictions to tokens from the context.\n",
        "\n",
        "    Args:\n",
        "        eval_dict (dict): Dictionary with eval info for the dataset. This is\n",
        "            used to perform the mapping from IDs and indices to actual text.\n",
        "        qa_id (int): List of QA example IDs.\n",
        "        y_start_list (list): List of start predictions.\n",
        "        y_end_list (list): List of end predictions.\n",
        "        no_answer (bool): Questions can have no answer. E.g., SQuAD 2.0.\n",
        "\n",
        "    Returns:\n",
        "        pred_dict (dict): Dictionary index IDs -> predicted answer text.\n",
        "        sub_dict (dict): Dictionary UUIDs -> predicted answer text (submission).\n",
        "    \"\"\"\n",
        "    pred_dict = {}\n",
        "    sub_dict = {}\n",
        "    for qid, y_start, y_end in zip(qa_id, y_start_list, y_end_list):\n",
        "        context = eval_dict[str(qid)][\"context\"]\n",
        "        spans = eval_dict[str(qid)][\"spans\"]\n",
        "        uuid = eval_dict[str(qid)][\"uuid\"]\n",
        "        start_idx = spans[y_start][0]\n",
        "        end_idx = spans[y_end][1]\n",
        "        pred_dict[str(qid)] = context[start_idx: end_idx]\n",
        "        sub_dict[uuid] = context[start_idx: end_idx]\n",
        "    return pred_dict, sub_dict\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    if not ground_truths:\n",
        "        return metric_fn(prediction, '')\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def eval_dicts(gold_dict, pred_dict):\n",
        "    avna = f1 = em = total = 0\n",
        "    id2index = {curr_id : idx for idx, curr_id in enumerate(gold_dict['id'])}\n",
        "    for curr_id in pred_dict:\n",
        "        total += 1\n",
        "        index = id2index[curr_id]\n",
        "        ground_truths = gold_dict['answer'][index]['text']\n",
        "        prediction = pred_dict[curr_id]\n",
        "        em += metric_max_over_ground_truths(compute_em, prediction, ground_truths)\n",
        "        f1 += metric_max_over_ground_truths(compute_f1, prediction, ground_truths)\n",
        "\n",
        "    eval_dict = {'EM': 100. * em / total,\n",
        "                 'F1': 100. * f1 / total}\n",
        "    return eval_dict\n",
        "\n",
        "def postprocess_qa_predictions(examples, features, predictions,\n",
        "                               n_best_size=20, max_answer_length=30):\n",
        "    all_start_logits, all_end_logits = predictions\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = ddict(list)\n",
        "    for i, feat_id in enumerate(features['id']):\n",
        "        features_per_example[example_id_to_index[feat_id]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    all_predictions = OrderedDict()\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index in tqdm(range(len(examples['id']))):\n",
        "        example = {key : examples[key][example_index] for key in examples}\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "        prelim_predictions = []\n",
        "\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            seq_ids = features.sequence_ids(feature_index)\n",
        "            non_pad_idx = len(seq_ids) - 1\n",
        "            while not seq_ids[non_pad_idx]:\n",
        "                non_pad_idx -= 1\n",
        "            start_logits = start_logits[:non_pad_idx]\n",
        "            end_logits = end_logits[:non_pad_idx]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[\"offset_mapping\"][feature_index]\n",
        "            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n",
        "            # available in the current feature.\n",
        "            token_is_max_context = features.get(\"token_is_max_context\", None)\n",
        "            if token_is_max_context:\n",
        "                token_is_max_context = token_is_max_context[feature_index]\n",
        "\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either = 0 or > max_answer_length.\n",
        "                    if end_index <= start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "                    # Don't consider answer that don't have the maximum context available (if such information is\n",
        "                    # provided).\n",
        "                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n",
        "                        continue\n",
        "                    prelim_predictions.append(\n",
        "                        {\n",
        "                            \"start_index\": start_index,\n",
        "                            \"end_index\": end_index,\n",
        "                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"start_logit\": start_logits[start_index],\n",
        "                            \"end_logit\": end_logits[end_index],\n",
        "                        }\n",
        "                    )\n",
        "        # Only keep the best `n_best_size` predictions.\n",
        "        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
        "\n",
        "        # Use the offsets to gather the answer text in the original context.\n",
        "        context = example[\"context\"]\n",
        "        for pred in predictions:\n",
        "            offsets = pred['offsets']\n",
        "            pred[\"text\"] = context[offsets[0] : offsets[1]]\n",
        "\n",
        "        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "        # failure.\n",
        "        if len(predictions) == 0:\n",
        "            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n",
        "\n",
        "        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n",
        "        # the LogSumExp trick).\n",
        "        scores = np.array([pred.pop(\"score\") for pred in predictions])\n",
        "        exp_scores = np.exp(scores - np.max(scores))\n",
        "        probs = exp_scores / exp_scores.sum()\n",
        "\n",
        "        # Include the probabilities in our predictions.\n",
        "        for prob, pred in zip(probs, predictions):\n",
        "            pred[\"probability\"] = prob\n",
        "\n",
        "        # need to find the best non-empty prediction.\n",
        "        i = 0\n",
        "        while i < len(predictions):\n",
        "            if predictions[i]['text'] != '':\n",
        "                break\n",
        "            i += 1\n",
        "        if i == len(predictions):\n",
        "            import pdb; pdb.set_trace();\n",
        "\n",
        "        best_non_null_pred = predictions[i]\n",
        "        all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "\n",
        "\n",
        "# All methods below this line are from the official SQuAD 2.0 eval script\n",
        "# https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Convert to lowercase and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "        return re.sub(regex, ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "    if not s:\n",
        "        return []\n",
        "    return normalize_answer(s).split()\n",
        "\n",
        "def compute_em(a_gold, a_pred):\n",
        "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = Counter(gold_toks) & Counter(pred_toks)\n",
        "    num_same = sum(common.values())\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "        return int(gold_toks == pred_toks)\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(pred_toks)\n",
        "    recall = 1.0 * num_same / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn72CZjoq-zB",
        "outputId": "fcdbe5b3-01e8-4302-fc3f-24d128a90865"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /tmp/util.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train.py"
      ],
      "metadata": {
        "id": "nU1kuEaIq8O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /tmp/train.py\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import csv\n",
        "import util\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import DistilBertForQuestionAnswering\n",
        "from transformers import AdamW\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
        "from args import get_train_test_args\n",
        "\n",
        "from tqdm import tqdm\n",
        "from ChunkData import *\n",
        "\n",
        "#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n",
        "def prepare_eval_data(dataset_dict, tokenizer):\n",
        "    tokenized_examples = tokenizer(dataset_dict['question'],\n",
        "                                   dataset_dict['context'],\n",
        "                                   truncation=\"only_second\",\n",
        "                                   stride=128,\n",
        "                                   max_length=384,\n",
        "                                   return_overflowing_tokens=True,\n",
        "                                   return_offsets_mapping=True,\n",
        "                                   padding='max_length')\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
        "    # corresponding example_id and we will store the offset mappings.\n",
        "    tokenized_examples[\"id\"] = []\n",
        "    for i in tqdm(range(len(tokenized_examples[\"input_ids\"]))):\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"id\"].append(dataset_dict[\"id\"][sample_index])\n",
        "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "        # position is part of the context or not.\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == 1 else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "\n",
        "def prepare_train_data(dataset_dict, tokenizer, remove_data=False):\n",
        "    import gc\n",
        "    import shutil\n",
        "    tokenized_examples = tokenizer(dataset_dict['question'],\n",
        "                                   dataset_dict['context'],\n",
        "                                   truncation=\"only_second\",\n",
        "                                   stride=128,\n",
        "                                   max_length=384,\n",
        "                                   return_overflowing_tokens=True,\n",
        "                                   return_offsets_mapping=True,\n",
        "                                   padding='max_length')\n",
        "    orig_tokenized_examples = tokenized_examples\n",
        "    sample_mapping = orig_tokenized_examples[\"overflow_to_sample_mapping\"]\n",
        "    offset_mapping = orig_tokenized_examples[\"offset_mapping\"]\n",
        "\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "    tokenized_examples['id'] = []\n",
        "\n",
        "    temp_tokenized_examples = tokenized_examples\n",
        "    if remove_data:\n",
        "        for c in temp_tokenized_examples.keys():\n",
        "            try:\n",
        "                shutil.rmtree(f\"/tmp/train/{str(c)}\")\n",
        "            except:\n",
        "                print(\"Could not remove \", c)\n",
        "\n",
        "    # TODO: RAJBIR: my additions, not sure if they work\n",
        "    tokenized_examples = {str(c): ChunkDataWriter(f\"/tmp/train/{str(c)}\") for c in temp_tokenized_examples.keys()}\n",
        "\n",
        "    for key in temp_tokenized_examples.keys():\n",
        "        key = str(key)\n",
        "        if key not in [\"start_positions\", \"end_positions\", \"id\", \"sequence_ids\", \"overflow_to_sample_mapping\", \"offset_mapping\"]:\n",
        "            v_from = temp_tokenized_examples[key]\n",
        "            v_to = tokenized_examples[key]\n",
        "            #print(f\"Copying {key}... {len(v_from)} {type(v_from)}\")\n",
        "            for i in range(len(v_from)):\n",
        "                v_to.append(v_from[i])\n",
        "                if key not in [\"offset_mapping\", \"overflow_mapping\"]:\n",
        "                    v_from[i] = None\n",
        "                    gc.collect(0)\n",
        "                    gc.collect(1)\n",
        "                    gc.collect(2)\n",
        "            temp_tokenized_examples[key] = None\n",
        "            gc.collect(0)\n",
        "            gc.collect(1)\n",
        "            gc.collect(2)\n",
        "            # print(\"done\")\n",
        "            v_to.finalize()\n",
        "\n",
        "    inaccurate = 0\n",
        "    for i, offsets in tqdm(enumerate(tqdm(offset_mapping)), \"Inner process\"):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = orig_tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answer = dataset_dict['answer'][sample_index]\n",
        "        # Start/end character index of the answer in the text.\n",
        "        start_char = answer['answer_start'][0]\n",
        "        end_char = start_char + len(answer['text'][0])\n",
        "        tokenized_examples['id'].append(dataset_dict['id'][sample_index])\n",
        "        # Start token index of the current span in the text.\n",
        "        token_start_index = 0\n",
        "        while sequence_ids[token_start_index] != 1:\n",
        "            token_start_index += 1\n",
        "\n",
        "        # End token index of the current span in the text.\n",
        "        token_end_index = len(input_ids) - 1\n",
        "        while sequence_ids[token_end_index] != 1:\n",
        "            token_end_index -= 1\n",
        "\n",
        "        # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "            # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                token_start_index += 1\n",
        "            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "            while offsets[token_end_index][1] >= end_char:\n",
        "                token_end_index -= 1\n",
        "            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "            # assertion to check if this checks out\n",
        "            context = dataset_dict['context'][sample_index]\n",
        "            offset_st = offsets[tokenized_examples['start_positions'][-1]][0]\n",
        "            offset_en = offsets[tokenized_examples['end_positions'][-1]][1]\n",
        "            if context[offset_st : offset_en] != answer['text'][0]:\n",
        "                inaccurate += 1\n",
        "\n",
        "    total = len(tokenized_examples['id'])\n",
        "    # print(f\"Preprocessing not completely accurate for {inaccurate}/{total} instances\")\n",
        "    # print([key for key in tokenized_examples.keys()])\n",
        "    # TODO: RAJBIR: my additions, not sure if they work\n",
        "    tokenized_examples = {k: ChunkDataReader(v) for k, v in tokenized_examples.items()}\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "def read_and_process(args, tokenizer, dataset_dict, dir_name, dataset_name, split):\n",
        "    #TODO: cache this if possible\n",
        "    cache_path = f'{dir_name}/{dataset_name}_encodings.pt'\n",
        "    if os.path.exists(cache_path) and not args.recompute_features:\n",
        "        tokenized_examples = util.load_pickle(cache_path)\n",
        "    else:\n",
        "        if split=='train':\n",
        "            tokenized_examples = prepare_train_data(dataset_dict, tokenizer)\n",
        "        else:\n",
        "            tokenized_examples = prepare_eval_data(dataset_dict, tokenizer)\n",
        "        util.save_pickle(tokenized_examples, cache_path)\n",
        "    return tokenized_examples\n",
        "\"\"\"\n",
        "\n",
        "# TODO: RAJBIR: Test code for only 50 items\n",
        "def read_and_process(args, tokenizer, dataset_dict, dir_name, dataset_name, split):\n",
        "    #TODO: cache this if possible\n",
        "    def get_dataset_slice(dataset_dict, slice_start, slice_length):\n",
        "        out_dict = {k: [] for k in dataset_dict.keys()}\n",
        "        for k in dataset_dict.keys():\n",
        "            arr = dataset_dict[k]\n",
        "            out_dict[k] = arr[slice_start:slice_start+slice_length]\n",
        "        return out_dict\n",
        "\n",
        "    def get_dataset_length(dataset_dict):\n",
        "        column = [c for c in dataset_dict.keys()][0]\n",
        "        return len(dataset_dict[column])\n",
        "\n",
        "    cache_path = f'{dir_name}/{dataset_name}_encodings.pt'\n",
        "    for key in dataset_dict.keys():\n",
        "        dataset_dict[key] = dataset_dict[key]\n",
        "    if split=='train':\n",
        "        cleardir = True\n",
        "        stride = 512\n",
        "        for i in tqdm(range(0, get_dataset_length(dataset_dict), stride), \"Preparing training data\"):\n",
        "            temp_dict = get_dataset_slice(dataset_dict, i, stride)\n",
        "            tokenized_examples = prepare_train_data(temp_dict, tokenizer, cleardir)\n",
        "            cleardir = False\n",
        "            # for c in tokenized_examples.keys():\n",
        "            #     print(c, len(tokenized_examples[str(c)]))\n",
        "            # print('-' * 80)\n",
        "    else:\n",
        "        tokenized_examples = prepare_eval_data(dataset_dict, tokenizer)\n",
        "    util.save_pickle(tokenized_examples, cache_path)\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#TODO: use a logger, use tensorboard\n",
        "class Trainer():\n",
        "    def __init__(self, args, log):\n",
        "        self.lr = args.lr\n",
        "        self.num_epochs = args.num_epochs\n",
        "        self.device = args.device\n",
        "        self.eval_every = args.eval_every\n",
        "        self.path = os.path.join(args.save_dir, 'checkpoint')\n",
        "        self.num_visuals = args.num_visuals\n",
        "        self.save_dir = args.save_dir\n",
        "        self.log = log\n",
        "        self.visualize_predictions = args.visualize_predictions\n",
        "        if not os.path.exists(self.path):\n",
        "            os.makedirs(self.path)\n",
        "\n",
        "    def save(self, model):\n",
        "        model.save_pretrained(self.path)\n",
        "\n",
        "    def evaluate(self, model, data_loader, data_dict, return_preds=False, split='validation'):\n",
        "        device = self.device\n",
        "\n",
        "        model.eval()\n",
        "        pred_dict = {}\n",
        "        all_start_logits = []\n",
        "        all_end_logits = []\n",
        "        with torch.no_grad(), \\\n",
        "                tqdm(total=len(data_loader.dataset)) as progress_bar:\n",
        "            for batch in data_loader:\n",
        "                # Setup for forward\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                batch_size = len(input_ids)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                # Forward\n",
        "                start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
        "                # TODO: compute loss\n",
        "\n",
        "                all_start_logits.append(start_logits)\n",
        "                all_end_logits.append(end_logits)\n",
        "                progress_bar.update(batch_size)\n",
        "\n",
        "        # Get F1 and EM scores\n",
        "        start_logits = torch.cat(all_start_logits).cpu().numpy()\n",
        "        end_logits = torch.cat(all_end_logits).cpu().numpy()\n",
        "        preds = util.postprocess_qa_predictions(data_dict,\n",
        "                                                 data_loader.dataset.encodings,\n",
        "                                                 (start_logits, end_logits))\n",
        "        if split == 'validation':\n",
        "            results = util.eval_dicts(data_dict, preds)\n",
        "            results_list = [('F1', results['F1']),\n",
        "                            ('EM', results['EM'])]\n",
        "        else:\n",
        "            results_list = [('F1', -1.0),\n",
        "                            ('EM', -1.0)]\n",
        "        results = OrderedDict(results_list)\n",
        "        if return_preds:\n",
        "            return preds, results\n",
        "        return results\n",
        "\n",
        "    def train(self, model, train_dataloader, eval_dataloader, val_dict):\n",
        "        device = self.device\n",
        "        model.to(device)\n",
        "        optim = AdamW(model.parameters(), lr=self.lr)\n",
        "        global_idx = 0\n",
        "        best_scores = {'F1': -1.0, 'EM': -1.0}\n",
        "        tbx = SummaryWriter(self.save_dir)\n",
        "\n",
        "        for epoch_num in range(self.num_epochs):\n",
        "            self.log.info(f'Epoch: {epoch_num}')\n",
        "            with torch.enable_grad(), tqdm(total=len(train_dataloader.dataset)) as progress_bar:\n",
        "                for batch in train_dataloader:\n",
        "                    optim.zero_grad()\n",
        "                    model.train()\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    start_positions = batch['start_positions'].to(device)\n",
        "                    end_positions = batch['end_positions'].to(device)\n",
        "                    outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                                    start_positions=start_positions,\n",
        "                                    end_positions=end_positions)\n",
        "                    loss = outputs[0]\n",
        "                    loss.backward()\n",
        "                    optim.step()\n",
        "                    progress_bar.update(len(input_ids))\n",
        "                    progress_bar.set_postfix(epoch=epoch_num, NLL=loss.item())\n",
        "                    tbx.add_scalar('train/NLL', loss.item(), global_idx)\n",
        "                    if (global_idx % self.eval_every) == 0:\n",
        "                        self.log.info(f'Evaluating at step {global_idx}...')\n",
        "                        preds, curr_score = self.evaluate(model, eval_dataloader, val_dict, return_preds=True)\n",
        "                        results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in curr_score.items())\n",
        "                        self.log.info('Visualizing in TensorBoard...')\n",
        "                        for k, v in curr_score.items():\n",
        "                            tbx.add_scalar(f'val/{k}', v, global_idx)\n",
        "                        self.log.info(f'Eval {results_str}')\n",
        "                        if self.visualize_predictions:\n",
        "                            util.visualize(tbx,\n",
        "                                           pred_dict=preds,\n",
        "                                           gold_dict=val_dict,\n",
        "                                           step=global_idx,\n",
        "                                           split='val',\n",
        "                                           num_visuals=self.num_visuals)\n",
        "                        if curr_score['F1'] >= best_scores['F1']:\n",
        "                            best_scores = curr_score\n",
        "                            self.save(model)\n",
        "                    global_idx += 1\n",
        "        return best_scores\n",
        "\n",
        "def get_dataset(args, datasets, data_dir, tokenizer, split_name):\n",
        "    datasets = datasets.split(',')\n",
        "    dataset_dict = None\n",
        "    dataset_name=''\n",
        "    for dataset in datasets:\n",
        "        dataset_name += f'_{dataset}'\n",
        "        dataset_dict_curr = util.read_squad(f'{data_dir}/{dataset}')\n",
        "        dataset_dict = util.merge(dataset_dict, dataset_dict_curr)\n",
        "    data_encodings = read_and_process(args, tokenizer, dataset_dict, data_dir, dataset_name, split_name)\n",
        "    return util.QADataset(data_encodings, train=(split_name=='train')), dataset_dict\n",
        "\n",
        "def main():\n",
        "    # define parser and arguments\n",
        "    args = get_train_test_args()\n",
        "\n",
        "    util.set_seed(args.seed)\n",
        "    model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    if args.do_train:\n",
        "        if not os.path.exists(args.save_dir):\n",
        "            os.makedirs(args.save_dir)\n",
        "        args.save_dir = util.get_save_dir(args.save_dir, args.run_name)\n",
        "        log = util.get_logger(args.save_dir, 'log_train')\n",
        "        log.info(f'Args: {json.dumps(vars(args), indent=4, sort_keys=True)}')\n",
        "        log.info(\"Preparing Training Data...\")\n",
        "        args.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        if torch.cuda.is_available():\n",
        "            print(\"USING CUDA !!!!\")\n",
        "        else:\n",
        "            printt(\"USING CPU !!!\")\n",
        "        trainer = Trainer(args, log)\n",
        "        train_dataset, _ = get_dataset(args, args.train_datasets, args.train_dir, tokenizer, 'train')\n",
        "        log.info(\"Preparing Validation Data...\")\n",
        "        val_dataset, val_dict = get_dataset(args, args.train_datasets, args.val_dir, tokenizer, 'val')\n",
        "        train_loader = DataLoader(train_dataset,\n",
        "                                batch_size=args.batch_size,\n",
        "                                sampler=RandomSampler(train_dataset))\n",
        "        val_loader = DataLoader(val_dataset,\n",
        "                                batch_size=args.batch_size,\n",
        "                                sampler=SequentialSampler(val_dataset))\n",
        "        best_scores = trainer.train(model, train_loader, val_loader, val_dict)\n",
        "    if args.do_eval:\n",
        "        args.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        split_name = 'test' if 'test' in args.eval_dir else 'validation'\n",
        "        log = util.get_logger(args.save_dir, f'log_{split_name}')\n",
        "        trainer = Trainer(args, log)\n",
        "        checkpoint_path = os.path.join(args.save_dir, 'checkpoint')\n",
        "        model = DistilBertForQuestionAnswering.from_pretrained(checkpoint_path)\n",
        "        model.to(args.device)\n",
        "        eval_dataset, eval_dict = get_dataset(args, args.eval_datasets, args.eval_dir, tokenizer, split_name)\n",
        "        eval_loader = DataLoader(eval_dataset,\n",
        "                                 batch_size=args.batch_size,\n",
        "                                 sampler=SequentialSampler(eval_dataset))\n",
        "        eval_preds, eval_scores = trainer.evaluate(model, eval_loader,\n",
        "                                                   eval_dict, return_preds=True,\n",
        "                                                   split=split_name)\n",
        "        results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in eval_scores.items())\n",
        "        log.info(f'Eval {results_str}')\n",
        "        # Write submission file\n",
        "        sub_path = os.path.join(args.save_dir, split_name + '_' + args.sub_file)\n",
        "        log.info(f'Writing submission file to {sub_path}...')\n",
        "        with open(sub_path, 'w', newline='', encoding='utf-8') as csv_fh:\n",
        "            csv_writer = csv.writer(csv_fh, delimiter=',')\n",
        "            csv_writer.writerow(['Id', 'Predicted'])\n",
        "            for uuid in sorted(eval_preds):\n",
        "                csv_writer.writerow([uuid, eval_preds[uuid]])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVWkdkWYHgC0",
        "outputId": "860c64ff-0f91-4cb1-dc9a-e35100bfdf2d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /tmp/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# run.sh"
      ],
      "metadata": {
        "id": "STjJF5WnEmq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run.sh\n",
        "#!/bin/bash\n",
        "\n",
        "# Modify this if python version is different\n",
        "export PATH=$PATH:/usr/lib/python3.7/venv/scripts/common/:/usr/lib/python3.6/venv/scripts/common/activate\n",
        "cd mtu-nlp-assignment/assignment2/\n",
        "rm -rf robustqa\n",
        "unzip -o robustqa.zip\n",
        "cd robustqa\n",
        "mv datasets_50k.tar.gz  datasets_50k.tar\n",
        "tar -xf datasets_50k.tar\n",
        "source activate robustqa\n",
        "cp -f /tmp/args.py ./args.py\n",
        "cp -f /tmp/train.py ./train.py\n",
        "cp -f /tmp/util.py ./util.py\n",
        "cp -f /tmp/ChunkData.py ./ChunkData.py\n",
        "grep batch.size args.py\n",
        "python train.py --do-train --run-name baseline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZQpzgaT95wt",
        "outputId": "92eedfbc-1046-4c9e-f171-975395ec091c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the model"
      ],
      "metadata": {
        "id": "qvxLkNEWEqsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./run.sh "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7BYx8jAEQCj",
        "outputId": "4a46e40c-5304-43e2-e59d-6d36be42c740"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  robustqa.zip\n",
            "   creating: robustqa/\n",
            "  inflating: robustqa/datasets_50k.tar.gz  \n",
            "  inflating: __MACOSX/robustqa/._datasets_50k.tar.gz  \n",
            "  inflating: robustqa/convert_to_squad.py  \n",
            "  inflating: robustqa/environment.yml  \n",
            "  inflating: robustqa/util.py        \n",
            "  inflating: robustqa/README.md      \n",
            "  inflating: __MACOSX/robustqa/._README.md  \n",
            "  inflating: robustqa/train.py       \n",
            "  inflating: robustqa/args.py        \n",
            "    parser.add_argument('--batch-size', type=int, default=4)\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 2.63MB/s]\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 3.23MB/s]\n",
            "[12.18.21 22:22:17] Args: {\n",
            "    \"batch_size\": 4,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race,relation_extraction,duorc\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 5000,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": false,\n",
            "    \"run_name\": \"baseline\",\n",
            "    \"save_dir\": \"save/baseline-01\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad,nat_questions,newsqa\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[12.18.21 22:22:17] Preparing Training Data...\n",
            "Traceback (most recent call last):\n",
            "  File \"train.py\", line 391, in <module>\n",
            "    main()\n",
            "  File \"train.py\", line 351, in main\n",
            "    printT(\"USING CPU !!!\")\n",
            "NameError: name 'printT' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!free -h\n",
        "!echo\n",
        "!df -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuYIdGhDHA1K",
        "outputId": "3ca9b35c-a404-48dc-9ed0-8cdc1a4ac549"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            35G        1.2G         11G        1.2M         22G         33G\n",
            "Swap:            0B          0B          0B\n",
            "\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay         226G   45G  181G  20% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "shm              17G     0   17G   0% /dev/shm\n",
            "/dev/root       2.0G  1.2G  821M  59% /sbin/docker-init\n",
            "tmpfs            18G   28K   18G   1% /var/colab\n",
            "/dev/sda1       233G   52G  181G  23% /etc/hosts\n",
            "tmpfs            18G     0   18G   0% /proc/acpi\n",
            "tmpfs            18G     0   18G   0% /proc/scsi\n",
            "tmpfs            18G     0   18G   0% /sys/firmware\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "QjAwQdlwJAFP",
        "outputId": "4a460663-bbe9-498e-b1a8-4d2a8601361c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qgIinvDvjwXI"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}