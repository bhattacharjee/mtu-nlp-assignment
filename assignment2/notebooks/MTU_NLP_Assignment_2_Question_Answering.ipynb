{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MTU-NLP-Assignment-2-Question-Answering",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM1AUsKYNXjThSs8sl73I+H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment2/notebooks/MTU_NLP_Assignment_2_Question_Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2nEpGzfbvwj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e25ad072-5725-4704-c80d-2e8699ecdb26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 12.8 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 16 kB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 4.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 69 kB 3.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 46.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 46.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 124 kB 5.0 MB/s \n",
            "\u001b[?25hCollecting virtualenv\n",
            "  Downloading virtualenv-20.10.0-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 5.0 MB/s \n",
            "\u001b[?25hCollecting distlib<1,>=0.3.1\n",
            "  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n",
            "\u001b[K     |████████████████████████████████| 461 kB 56.9 MB/s \n",
            "\u001b[?25hCollecting backports.entry-points-selectable>=1.0.4\n",
            "  Downloading backports.entry_points_selectable-1.1.1-py2.py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six<2,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (1.15.0)\n",
            "Requirement already satisfied: filelock<4,>=3.2 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (3.4.0)\n",
            "Collecting platformdirs<3,>=2\n",
            "  Downloading platformdirs-2.4.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->virtualenv) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->virtualenv) (3.10.0.2)\n",
            "Installing collected packages: platformdirs, distlib, backports.entry-points-selectable, virtualenv\n",
            "Successfully installed backports.entry-points-selectable-1.1.1 distlib-0.3.4 platformdirs-2.4.0 virtualenv-20.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision==0.8.2  -q\n",
        "!pip install torchtext==0.8.1    -q\n",
        "!pip install torchaudio==0.7.2   -q\n",
        "!pip install torch==1.7.1        -q\n",
        "!pip install tqdm==4.49.0        -q\n",
        "!pip install transformers==4.2.2 -q\n",
        "!pip install tensorflow          -q\n",
        "!pip install tensorboard         -q\n",
        "!pip install tensorboardX        -q\n",
        "!pip install --upgrade virtualenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDtkhWtMyuZR",
        "outputId": "b67ec2eb-031b-4e2e-c328-7e99488dd4e9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bhattacharjee/mtu-nlp-assignment.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOK9thHC0rC1",
        "outputId": "cd568ba9-4e28-46e5-d949-935cd423fd88"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mtu-nlp-assignment'...\n",
            "remote: Enumerating objects: 605, done.\u001b[K\n",
            "remote: Counting objects: 100% (605/605), done.\u001b[K\n",
            "remote: Compressing objects: 100% (529/529), done.\u001b[K\n",
            "remote: Total 605 (delta 261), reused 209 (delta 72), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (605/605), 68.67 MiB | 41.17 MiB/s, done.\n",
            "Resolving deltas: 100% (261/261), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run.sh\n",
        "#!/bin/bash\n",
        "\n",
        "# Modify this if python version is different\n",
        "export PATH=$PATH:/usr/lib/python3.7/venv/scripts/common/:/usr/lib/python3.6/venv/scripts/common/activate\n",
        "cd mtu-nlp-assignment/assignment2/\n",
        "rm -rf robustqa\n",
        "unzip -o robustqa.zip\n",
        "cd robustqa\n",
        "mv datasets_50k.tar.gz  datasets_50k.tar\n",
        "tar -xf datasets_50k.tar\n",
        "source activate robustqa\n",
        "python train.py --do-train --run-name baseline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZQpzgaT95wt",
        "outputId": "53143fbc-3398-4140-94d7-d72aed6bd791"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./run.sh "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7BYx8jAEQCj",
        "outputId": "8dbe49dd-74e5-4f51-cf02-507e0972bd1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  robustqa.zip\n",
            "   creating: robustqa/\n",
            "  inflating: robustqa/datasets_50k.tar.gz  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/robustqa/\n",
            "  inflating: __MACOSX/robustqa/._datasets_50k.tar.gz  \n",
            "  inflating: robustqa/convert_to_squad.py  \n",
            "  inflating: robustqa/environment.yml  \n",
            "  inflating: robustqa/util.py        \n",
            "  inflating: robustqa/README.md      \n",
            "  inflating: __MACOSX/robustqa/._README.md  \n",
            "  inflating: robustqa/train.py       \n",
            "  inflating: robustqa/args.py        \n",
            "Downloading: 100% 483/483 [00:00<00:00, 373kB/s]\n",
            "Downloading: 100% 268M/268M [00:05<00:00, 46.5MB/s]\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 2.93MB/s]\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 4.97MB/s]\n",
            "[12.10.21 20:41:39] Args: {\n",
            "    \"batch_size\": 16,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race,relation_extraction,duorc\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 5000,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": false,\n",
            "    \"run_name\": \"baseline\",\n",
            "    \"save_dir\": \"save/baseline-01\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad,nat_questions,newsqa\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[12.10.21 20:41:39] Preparing Training Data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HuMnGBhHE1m7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}