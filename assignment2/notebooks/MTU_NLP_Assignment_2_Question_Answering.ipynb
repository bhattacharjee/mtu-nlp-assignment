{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MTU-NLP-Assignment-2-Question-Answering",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN8RasLT1Kx8JqulHIOHT1Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment2/notebooks/MTU_NLP_Assignment_2_Question_Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2nEpGzfbvwj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5370b8c-6d58-4c7f-8dc7-ddfa350b1052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 12.8 MB 6.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 15 kB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 7.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 7.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 69 kB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 8.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 67.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 64.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 124 kB 8.8 MB/s \n",
            "\u001b[?25hCollecting virtualenv\n",
            "  Downloading virtualenv-20.10.0-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 6.9 MB/s \n",
            "\u001b[?25hCollecting backports.entry-points-selectable>=1.0.4\n",
            "  Downloading backports.entry_points_selectable-1.1.1-py2.py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (4.8.2)\n",
            "Requirement already satisfied: six<2,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (1.15.0)\n",
            "Requirement already satisfied: filelock<4,>=3.2 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (3.4.0)\n",
            "Collecting platformdirs<3,>=2\n",
            "  Downloading platformdirs-2.4.0-py3-none-any.whl (14 kB)\n",
            "Collecting distlib<1,>=0.3.1\n",
            "  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n",
            "\u001b[K     |████████████████████████████████| 461 kB 78.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->virtualenv) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->virtualenv) (3.10.0.2)\n",
            "Installing collected packages: platformdirs, distlib, backports.entry-points-selectable, virtualenv\n",
            "Successfully installed backports.entry-points-selectable-1.1.1 distlib-0.3.4 platformdirs-2.4.0 virtualenv-20.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision==0.8.2  -q\n",
        "!pip install torchtext==0.8.1    -q\n",
        "!pip install torchaudio==0.7.2   -q\n",
        "!pip install torch==1.7.1        -q\n",
        "!pip install tqdm==4.49.0        -q\n",
        "!pip install transformers==4.2.2 -q\n",
        "!pip install tensorflow          -q\n",
        "!pip install tensorboard         -q\n",
        "!pip install tensorboardX        -q\n",
        "!pip install --upgrade virtualenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDtkhWtMyuZR",
        "outputId": "3e6ca801-621d-4495-8860-fdda34100826"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bhattacharjee/mtu-nlp-assignment.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOK9thHC0rC1",
        "outputId": "d1291e81-03fa-4db0-8838-467aa34adfc6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mtu-nlp-assignment'...\n",
            "remote: Enumerating objects: 615, done.\u001b[K\n",
            "remote: Counting objects: 100% (615/615), done.\u001b[K\n",
            "remote: Compressing objects: 100% (539/539), done.\u001b[K\n",
            "remote: Total 615 (delta 265), reused 209 (delta 72), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (615/615), 68.67 MiB | 29.40 MiB/s, done.\n",
            "Resolving deltas: 100% (265/265), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /tmp/args.py\n",
        "import argparse\n",
        "\n",
        "def get_train_test_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--batch-size', type=int, default=1)\n",
        "    parser.add_argument('--num-epochs', type=int, default=3)\n",
        "    parser.add_argument('--lr', type=float, default=3e-5)\n",
        "    parser.add_argument('--num-visuals', type=int, default=10)\n",
        "    parser.add_argument('--seed', type=int, default=42)\n",
        "    parser.add_argument('--save-dir', type=str, default='save/')\n",
        "    parser.add_argument('--train', action='store_true')\n",
        "    parser.add_argument('--eval', action='store_true')\n",
        "    parser.add_argument('--train-datasets', type=str, default='squad,nat_questions,newsqa')\n",
        "    parser.add_argument('--run-name', type=str, default='multitask_distilbert')\n",
        "    parser.add_argument('--recompute-features', action='store_true')\n",
        "    parser.add_argument('--train-dir', type=str, default='datasets/indomain_train')\n",
        "    parser.add_argument('--val-dir', type=str, default='datasets/indomain_val')\n",
        "    parser.add_argument('--eval-dir', type=str, default='datasets/oodomain_test')\n",
        "    parser.add_argument('--eval-datasets', type=str, default='race,relation_extraction,duorc')\n",
        "    parser.add_argument('--do-train', action='store_true')\n",
        "    parser.add_argument('--do-eval', action='store_true')\n",
        "    parser.add_argument('--sub-file', type=str, default='')\n",
        "    parser.add_argument('--visualize-predictions', action='store_true')\n",
        "    parser.add_argument('--eval-every', type=int, default=5000)\n",
        "    args = parser.parse_args()\n",
        "    return args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr1LRFt1GAUJ",
        "outputId": "7121fa11-c451-436a-c92e-6d9ff4fe0899"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /tmp/args.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /tmp/train.py\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import csv\n",
        "import util\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import DistilBertForQuestionAnswering\n",
        "from transformers import AdamW\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
        "from args import get_train_test_args\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def prepare_eval_data(dataset_dict, tokenizer):\n",
        "    tokenized_examples = tokenizer(dataset_dict['question'],\n",
        "                                   dataset_dict['context'],\n",
        "                                   truncation=\"only_second\",\n",
        "                                   stride=128,\n",
        "                                   max_length=384,\n",
        "                                   return_overflowing_tokens=True,\n",
        "                                   return_offsets_mapping=True,\n",
        "                                   padding='max_length')\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
        "    # corresponding example_id and we will store the offset mappings.\n",
        "    tokenized_examples[\"id\"] = []\n",
        "    for i in tqdm(range(len(tokenized_examples[\"input_ids\"]))):\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"id\"].append(dataset_dict[\"id\"][sample_index])\n",
        "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "        # position is part of the context or not.\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == 1 else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "\n",
        "def prepare_train_data(dataset_dict, tokenizer):\n",
        "    tokenized_examples = tokenizer(dataset_dict['question'],\n",
        "                                   dataset_dict['context'],\n",
        "                                   truncation=\"only_second\",\n",
        "                                   stride=128,\n",
        "                                   max_length=384,\n",
        "                                   return_overflowing_tokens=True,\n",
        "                                   return_offsets_mapping=True,\n",
        "                                   padding='max_length')\n",
        "    sample_mapping = tokenized_examples[\"overflow_to_sample_mapping\"]\n",
        "    offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "    tokenized_examples['id'] = []\n",
        "    inaccurate = 0\n",
        "    for i, offsets in enumerate(tqdm(offset_mapping)):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answer = dataset_dict['answer'][sample_index]\n",
        "        # Start/end character index of the answer in the text.\n",
        "        start_char = answer['answer_start'][0]\n",
        "        end_char = start_char + len(answer['text'][0])\n",
        "        tokenized_examples['id'].append(dataset_dict['id'][sample_index])\n",
        "        # Start token index of the current span in the text.\n",
        "        token_start_index = 0\n",
        "        while sequence_ids[token_start_index] != 1:\n",
        "            token_start_index += 1\n",
        "\n",
        "        # End token index of the current span in the text.\n",
        "        token_end_index = len(input_ids) - 1\n",
        "        while sequence_ids[token_end_index] != 1:\n",
        "            token_end_index -= 1\n",
        "\n",
        "        # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "            # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                token_start_index += 1\n",
        "            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "            while offsets[token_end_index][1] >= end_char:\n",
        "                token_end_index -= 1\n",
        "            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "            # assertion to check if this checks out\n",
        "            context = dataset_dict['context'][sample_index]\n",
        "            offset_st = offsets[tokenized_examples['start_positions'][-1]][0]\n",
        "            offset_en = offsets[tokenized_examples['end_positions'][-1]][1]\n",
        "            if context[offset_st : offset_en] != answer['text'][0]:\n",
        "                inaccurate += 1\n",
        "\n",
        "    total = len(tokenized_examples['id'])\n",
        "    print(f\"Preprocessing not completely accurate for {inaccurate}/{total} instances\")\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "\n",
        "def read_and_process(args, tokenizer, dataset_dict, dir_name, dataset_name, split):\n",
        "    #TODO: cache this if possible\n",
        "    cache_path = f'{dir_name}/{dataset_name}_encodings.pt'\n",
        "    if os.path.exists(cache_path) and not args.recompute_features:\n",
        "        tokenized_examples = util.load_pickle(cache_path)\n",
        "    else:\n",
        "        if split=='train':\n",
        "            tokenized_examples = prepare_train_data(dataset_dict, tokenizer)\n",
        "        else:\n",
        "            tokenized_examples = prepare_eval_data(dataset_dict, tokenizer)\n",
        "        util.save_pickle(tokenized_examples, cache_path)\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "\n",
        "#TODO: use a logger, use tensorboard\n",
        "class Trainer():\n",
        "    def __init__(self, args, log):\n",
        "        self.lr = args.lr\n",
        "        self.num_epochs = args.num_epochs\n",
        "        self.device = args.device\n",
        "        self.eval_every = args.eval_every\n",
        "        self.path = os.path.join(args.save_dir, 'checkpoint')\n",
        "        self.num_visuals = args.num_visuals\n",
        "        self.save_dir = args.save_dir\n",
        "        self.log = log\n",
        "        self.visualize_predictions = args.visualize_predictions\n",
        "        if not os.path.exists(self.path):\n",
        "            os.makedirs(self.path)\n",
        "\n",
        "    def save(self, model):\n",
        "        model.save_pretrained(self.path)\n",
        "\n",
        "    def evaluate(self, model, data_loader, data_dict, return_preds=False, split='validation'):\n",
        "        device = self.device\n",
        "\n",
        "        model.eval()\n",
        "        pred_dict = {}\n",
        "        all_start_logits = []\n",
        "        all_end_logits = []\n",
        "        with torch.no_grad(), \\\n",
        "                tqdm(total=len(data_loader.dataset)) as progress_bar:\n",
        "            for batch in data_loader:\n",
        "                # Setup for forward\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                batch_size = len(input_ids)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                # Forward\n",
        "                start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
        "                # TODO: compute loss\n",
        "\n",
        "                all_start_logits.append(start_logits)\n",
        "                all_end_logits.append(end_logits)\n",
        "                progress_bar.update(batch_size)\n",
        "\n",
        "        # Get F1 and EM scores\n",
        "        start_logits = torch.cat(all_start_logits).cpu().numpy()\n",
        "        end_logits = torch.cat(all_end_logits).cpu().numpy()\n",
        "        preds = util.postprocess_qa_predictions(data_dict,\n",
        "                                                 data_loader.dataset.encodings,\n",
        "                                                 (start_logits, end_logits))\n",
        "        if split == 'validation':\n",
        "            results = util.eval_dicts(data_dict, preds)\n",
        "            results_list = [('F1', results['F1']),\n",
        "                            ('EM', results['EM'])]\n",
        "        else:\n",
        "            results_list = [('F1', -1.0),\n",
        "                            ('EM', -1.0)]\n",
        "        results = OrderedDict(results_list)\n",
        "        if return_preds:\n",
        "            return preds, results\n",
        "        return results\n",
        "\n",
        "    def train(self, model, train_dataloader, eval_dataloader, val_dict):\n",
        "        device = self.device\n",
        "        model.to(device)\n",
        "        optim = AdamW(model.parameters(), lr=self.lr)\n",
        "        global_idx = 0\n",
        "        best_scores = {'F1': -1.0, 'EM': -1.0}\n",
        "        tbx = SummaryWriter(self.save_dir)\n",
        "\n",
        "        for epoch_num in range(self.num_epochs):\n",
        "            self.log.info(f'Epoch: {epoch_num}')\n",
        "            with torch.enable_grad(), tqdm(total=len(train_dataloader.dataset)) as progress_bar:\n",
        "                for batch in train_dataloader:\n",
        "                    optim.zero_grad()\n",
        "                    model.train()\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    start_positions = batch['start_positions'].to(device)\n",
        "                    end_positions = batch['end_positions'].to(device)\n",
        "                    outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                                    start_positions=start_positions,\n",
        "                                    end_positions=end_positions)\n",
        "                    loss = outputs[0]\n",
        "                    loss.backward()\n",
        "                    optim.step()\n",
        "                    progress_bar.update(len(input_ids))\n",
        "                    progress_bar.set_postfix(epoch=epoch_num, NLL=loss.item())\n",
        "                    tbx.add_scalar('train/NLL', loss.item(), global_idx)\n",
        "                    if (global_idx % self.eval_every) == 0:\n",
        "                        self.log.info(f'Evaluating at step {global_idx}...')\n",
        "                        preds, curr_score = self.evaluate(model, eval_dataloader, val_dict, return_preds=True)\n",
        "                        results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in curr_score.items())\n",
        "                        self.log.info('Visualizing in TensorBoard...')\n",
        "                        for k, v in curr_score.items():\n",
        "                            tbx.add_scalar(f'val/{k}', v, global_idx)\n",
        "                        self.log.info(f'Eval {results_str}')\n",
        "                        if self.visualize_predictions:\n",
        "                            util.visualize(tbx,\n",
        "                                           pred_dict=preds,\n",
        "                                           gold_dict=val_dict,\n",
        "                                           step=global_idx,\n",
        "                                           split='val',\n",
        "                                           num_visuals=self.num_visuals)\n",
        "                        if curr_score['F1'] >= best_scores['F1']:\n",
        "                            best_scores = curr_score\n",
        "                            self.save(model)\n",
        "                    global_idx += 1\n",
        "        return best_scores\n",
        "\n",
        "def get_dataset(args, datasets, data_dir, tokenizer, split_name):\n",
        "    datasets = datasets.split(',')\n",
        "    dataset_dict = None\n",
        "    dataset_name=''\n",
        "    for dataset in datasets:\n",
        "        dataset_name += f'_{dataset}'\n",
        "        dataset_dict_curr = util.read_squad(f'{data_dir}/{dataset}')\n",
        "        dataset_dict = util.merge(dataset_dict, dataset_dict_curr)\n",
        "    data_encodings = read_and_process(args, tokenizer, dataset_dict, data_dir, dataset_name, split_name)\n",
        "    return util.QADataset(data_encodings, train=(split_name=='train')), dataset_dict\n",
        "\n",
        "def main():\n",
        "    # define parser and arguments\n",
        "    args = get_train_test_args()\n",
        "\n",
        "    util.set_seed(args.seed)\n",
        "    model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    if args.do_train:\n",
        "        if not os.path.exists(args.save_dir):\n",
        "            os.makedirs(args.save_dir)\n",
        "        args.save_dir = util.get_save_dir(args.save_dir, args.run_name)\n",
        "        log = util.get_logger(args.save_dir, 'log_train')\n",
        "        log.info(f'Args: {json.dumps(vars(args), indent=4, sort_keys=True)}')\n",
        "        log.info(\"Preparing Training Data...\")\n",
        "        args.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        if torch.cuda.is_available():\n",
        "            print(\"cuda is available\")\n",
        "        else:\n",
        "            print(\"cuda is not available\")\n",
        "        trainer = Trainer(args, log)\n",
        "        train_dataset, _ = get_dataset(args, args.train_datasets, args.train_dir, tokenizer, 'train')\n",
        "        log.info(\"Preparing Validation Data...\")\n",
        "        val_dataset, val_dict = get_dataset(args, args.train_datasets, args.val_dir, tokenizer, 'val')\n",
        "        train_loader = DataLoader(train_dataset,\n",
        "                                batch_size=args.batch_size,\n",
        "                                sampler=RandomSampler(train_dataset))\n",
        "        val_loader = DataLoader(val_dataset,\n",
        "                                batch_size=args.batch_size,\n",
        "                                sampler=SequentialSampler(val_dataset))\n",
        "        best_scores = trainer.train(model, train_loader, val_loader, val_dict)\n",
        "    if args.do_eval:\n",
        "        args.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        split_name = 'test' if 'test' in args.eval_dir else 'validation'\n",
        "        log = util.get_logger(args.save_dir, f'log_{split_name}')\n",
        "        trainer = Trainer(args, log)\n",
        "        checkpoint_path = os.path.join(args.save_dir, 'checkpoint')\n",
        "        model = DistilBertForQuestionAnswering.from_pretrained(checkpoint_path)\n",
        "        model.to(args.device)\n",
        "        eval_dataset, eval_dict = get_dataset(args, args.eval_datasets, args.eval_dir, tokenizer, split_name)\n",
        "        eval_loader = DataLoader(eval_dataset,\n",
        "                                 batch_size=args.batch_size,\n",
        "                                 sampler=SequentialSampler(eval_dataset))\n",
        "        eval_preds, eval_scores = trainer.evaluate(model, eval_loader,\n",
        "                                                   eval_dict, return_preds=True,\n",
        "                                                   split=split_name)\n",
        "        results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in eval_scores.items())\n",
        "        log.info(f'Eval {results_str}')\n",
        "        # Write submission file\n",
        "        sub_path = os.path.join(args.save_dir, split_name + '_' + args.sub_file)\n",
        "        log.info(f'Writing submission file to {sub_path}...')\n",
        "        with open(sub_path, 'w', newline='', encoding='utf-8') as csv_fh:\n",
        "            csv_writer = csv.writer(csv_fh, delimiter=',')\n",
        "            csv_writer.writerow(['Id', 'Predicted'])\n",
        "            for uuid in sorted(eval_preds):\n",
        "                csv_writer.writerow([uuid, eval_preds[uuid]])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVWkdkWYHgC0",
        "outputId": "5fcffdde-50dd-4261-9400-c0ca814dba8c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /tmp/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run.sh\n",
        "#!/bin/bash\n",
        "\n",
        "# Modify this if python version is different\n",
        "export PATH=$PATH:/usr/lib/python3.7/venv/scripts/common/:/usr/lib/python3.6/venv/scripts/common/activate\n",
        "cd mtu-nlp-assignment/assignment2/\n",
        "rm -rf robustqa\n",
        "unzip -o robustqa.zip\n",
        "cd robustqa\n",
        "mv datasets_50k.tar.gz  datasets_50k.tar\n",
        "tar -xf datasets_50k.tar\n",
        "source activate robustqa\n",
        "cp -f /tmp/args.py ./args.py\n",
        "cp -f /tmp/train.py ./train.py\n",
        "grep batch.size args.py\n",
        "python train.py --do-train --run-name baseline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZQpzgaT95wt",
        "outputId": "df0c5c93-75a9-498c-a1d8-f7d273e5a0a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing run.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./run.sh "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7BYx8jAEQCj",
        "outputId": "93b89f69-8561-4887-f010-af1aca4018f3"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  robustqa.zip\n",
            "   creating: robustqa/\n",
            "  inflating: robustqa/datasets_50k.tar.gz  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/robustqa/\n",
            "  inflating: __MACOSX/robustqa/._datasets_50k.tar.gz  \n",
            "  inflating: robustqa/convert_to_squad.py  \n",
            "  inflating: robustqa/environment.yml  \n",
            "  inflating: robustqa/util.py        \n",
            "  inflating: robustqa/README.md      \n",
            "  inflating: __MACOSX/robustqa/._README.md  \n",
            "  inflating: robustqa/train.py       \n",
            "  inflating: robustqa/args.py        \n",
            "    parser.add_argument('--batch-size', type=int, default=1)\n",
            "Downloading: 100% 483/483 [00:00<00:00, 475kB/s]\n",
            "Downloading: 100% 268M/268M [00:04<00:00, 59.7MB/s]\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Downloading: 100% 232k/232k [00:00<00:00, 908kB/s]\n",
            "Downloading: 100% 466k/466k [00:00<00:00, 1.49MB/s]\n",
            "[12.10.21 21:07:03] Args: {\n",
            "    \"batch_size\": 1,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race,relation_extraction,duorc\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 5000,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": false,\n",
            "    \"run_name\": \"baseline\",\n",
            "    \"save_dir\": \"save/baseline-01\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad,nat_questions,newsqa\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[12.10.21 21:07:03] Preparing Training Data...\n",
            "cuda is available\n",
            "./run.sh: line 15:   268 Killed                  python train.py --do-train --run-name baseline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!free -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuYIdGhDHA1K",
        "outputId": "a1e20fd9-008d-4231-c9ec-c218a612b805"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            25G        613M         24G        1.2M        709M         24G\n",
            "Swap:            0B          0B          0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QjAwQdlwJAFP"
      },
      "execution_count": 8,
      "outputs": []
    }
  ]
}