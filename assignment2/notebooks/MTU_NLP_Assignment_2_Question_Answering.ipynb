{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MTU-NLP-Assignment-2-Question-Answering",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPqcHLCE4DhLum6MXuDimUO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment2/notebooks/MTU_NLP_Assignment_2_Question_Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nEpGzfbvwj-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e347d4c9-c994-4ea2-a7cb-4c506df2542d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 12.8 MB 8.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 15 kB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 7.0 MB 6.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 7.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 69 kB 4.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 7.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 75.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 78.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 124 kB 8.9 MB/s \n",
            "\u001b[?25hCollecting virtualenv\n",
            "  Downloading virtualenv-20.10.0-py2.py3-none-any.whl (5.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 9.1 MB/s \n",
            "\u001b[?25hCollecting platformdirs<3,>=2\n",
            "  Downloading platformdirs-2.4.0-py3-none-any.whl (14 kB)\n",
            "Collecting backports.entry-points-selectable>=1.0.4\n",
            "  Downloading backports.entry_points_selectable-1.1.1-py2.py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: six<2,>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (4.8.2)\n",
            "Collecting distlib<1,>=0.3.1\n",
            "  Downloading distlib-0.3.4-py2.py3-none-any.whl (461 kB)\n",
            "\u001b[K     |████████████████████████████████| 461 kB 79.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock<4,>=3.2 in /usr/local/lib/python3.7/dist-packages (from virtualenv) (3.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->virtualenv) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.12->virtualenv) (3.10.0.2)\n",
            "Installing collected packages: platformdirs, distlib, backports.entry-points-selectable, virtualenv\n",
            "Successfully installed backports.entry-points-selectable-1.1.1 distlib-0.3.4 platformdirs-2.4.0 virtualenv-20.10.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision==0.8.2  -q\n",
        "!pip install torchtext==0.8.1    -q\n",
        "!pip install torchaudio==0.7.2   -q\n",
        "!pip install torch==1.7.1        -q\n",
        "!pip install tqdm==4.49.0        -q\n",
        "!pip install transformers==4.2.2 -q\n",
        "!pip install tensorflow          -q\n",
        "!pip install tensorboard         -q\n",
        "!pip install tensorboardX        -q\n",
        "!pip install --upgrade virtualenv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDtkhWtMyuZR",
        "outputId": "b56f26d6-a59e-410d-ccfb-b57d09000007"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "drive.mount('/content/gdrive')\n",
        "#shutil.copy('/content/gdrive/MyDrive/MTUNLPA2/saved_pickle.tar.gz', '/tmp/saved_pickle.tar.gz')"
      ],
      "metadata": {
        "id": "daoDMfvJpfqa",
        "outputId": "9219778d-039c-4b9d-f244-84a45d54e2fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/tmp/saved_pickle.tar.gz'"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!tar -xvzf /tmp/saved_pickle.tar.gz\n",
        "#!rm -f /tmp/saved_pickle.tar.gz"
      ],
      "metadata": {
        "id": "4jqbwyEls1AS",
        "outputId": "814b40d8-1875-46cd-8be8-8fd3efe59746",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saved_pickle/\n",
            "saved_pickle/_squad_nat_questions_newsqa_encodings.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls saved_pickle\n",
        "#!du -sh saved_pickle\n",
        "#!pwd\n"
      ],
      "metadata": {
        "id": "95IyWg1-ujLC",
        "outputId": "43370c6e-d428-482b-954c-8ddf384d50bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_squad_nat_questions_newsqa_encodings.pt\n",
            "5.3G\tsaved_pickle\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/bhattacharjee/mtu-nlp-assignment.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOK9thHC0rC1",
        "outputId": "69036760-e37c-4f19-83be-3d2a96517c61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mtu-nlp-assignment'...\n",
            "remote: Enumerating objects: 750, done.\u001b[K\n",
            "remote: Counting objects: 100% (750/750), done.\u001b[K\n",
            "remote: Compressing objects: 100% (649/649), done.\u001b[K\n",
            "remote: Total 750 (delta 332), reused 284 (delta 97), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (750/750), 92.87 MiB | 28.51 MiB/s, done.\n",
            "Resolving deltas: 100% (332/332), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## args.py"
      ],
      "metadata": {
        "id": "454WJr-Rq5rh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /tmp/args.py\n",
        "import argparse\n",
        "\n",
        "def get_train_test_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--batch-size', type=int, default=1)\n",
        "    parser.add_argument('--num-epochs', type=int, default=3)\n",
        "    parser.add_argument('--lr', type=float, default=3e-5)\n",
        "    parser.add_argument('--num-visuals', type=int, default=10)\n",
        "    parser.add_argument('--seed', type=int, default=42)\n",
        "    parser.add_argument('--save-dir', type=str, default='save/')\n",
        "    parser.add_argument('--train', action='store_true')\n",
        "    parser.add_argument('--eval', action='store_true')\n",
        "    parser.add_argument('--train-datasets', type=str, default='squad,nat_questions,newsqa')\n",
        "    parser.add_argument('--run-name', type=str, default='multitask_distilbert')\n",
        "    parser.add_argument('--recompute-features', action='store_true')\n",
        "    parser.add_argument('--train-dir', type=str, default='datasets/indomain_train')\n",
        "    parser.add_argument('--val-dir', type=str, default='datasets/indomain_val')\n",
        "    parser.add_argument('--eval-dir', type=str, default='datasets/oodomain_test')\n",
        "    parser.add_argument('--eval-datasets', type=str, default='race,relation_extraction,duorc')\n",
        "    parser.add_argument('--do-train', action='store_true')\n",
        "    parser.add_argument('--do-eval', action='store_true')\n",
        "    parser.add_argument('--sub-file', type=str, default='')\n",
        "    parser.add_argument('--visualize-predictions', action='store_true')\n",
        "    parser.add_argument('--eval-every', type=int, default=5000)\n",
        "    args = parser.parse_args()\n",
        "    return args"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nr1LRFt1GAUJ",
        "outputId": "bde3fa27-903f-4489-f74f-5124ea0e2574"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /tmp/args.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cPickle as pickle"
      ],
      "metadata": {
        "id": "aLntfROp1-pw",
        "outputId": "099626f7-511f-4ea9-b295-fdbe9d76c652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-49-400cc400590d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcPickle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cPickle'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## util.py"
      ],
      "metadata": {
        "id": "t444EYHjrbrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /tmp/util.py\n",
        "\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "import logging\n",
        "import cpickle as pickle\n",
        "import string\n",
        "import re\n",
        "from pathlib import Path\n",
        "from collections import Counter, OrderedDict, defaultdict as ddict\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def print_ram_usage():\n",
        "    import os, psutil\n",
        "    print(\"RAM USAGE\")\n",
        "    print(psutil.Process(os.getpid()).memory_info().rss / 1024 ** 2)\n",
        "\n",
        "def load_pickle(path):\n",
        "    print(\"Calling gc\")\n",
        "    import gc\n",
        "    gc.collect(0)\n",
        "    gc.collect(1)\n",
        "    gc.collect(2)\n",
        "    print_ram_usage()\n",
        "    with open(path, 'rb') as f:\n",
        "        obj = pickle.load(f)\n",
        "    print(\"Pickle is done\")\n",
        "    gc.collect(0)\n",
        "    gc.collect(1)\n",
        "    gc.collect(2)\n",
        "    return obj\n",
        "\n",
        "def save_pickle(obj, path):\n",
        "    with open(path, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "    return\n",
        "\n",
        "def visualize(tbx, pred_dict, gold_dict, step, split, num_visuals):\n",
        "    \"\"\"Visualize text examples to TensorBoard.\n",
        "\n",
        "    Args:\n",
        "        tbx (tensorboardX.SummaryWriter): Summary writer.\n",
        "        pred_dict (dict): dict of predictions of the form id -> pred.\n",
        "        step (int): Number of examples seen so far during training.\n",
        "        split (str): Name of data split being visualized.\n",
        "        num_visuals (int): Number of visuals to select at random from preds.\n",
        "    \"\"\"\n",
        "    if num_visuals <= 0:\n",
        "        return\n",
        "    if num_visuals > len(pred_dict):\n",
        "        num_visuals = len(pred_dict)\n",
        "    id2index = {curr_id : idx for idx, curr_id in enumerate(gold_dict['id'])}\n",
        "    visual_ids = np.random.choice(list(pred_dict), size=num_visuals, replace=False)\n",
        "    for i, id_ in enumerate(visual_ids):\n",
        "        pred = pred_dict[id_] or 'N/A'\n",
        "        idx_gold_dict = id2index[id_]\n",
        "        question = gold_dict['question'][idx_gold_dict]\n",
        "        context = gold_dict['context'][idx_gold_dict]\n",
        "        answers = gold_dict['answer'][idx_gold_dict]\n",
        "        gold = answers['text'][0] if answers else 'N/A'\n",
        "        tbl_fmt = (f'- **Question:** {question}\\n'\n",
        "                   + f'- **Context:** {context}\\n'\n",
        "                   + f'- **Answer:** {gold}\\n'\n",
        "                   + f'- **Prediction:** {pred}')\n",
        "        tbx.add_text(tag=f'{split}/{i+1}_of_{num_visuals}',\n",
        "                     text_string=tbl_fmt,\n",
        "                     global_step=step)\n",
        "\n",
        "\n",
        "def get_save_dir(base_dir, name, id_max=100):\n",
        "    for uid in range(1, id_max):\n",
        "        save_dir = os.path.join(base_dir, f'{name}-{uid:02d}')\n",
        "        if not os.path.exists(save_dir):\n",
        "            os.makedirs(save_dir)\n",
        "            return save_dir\n",
        "\n",
        "    raise RuntimeError('Too many save directories created with the same name. \\\n",
        "                       Delete old save directories or use another name.')\n",
        "\n",
        "\n",
        "def filter_encodings(encodings):\n",
        "    filter_idx = [idx for idx, val in enumerate(encodings['end_positions'])\n",
        "                 if not val]\n",
        "    filter_idx = set(filter_idx)\n",
        "    encodings_filtered = {key : [] for key in encodings}\n",
        "    sz = len(encodings['input_ids'])\n",
        "    for idx in range(sz):\n",
        "        if idx not in filter_idx:\n",
        "            for key in encodings:\n",
        "                encodings_filtered[key].append(encodings[key][idx])\n",
        "    return encodings_filtered\n",
        "\n",
        "def merge(encodings, new_encoding):\n",
        "    if not encodings:\n",
        "        return new_encoding\n",
        "    else:\n",
        "        for key in new_encoding:\n",
        "            encodings[key] += new_encoding[key]\n",
        "        return encodings\n",
        "\n",
        "def get_logger(log_dir, name):\n",
        "    \"\"\"Get a `logging.Logger` instance that prints to the console\n",
        "    and an auxiliary file.\n",
        "\n",
        "    Args:\n",
        "        log_dir (str): Directory in which to create the log file.\n",
        "        name (str): Name to identify the logs.\n",
        "\n",
        "    Returns:\n",
        "        logger (logging.Logger): Logger instance for logging events.\n",
        "    \"\"\"\n",
        "    class StreamHandlerWithTQDM(logging.Handler):\n",
        "        \"\"\"Let `logging` print without breaking `tqdm` progress bars.\n",
        "\n",
        "        See Also:\n",
        "            > https://stackoverflow.com/questions/38543506\n",
        "        \"\"\"\n",
        "        def emit(self, record):\n",
        "            try:\n",
        "                msg = self.format(record)\n",
        "                tqdm.write(msg)\n",
        "                self.flush()\n",
        "            except (KeyboardInterrupt, SystemExit):\n",
        "                raise\n",
        "            except:\n",
        "                self.handleError(record)\n",
        "\n",
        "    # Create logger\n",
        "    logger = logging.getLogger(name)\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "\n",
        "    # Log everything (i.e., DEBUG level and above) to a file\n",
        "    log_path = os.path.join(log_dir, f'{name}.txt')\n",
        "    file_handler = logging.FileHandler(log_path)\n",
        "    file_handler.setLevel(logging.DEBUG)\n",
        "\n",
        "    # Log everything except DEBUG level (i.e., INFO level and above) to console\n",
        "    console_handler = StreamHandlerWithTQDM()\n",
        "    console_handler.setLevel(logging.INFO)\n",
        "\n",
        "    # Create format for the logs\n",
        "    file_formatter = logging.Formatter('[%(asctime)s] %(message)s',\n",
        "                                       datefmt='%m.%d.%y %H:%M:%S')\n",
        "    file_handler.setFormatter(file_formatter)\n",
        "    console_formatter = logging.Formatter('[%(asctime)s] %(message)s',\n",
        "                                          datefmt='%m.%d.%y %H:%M:%S')\n",
        "    console_handler.setFormatter(console_formatter)\n",
        "\n",
        "    # add the handlers to the logger\n",
        "    logger.addHandler(file_handler)\n",
        "    logger.addHandler(console_handler)\n",
        "\n",
        "    return logger\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"Keep track of average values over time.\n",
        "\n",
        "    Adapted from:\n",
        "        > https://github.com/pytorch/examples/blob/master/imagenet/main.py\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset meter.\"\"\"\n",
        "        self.__init__()\n",
        "\n",
        "    def update(self, val, num_samples=1):\n",
        "        \"\"\"Update meter with new value `val`, the average of `num` samples.\n",
        "\n",
        "        Args:\n",
        "            val (float): Average value to update the meter with.\n",
        "            num_samples (int): Number of samples that were averaged to\n",
        "                produce `val`.\n",
        "        \"\"\"\n",
        "        self.count += num_samples\n",
        "        self.sum += val * num_samples\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, encodings, train=True):\n",
        "        self.encodings = encodings\n",
        "        self.keys = ['input_ids', 'attention_mask']\n",
        "        if train:\n",
        "            self.keys += ['start_positions', 'end_positions']\n",
        "        assert(all(key in self.encodings for key in self.keys))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key : torch.tensor(self.encodings[key][idx]) for key in self.keys}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings['input_ids'])\n",
        "\n",
        "def read_squad(path):\n",
        "    path = Path(path)\n",
        "    with open(path, 'rb') as f:\n",
        "        squad_dict = json.load(f)\n",
        "    data_dict = {'question': [], 'context': [], 'id': [], 'answer': []}\n",
        "    for group in squad_dict['data']:\n",
        "        for passage in group['paragraphs']:\n",
        "            context = passage['context']\n",
        "            for qa in passage['qas']:\n",
        "                question = qa['question']\n",
        "                if len(qa['answers']) == 0:\n",
        "                    data_dict['question'].append(question)\n",
        "                    data_dict['context'].append(context)\n",
        "                    data_dict['id'].append(qa['id'])\n",
        "                else:\n",
        "                    for answer in  qa['answers']:\n",
        "                        data_dict['question'].append(question)\n",
        "                        data_dict['context'].append(context)\n",
        "                        data_dict['id'].append(qa['id'])\n",
        "                        data_dict['answer'].append(answer)\n",
        "    id_map = ddict(list)\n",
        "    for idx, qid in enumerate(data_dict['id']):\n",
        "        id_map[qid].append(idx)\n",
        "\n",
        "    data_dict_collapsed = {'question': [], 'context': [], 'id': []}\n",
        "    if data_dict['answer']:\n",
        "        data_dict_collapsed['answer'] = []\n",
        "    for qid in id_map:\n",
        "        ex_ids = id_map[qid]\n",
        "        data_dict_collapsed['question'].append(data_dict['question'][ex_ids[0]])\n",
        "        data_dict_collapsed['context'].append(data_dict['context'][ex_ids[0]])\n",
        "        data_dict_collapsed['id'].append(qid)\n",
        "        if data_dict['answer']:\n",
        "            all_answers = [data_dict['answer'][idx] for idx in ex_ids]\n",
        "            data_dict_collapsed['answer'].append({'answer_start': [answer['answer_start'] for answer in all_answers],\n",
        "                                                  'text': [answer['text'] for answer in all_answers]})\n",
        "    return data_dict_collapsed\n",
        "\n",
        "def add_token_positions(encodings, answers, tokenizer):\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i in range(len(answers)):\n",
        "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
        "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n",
        "\n",
        "        # if start position is None, the answer passage has been truncated\n",
        "        if start_positions[-1] is None:\n",
        "            start_positions[-1] = tokenizer.model_max_length\n",
        "\n",
        "        # if end position is None, the 'char_to_token' function points to the space before the correct token - > add + 1\n",
        "        if end_positions[-1] is None:\n",
        "            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] + 1)\n",
        "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
        "\n",
        "\n",
        "def add_end_idx(answers, contexts):\n",
        "    for answer, context in zip(answers, contexts):\n",
        "        gold_text = answer['text']\n",
        "        start_idx = answer['answer_start']\n",
        "        end_idx = start_idx + len(gold_text)\n",
        "\n",
        "        # sometimes squad answers are off by a character or two – fix this\n",
        "        if context[start_idx:end_idx] == gold_text:\n",
        "            answer['answer_end'] = end_idx\n",
        "        elif context[start_idx-1:end_idx-1] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 1\n",
        "            answer['answer_end'] = end_idx - 1     # When the gold label is off by one character\n",
        "        elif context[start_idx-2:end_idx-2] == gold_text:\n",
        "            answer['answer_start'] = start_idx - 2\n",
        "            answer['answer_end'] = end_idx - 2     # When the gold label is off by two characters\n",
        "\n",
        "def convert_tokens(eval_dict, qa_id, y_start_list, y_end_list):\n",
        "    \"\"\"Convert predictions to tokens from the context.\n",
        "\n",
        "    Args:\n",
        "        eval_dict (dict): Dictionary with eval info for the dataset. This is\n",
        "            used to perform the mapping from IDs and indices to actual text.\n",
        "        qa_id (int): List of QA example IDs.\n",
        "        y_start_list (list): List of start predictions.\n",
        "        y_end_list (list): List of end predictions.\n",
        "        no_answer (bool): Questions can have no answer. E.g., SQuAD 2.0.\n",
        "\n",
        "    Returns:\n",
        "        pred_dict (dict): Dictionary index IDs -> predicted answer text.\n",
        "        sub_dict (dict): Dictionary UUIDs -> predicted answer text (submission).\n",
        "    \"\"\"\n",
        "    pred_dict = {}\n",
        "    sub_dict = {}\n",
        "    for qid, y_start, y_end in zip(qa_id, y_start_list, y_end_list):\n",
        "        context = eval_dict[str(qid)][\"context\"]\n",
        "        spans = eval_dict[str(qid)][\"spans\"]\n",
        "        uuid = eval_dict[str(qid)][\"uuid\"]\n",
        "        start_idx = spans[y_start][0]\n",
        "        end_idx = spans[y_end][1]\n",
        "        pred_dict[str(qid)] = context[start_idx: end_idx]\n",
        "        sub_dict[uuid] = context[start_idx: end_idx]\n",
        "    return pred_dict, sub_dict\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    if not ground_truths:\n",
        "        return metric_fn(prediction, '')\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def eval_dicts(gold_dict, pred_dict):\n",
        "    avna = f1 = em = total = 0\n",
        "    id2index = {curr_id : idx for idx, curr_id in enumerate(gold_dict['id'])}\n",
        "    for curr_id in pred_dict:\n",
        "        total += 1\n",
        "        index = id2index[curr_id]\n",
        "        ground_truths = gold_dict['answer'][index]['text']\n",
        "        prediction = pred_dict[curr_id]\n",
        "        em += metric_max_over_ground_truths(compute_em, prediction, ground_truths)\n",
        "        f1 += metric_max_over_ground_truths(compute_f1, prediction, ground_truths)\n",
        "\n",
        "    eval_dict = {'EM': 100. * em / total,\n",
        "                 'F1': 100. * f1 / total}\n",
        "    return eval_dict\n",
        "\n",
        "def postprocess_qa_predictions(examples, features, predictions,\n",
        "                               n_best_size=20, max_answer_length=30):\n",
        "    all_start_logits, all_end_logits = predictions\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = ddict(list)\n",
        "    for i, feat_id in enumerate(features['id']):\n",
        "        features_per_example[example_id_to_index[feat_id]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    all_predictions = OrderedDict()\n",
        "\n",
        "    # Let's loop over all the examples!\n",
        "    for example_index in tqdm(range(len(examples['id']))):\n",
        "        example = {key : examples[key][example_index] for key in examples}\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "        prelim_predictions = []\n",
        "\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = all_start_logits[feature_index]\n",
        "            end_logits = all_end_logits[feature_index]\n",
        "            seq_ids = features.sequence_ids(feature_index)\n",
        "            non_pad_idx = len(seq_ids) - 1\n",
        "            while not seq_ids[non_pad_idx]:\n",
        "                non_pad_idx -= 1\n",
        "            start_logits = start_logits[:non_pad_idx]\n",
        "            end_logits = end_logits[:non_pad_idx]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[\"offset_mapping\"][feature_index]\n",
        "            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n",
        "            # available in the current feature.\n",
        "            token_is_max_context = features.get(\"token_is_max_context\", None)\n",
        "            if token_is_max_context:\n",
        "                token_is_max_context = token_is_max_context[feature_index]\n",
        "\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or offset_mapping[start_index] is None\n",
        "                        or offset_mapping[end_index] is None\n",
        "                    ):\n",
        "                        continue\n",
        "                    # Don't consider answers with a length that is either = 0 or > max_answer_length.\n",
        "                    if end_index <= start_index or end_index - start_index + 1 > max_answer_length:\n",
        "                        continue\n",
        "                    # Don't consider answer that don't have the maximum context available (if such information is\n",
        "                    # provided).\n",
        "                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n",
        "                        continue\n",
        "                    prelim_predictions.append(\n",
        "                        {\n",
        "                            \"start_index\": start_index,\n",
        "                            \"end_index\": end_index,\n",
        "                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n",
        "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
        "                            \"start_logit\": start_logits[start_index],\n",
        "                            \"end_logit\": end_logits[end_index],\n",
        "                        }\n",
        "                    )\n",
        "        # Only keep the best `n_best_size` predictions.\n",
        "        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
        "\n",
        "        # Use the offsets to gather the answer text in the original context.\n",
        "        context = example[\"context\"]\n",
        "        for pred in predictions:\n",
        "            offsets = pred['offsets']\n",
        "            pred[\"text\"] = context[offsets[0] : offsets[1]]\n",
        "\n",
        "        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "        # failure.\n",
        "        if len(predictions) == 0:\n",
        "            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n",
        "\n",
        "        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n",
        "        # the LogSumExp trick).\n",
        "        scores = np.array([pred.pop(\"score\") for pred in predictions])\n",
        "        exp_scores = np.exp(scores - np.max(scores))\n",
        "        probs = exp_scores / exp_scores.sum()\n",
        "\n",
        "        # Include the probabilities in our predictions.\n",
        "        for prob, pred in zip(probs, predictions):\n",
        "            pred[\"probability\"] = prob\n",
        "\n",
        "        # need to find the best non-empty prediction.\n",
        "        i = 0\n",
        "        while i < len(predictions):\n",
        "            if predictions[i]['text'] != '':\n",
        "                break\n",
        "            i += 1\n",
        "        if i == len(predictions):\n",
        "            import pdb; pdb.set_trace();\n",
        "\n",
        "        best_non_null_pred = predictions[i]\n",
        "        all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "\n",
        "\n",
        "# All methods below this line are from the official SQuAD 2.0 eval script\n",
        "# https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Convert to lowercase and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
        "        return re.sub(regex, ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def get_tokens(s):\n",
        "    if not s:\n",
        "        return []\n",
        "    return normalize_answer(s).split()\n",
        "\n",
        "def compute_em(a_gold, a_pred):\n",
        "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
        "\n",
        "\n",
        "def compute_f1(a_gold, a_pred):\n",
        "    gold_toks = get_tokens(a_gold)\n",
        "    pred_toks = get_tokens(a_pred)\n",
        "    common = Counter(gold_toks) & Counter(pred_toks)\n",
        "    num_same = sum(common.values())\n",
        "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
        "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
        "        return int(gold_toks == pred_toks)\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(pred_toks)\n",
        "    recall = 1.0 * num_same / len(gold_toks)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn72CZjoq-zB",
        "outputId": "19a58bc1-120e-4f65-b28c-82cbff90105f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /tmp/util.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## train.py"
      ],
      "metadata": {
        "id": "nU1kuEaIq8O_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /tmp/train.py\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "from collections import OrderedDict\n",
        "import torch\n",
        "import csv\n",
        "import util\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import DistilBertForQuestionAnswering\n",
        "from transformers import AdamW\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.sampler import RandomSampler, SequentialSampler\n",
        "from args import get_train_test_args\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "def prepare_eval_data(dataset_dict, tokenizer):\n",
        "    tokenized_examples = tokenizer(dataset_dict['question'],\n",
        "                                   dataset_dict['context'],\n",
        "                                   truncation=\"only_second\",\n",
        "                                   stride=128,\n",
        "                                   max_length=384,\n",
        "                                   return_overflowing_tokens=True,\n",
        "                                   return_offsets_mapping=True,\n",
        "                                   padding='max_length')\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n",
        "    # corresponding example_id and we will store the offset mappings.\n",
        "    tokenized_examples[\"id\"] = []\n",
        "    for i in tqdm(range(len(tokenized_examples[\"input_ids\"]))):\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"id\"].append(dataset_dict[\"id\"][sample_index])\n",
        "        # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "        # position is part of the context or not.\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == 1 else None)\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "\n",
        "def prepare_train_data(dataset_dict, tokenizer):\n",
        "    tokenized_examples = tokenizer(dataset_dict['question'],\n",
        "                                   dataset_dict['context'],\n",
        "                                   truncation=\"only_second\",\n",
        "                                   stride=128,\n",
        "                                   max_length=384,\n",
        "                                   return_overflowing_tokens=True,\n",
        "                                   return_offsets_mapping=True,\n",
        "                                   padding='max_length')\n",
        "    sample_mapping = tokenized_examples[\"overflow_to_sample_mapping\"]\n",
        "    offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "    tokenized_examples['id'] = []\n",
        "    inaccurate = 0\n",
        "    for i, offsets in enumerate(tqdm(offset_mapping)):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answer = dataset_dict['answer'][sample_index]\n",
        "        # Start/end character index of the answer in the text.\n",
        "        start_char = answer['answer_start'][0]\n",
        "        end_char = start_char + len(answer['text'][0])\n",
        "        tokenized_examples['id'].append(dataset_dict['id'][sample_index])\n",
        "        # Start token index of the current span in the text.\n",
        "        token_start_index = 0\n",
        "        while sequence_ids[token_start_index] != 1:\n",
        "            token_start_index += 1\n",
        "\n",
        "        # End token index of the current span in the text.\n",
        "        token_end_index = len(input_ids) - 1\n",
        "        while sequence_ids[token_end_index] != 1:\n",
        "            token_end_index -= 1\n",
        "\n",
        "        # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "            # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                token_start_index += 1\n",
        "            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "            while offsets[token_end_index][1] >= end_char:\n",
        "                token_end_index -= 1\n",
        "            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "            # assertion to check if this checks out\n",
        "            context = dataset_dict['context'][sample_index]\n",
        "            offset_st = offsets[tokenized_examples['start_positions'][-1]][0]\n",
        "            offset_en = offsets[tokenized_examples['end_positions'][-1]][1]\n",
        "            if context[offset_st : offset_en] != answer['text'][0]:\n",
        "                inaccurate += 1\n",
        "\n",
        "    total = len(tokenized_examples['id'])\n",
        "    print(f\"Preprocessing not completely accurate for {inaccurate}/{total} instances\")\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "\n",
        "def read_and_process(args, tokenizer, dataset_dict, dir_name, dataset_name, split):\n",
        "    #TODO: cache this if possible\n",
        "    cache_path = f'{dir_name}/{dataset_name}_encodings.pt'\n",
        "    #TODO: RAJBIR: remove the following line\n",
        "    cache_path = '/content/saved_pickle/_squad_nat_questions_newsqa_encodings.pt'\n",
        "    print(cache_path)\n",
        "    if os.path.exists(cache_path) and not args.recompute_features:\n",
        "        print(f\"Loading saved from {cache_path}\")\n",
        "        import gc\n",
        "        gc.collect(0)\n",
        "        gc.collect(1)\n",
        "        gc.collect(2)\n",
        "        gc.collect(0)\n",
        "        gc.collect(1)\n",
        "        gc.collect(2)\n",
        "        tokenized_examples = util.load_pickle(cache_path)\n",
        "        print(\"Loaded tokenized examples\")\n",
        "        gc.collect(0)\n",
        "        gc.collect(1)\n",
        "        gc.collect(2)\n",
        "        gc.collect(0)\n",
        "        gc.collect(1)\n",
        "        gc.collect(2)\n",
        "    else:\n",
        "        if split=='train':\n",
        "            tokenized_examples = prepare_train_data(dataset_dict, tokenizer)\n",
        "        else:\n",
        "            tokenized_examples = prepare_eval_data(dataset_dict, tokenizer)\n",
        "        util.save_pickle(tokenized_examples, cache_path)\n",
        "    print(\"Done with read_and_process\")\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "\n",
        "#TODO: use a logger, use tensorboard\n",
        "class Trainer():\n",
        "    def __init__(self, args, log):\n",
        "        self.lr = args.lr\n",
        "        self.num_epochs = args.num_epochs\n",
        "        self.device = args.device\n",
        "        self.eval_every = args.eval_every\n",
        "        self.path = os.path.join(args.save_dir, 'checkpoint')\n",
        "        self.num_visuals = args.num_visuals\n",
        "        self.save_dir = args.save_dir\n",
        "        self.log = log\n",
        "        self.visualize_predictions = args.visualize_predictions\n",
        "        if not os.path.exists(self.path):\n",
        "            os.makedirs(self.path)\n",
        "\n",
        "    def save(self, model):\n",
        "        model.save_pretrained(self.path)\n",
        "\n",
        "    def evaluate(self, model, data_loader, data_dict, return_preds=False, split='validation'):\n",
        "        device = self.device\n",
        "\n",
        "        model.eval()\n",
        "        pred_dict = {}\n",
        "        all_start_logits = []\n",
        "        all_end_logits = []\n",
        "        with torch.no_grad(), \\\n",
        "                tqdm(total=len(data_loader.dataset)) as progress_bar:\n",
        "            for batch in data_loader:\n",
        "                # Setup for forward\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                batch_size = len(input_ids)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                # Forward\n",
        "                start_logits, end_logits = outputs.start_logits, outputs.end_logits\n",
        "                # TODO: compute loss\n",
        "\n",
        "                all_start_logits.append(start_logits)\n",
        "                all_end_logits.append(end_logits)\n",
        "                progress_bar.update(batch_size)\n",
        "\n",
        "        # Get F1 and EM scores\n",
        "        start_logits = torch.cat(all_start_logits).cpu().numpy()\n",
        "        end_logits = torch.cat(all_end_logits).cpu().numpy()\n",
        "        preds = util.postprocess_qa_predictions(data_dict,\n",
        "                                                 data_loader.dataset.encodings,\n",
        "                                                 (start_logits, end_logits))\n",
        "        if split == 'validation':\n",
        "            results = util.eval_dicts(data_dict, preds)\n",
        "            results_list = [('F1', results['F1']),\n",
        "                            ('EM', results['EM'])]\n",
        "        else:\n",
        "            results_list = [('F1', -1.0),\n",
        "                            ('EM', -1.0)]\n",
        "        results = OrderedDict(results_list)\n",
        "        if return_preds:\n",
        "            return preds, results\n",
        "        return results\n",
        "\n",
        "    def train(self, model, train_dataloader, eval_dataloader, val_dict):\n",
        "        device = self.device\n",
        "        model.to(device)\n",
        "        optim = AdamW(model.parameters(), lr=self.lr)\n",
        "        global_idx = 0\n",
        "        best_scores = {'F1': -1.0, 'EM': -1.0}\n",
        "        tbx = SummaryWriter(self.save_dir)\n",
        "\n",
        "        for epoch_num in range(self.num_epochs):\n",
        "            self.log.info(f'Epoch: {epoch_num}')\n",
        "            with torch.enable_grad(), tqdm(total=len(train_dataloader.dataset)) as progress_bar:\n",
        "                for batch in train_dataloader:\n",
        "                    optim.zero_grad()\n",
        "                    model.train()\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    start_positions = batch['start_positions'].to(device)\n",
        "                    end_positions = batch['end_positions'].to(device)\n",
        "                    outputs = model(input_ids, attention_mask=attention_mask,\n",
        "                                    start_positions=start_positions,\n",
        "                                    end_positions=end_positions)\n",
        "                    loss = outputs[0]\n",
        "                    loss.backward()\n",
        "                    optim.step()\n",
        "                    progress_bar.update(len(input_ids))\n",
        "                    progress_bar.set_postfix(epoch=epoch_num, NLL=loss.item())\n",
        "                    tbx.add_scalar('train/NLL', loss.item(), global_idx)\n",
        "                    if (global_idx % self.eval_every) == 0:\n",
        "                        self.log.info(f'Evaluating at step {global_idx}...')\n",
        "                        preds, curr_score = self.evaluate(model, eval_dataloader, val_dict, return_preds=True)\n",
        "                        results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in curr_score.items())\n",
        "                        self.log.info('Visualizing in TensorBoard...')\n",
        "                        for k, v in curr_score.items():\n",
        "                            tbx.add_scalar(f'val/{k}', v, global_idx)\n",
        "                        self.log.info(f'Eval {results_str}')\n",
        "                        if self.visualize_predictions:\n",
        "                            util.visualize(tbx,\n",
        "                                           pred_dict=preds,\n",
        "                                           gold_dict=val_dict,\n",
        "                                           step=global_idx,\n",
        "                                           split='val',\n",
        "                                           num_visuals=self.num_visuals)\n",
        "                        if curr_score['F1'] >= best_scores['F1']:\n",
        "                            best_scores = curr_score\n",
        "                            self.save(model)\n",
        "                    global_idx += 1\n",
        "        return best_scores\n",
        "\n",
        "def get_dataset(args, datasets, data_dir, tokenizer, split_name):\n",
        "    datasets = datasets.split(',')\n",
        "    dataset_dict = None\n",
        "    dataset_name=''\n",
        "    for dataset in datasets:\n",
        "        dataset_name += f'_{dataset}'\n",
        "        dataset_dict_curr = util.read_squad(f'{data_dir}/{dataset}')\n",
        "        dataset_dict = util.merge(dataset_dict, dataset_dict_curr)\n",
        "    print(\"read_and_process\")\n",
        "    data_encodings = read_and_process(args, tokenizer, dataset_dict, data_dir, dataset_name, split_name)\n",
        "    print(\"finished read and process, calling QADataset\")\n",
        "    ds = util.QADataset(data_encodings, train=(split_name=='train')), dataset_dict\n",
        "    print(\"Finished QADataset\")\n",
        "    return ds\n",
        "\n",
        "def main():\n",
        "    # define parser and arguments\n",
        "    args = get_train_test_args()\n",
        "\n",
        "    util.set_seed(args.seed)\n",
        "    model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
        "    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "    if args.do_train:\n",
        "        if not os.path.exists(args.save_dir):\n",
        "            os.makedirs(args.save_dir)\n",
        "        args.save_dir = util.get_save_dir(args.save_dir, args.run_name)\n",
        "        log = util.get_logger(args.save_dir, 'log_train')\n",
        "        log.info(f'Args: {json.dumps(vars(args), indent=4, sort_keys=True)}')\n",
        "        log.info(\"Preparing Training Data...\")\n",
        "        args.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        if torch.cuda.is_available():\n",
        "            print(\"cuda is available\")\n",
        "        else:\n",
        "            print(\"cuda is not available\")\n",
        "        trainer = Trainer(args, log)\n",
        "        train_dataset, _ = get_dataset(args, args.train_datasets, args.train_dir, tokenizer, 'train')\n",
        "        log.info(\"Preparing Validation Data...\")\n",
        "        val_dataset, val_dict = get_dataset(args, args.train_datasets, args.val_dir, tokenizer, 'val')\n",
        "        train_loader = DataLoader(train_dataset,\n",
        "                                batch_size=args.batch_size,\n",
        "                                sampler=RandomSampler(train_dataset))\n",
        "        val_loader = DataLoader(val_dataset,\n",
        "                                batch_size=args.batch_size,\n",
        "                                sampler=SequentialSampler(val_dataset))\n",
        "        best_scores = trainer.train(model, train_loader, val_loader, val_dict)\n",
        "    if args.do_eval:\n",
        "        args.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        split_name = 'test' if 'test' in args.eval_dir else 'validation'\n",
        "        log = util.get_logger(args.save_dir, f'log_{split_name}')\n",
        "        trainer = Trainer(args, log)\n",
        "        checkpoint_path = os.path.join(args.save_dir, 'checkpoint')\n",
        "        model = DistilBertForQuestionAnswering.from_pretrained(checkpoint_path)\n",
        "        model.to(args.device)\n",
        "        eval_dataset, eval_dict = get_dataset(args, args.eval_datasets, args.eval_dir, tokenizer, split_name)\n",
        "        eval_loader = DataLoader(eval_dataset,\n",
        "                                 batch_size=args.batch_size,\n",
        "                                 sampler=SequentialSampler(eval_dataset))\n",
        "        eval_preds, eval_scores = trainer.evaluate(model, eval_loader,\n",
        "                                                   eval_dict, return_preds=True,\n",
        "                                                   split=split_name)\n",
        "        results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in eval_scores.items())\n",
        "        log.info(f'Eval {results_str}')\n",
        "        # Write submission file\n",
        "        sub_path = os.path.join(args.save_dir, split_name + '_' + args.sub_file)\n",
        "        log.info(f'Writing submission file to {sub_path}...')\n",
        "        with open(sub_path, 'w', newline='', encoding='utf-8') as csv_fh:\n",
        "            csv_writer = csv.writer(csv_fh, delimiter=',')\n",
        "            csv_writer.writerow(['Id', 'Predicted'])\n",
        "            for uuid in sorted(eval_preds):\n",
        "                csv_writer.writerow([uuid, eval_preds[uuid]])\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVWkdkWYHgC0",
        "outputId": "dfe6ba10-539e-4580-e0a3-f170143d3394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /tmp/train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile run.sh\n",
        "#!/bin/bash\n",
        "\n",
        "# Modify this if python version is different\n",
        "export PATH=$PATH:/usr/lib/python3.7/venv/scripts/common/:/usr/lib/python3.6/venv/scripts/common/activate\n",
        "cd mtu-nlp-assignment/assignment2/\n",
        "rm -rf robustqa\n",
        "unzip -o robustqa.zip\n",
        "cd robustqa\n",
        "mv datasets_50k.tar.gz  datasets_50k.tar\n",
        "tar -xf datasets_50k.tar\n",
        "source activate robustqa\n",
        "cp -f /tmp/args.py ./args.py\n",
        "cp -f /tmp/train.py ./train.py\n",
        "cp -f /tmp/util.py ./util.py\n",
        "grep batch.size args.py\n",
        "python train.py --do-train --run-name baseline\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZQpzgaT95wt",
        "outputId": "03a5b670-0032-4eb0-f6d1-a7d9238bef65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting run.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./run.sh "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7BYx8jAEQCj",
        "outputId": "a803b8df-2dc8-455b-f130-e96060f896a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  robustqa.zip\n",
            "   creating: robustqa/\n",
            "  inflating: robustqa/datasets_50k.tar.gz  \n",
            "  inflating: __MACOSX/robustqa/._datasets_50k.tar.gz  \n",
            "  inflating: robustqa/convert_to_squad.py  \n",
            "  inflating: robustqa/environment.yml  \n",
            "  inflating: robustqa/util.py        \n",
            "  inflating: robustqa/README.md      \n",
            "  inflating: __MACOSX/robustqa/._README.md  \n",
            "  inflating: robustqa/train.py       \n",
            "  inflating: robustqa/args.py        \n",
            "    parser.add_argument('--batch-size', type=int, default=1)\n",
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "[12.17.21 23:55:56] Args: {\n",
            "    \"batch_size\": 1,\n",
            "    \"do_eval\": false,\n",
            "    \"do_train\": true,\n",
            "    \"eval\": false,\n",
            "    \"eval_datasets\": \"race,relation_extraction,duorc\",\n",
            "    \"eval_dir\": \"datasets/oodomain_test\",\n",
            "    \"eval_every\": 5000,\n",
            "    \"lr\": 3e-05,\n",
            "    \"num_epochs\": 3,\n",
            "    \"num_visuals\": 10,\n",
            "    \"recompute_features\": false,\n",
            "    \"run_name\": \"baseline\",\n",
            "    \"save_dir\": \"save/baseline-01\",\n",
            "    \"seed\": 42,\n",
            "    \"sub_file\": \"\",\n",
            "    \"train\": false,\n",
            "    \"train_datasets\": \"squad,nat_questions,newsqa\",\n",
            "    \"train_dir\": \"datasets/indomain_train\",\n",
            "    \"val_dir\": \"datasets/indomain_val\",\n",
            "    \"visualize_predictions\": false\n",
            "}\n",
            "[12.17.21 23:55:56] Preparing Training Data...\n",
            "cuda is available\n",
            "read_and_process\n",
            "/content/saved_pickle/_squad_nat_questions_newsqa_encodings.pt\n",
            "Loading saved from /content/saved_pickle/_squad_nat_questions_newsqa_encodings.pt\n",
            "Calling gc\n",
            "RAM USAGE\n",
            "987.8359375\n",
            "tcmalloc: large alloc 1073741824 bytes == 0x560770e9e000 @  0x7fd3b11ec2a4 0x560692c3f7d2 0x560692b2aae0 0x560692b8846c 0x560692b88240 0x560692bfc0f3 0x560692b89afa 0x560692bfbd00 0x560692b89afa 0x560692bf7915 0x560692b89afa 0x560692bf7915 0x560692b89afa 0x560692bf7915 0x560692bf69ee 0x560692bf66f3 0x560692cc04c2 0x560692cc083d 0x560692cc06e6 0x560692c98163 0x560692c97e0c 0x7fd3affd5bf7 0x560692c97cea\n",
            "./run.sh: line 16:  1146 Killed                  python train.py --do-train --run-name baseline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!free -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iuYIdGhDHA1K",
        "outputId": "57523bf4-ee15-491e-c058-a34fbb4d9ba3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:            25G        719M         23G        1.2M        852M         24G\n",
            "Swap:            0B          0B          0B\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QjAwQdlwJAFP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}