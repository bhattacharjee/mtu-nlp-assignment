{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_qa_rahul_agarwal.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOTKtuXOdVx4fnFdSiIPxAC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/web-examples/bert_qa_rahul_agarwal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch -q -q -q\n",
        "!pip install transformers -q -q -q\n",
        "!pip install pytorch-nlp -q -q -q\n",
        "!pip install datasets -q -q -q\n"
      ],
      "metadata": {
        "id": "Rna0-Tcd03L2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "t_JMQ1o-0ra6"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "#from datasets import load_dataset, load_metric\n",
        "import random\n",
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "import torch\n",
        "from transformers import default_data_collator\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Use cuda by default\n",
        "cuda = torch.device('cuda')\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use the default BERT pretrained on SQUAD for examples"
      ],
      "metadata": {
        "id": "-rPsBsuW8icn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "bert_model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(bert_model_name)\n",
        "\n",
        "text = r\"\"\"\n",
        "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
        "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "TensorFlow 2.0 and PyTorch.\n",
        "\"\"\"\n",
        "questions = [\n",
        "    \"How many pretrained models are available in Transformers?\",\n",
        "    \"What does Transformers provide?\",\n",
        "    \"Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "for question in questions:\n",
        "    t1 = time.monotonic()\n",
        "    inputs = tokenizer.encode_plus(\\\n",
        "                question,\\\n",
        "                text,\\\n",
        "                add_special_tokens=True,\\\n",
        "                return_tensors=\"pt\")\n",
        "\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    pred = model(**inputs)\n",
        "    answer_start_scores = pred['start_logits'][0]\n",
        "    answer_end_scores = pred['end_logits'][0]\n",
        "\n",
        "    # Get the most likely beginning of answer with the argmax of the score\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    # Get the most likely end of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1\n",
        "\n",
        "    #print(answer_start_scores.detach().cpu().numpy().shape)\n",
        "    #print(answer_end_scores.detach().cpu().numpy().shape)\n",
        "    #print(answer_start, answer_end)\n",
        "\n",
        "    #print(\"--->\\n\", inputs['input_ids'].detach().cpu().numpy())\n",
        "\n",
        "\n",
        "    answer = tokenizer\\\n",
        "                    .convert_tokens_to_string(\\\n",
        "                        tokenizer.convert_ids_to_tokens(\\\n",
        "                            input_ids[answer_start:answer_end]))\n",
        "\n",
        "    print(f\"Time taken = {time.monotonic() - t1}\")\n",
        "    print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\\n\")"
      ],
      "metadata": {
        "id": "vvXNHYJD05Fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the SQUAD dataset"
      ],
      "metadata": {
        "id": "BCp1qQaR-8lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = load_dataset(\"squad\")\n",
        "def visualize(datasets, datatype = 'train', n_questions=10):\n",
        "    n = len(datasets[datatype])\n",
        "    random_questions=random.choices(list(range(n)),k=n_questions)\n",
        "    for i in random_questions:\n",
        "        print(f\"Context:{datasets[datatype][i]['context']}\")\n",
        "        print(f\"Question:{datasets[datatype][i]['question']}\")\n",
        "        print(f\"Answer:{datasets[datatype][i]['answers']['text']}\")\n",
        "        print(f\"Answer Start in Text:{datasets[datatype][i]['answers']['answer_start']}\")\n",
        "        print(\"-\"*100)\n",
        "visualize(datasets)"
      ],
      "metadata": {
        "id": "YsSopevr3_pc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing the dataset"
      ],
      "metadata": {
        "id": "JbHou4flHiSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dealing with long docs:\n",
        "max_length = 384 # The maximum length of a feature (question and context)\n",
        "doc_stride = 128 # The authorized overlap between two part of the context when splitting it\n",
        "\n",
        "def prepare_train_features(examples):\n",
        "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    print(examples.keys())\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" ],\n",
        "        examples[\"context\" ],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=max_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    # Looks like [0,1,2,2,2,3,4,5,5...] - Here 2nd input pair has been split in 3 parts\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "    # help us compute the start_positions and end_positions.\n",
        "    # Looks like [[(0,0),(0,3),(3,4)...] ] - Contains the actual start indices and end indices for each word in the input.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "        # If no answers are given, set the cls_index as answer.\n",
        "        if len(answers[\"answer_start\"]) == 0:\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # Start/end character index of the answer in the text.\n",
        "            start_char = answers[\"answer_start\"][0]\n",
        "            end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "            # Start token index of the current span in the text.\n",
        "            token_start_index = 0\n",
        "            while sequence_ids[token_start_index] != 1:\n",
        "                token_start_index += 1\n",
        "\n",
        "            # End token index of the current span in the text.\n",
        "            token_end_index = len(input_ids) - 1\n",
        "            while sequence_ids[token_end_index] != 1:\n",
        "                token_end_index -= 1\n",
        "\n",
        "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "            else:\n",
        "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
        "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
        "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
        "                    token_start_index += 1\n",
        "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "                while offsets[token_end_index][1] >= end_char:\n",
        "                    token_end_index -= 1\n",
        "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "\n",
        "    return tokenized_examples"
      ],
      "metadata": {
        "id": "-u_wseUh9krQ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Actually train the model"
      ],
      "metadata": {
        "id": "0Fl1mh21H05Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"bert-base-uncased\"\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"test-squad\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "data_collator = default_data_collator\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(trainer.save_model(\"test-squad-trained\"))"
      ],
      "metadata": {
        "id": "xzsMeIRH_w8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using at evaluate time"
      ],
      "metadata": {
        "id": "LPA6_cPqHens"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(\"test-squad-trained\")\n",
        "text = r\"\"\"\n",
        "ðŸ¤— Transformers (formerly known as pytorch-transformers and pytorch-pretrained-bert) provides general-purpose\n",
        "architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNetâ€¦) for Natural Language Understanding (NLU) and Natural\n",
        "Language Generation (NLG) with over 32+ pretrained models in 100+ languages and deep interoperability between\n",
        "TensorFlow 2.0 and PyTorch\n",
        "\"\"\"\n",
        "questions = [\n",
        "    \"How many pretrained models are available in Transformers?\",\n",
        "    \"What does Transformers provide?\",\n",
        "    \"Transformers provides interoperability between which frameworks?\",\n",
        "]\n",
        "for question in questions:\n",
        "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].tolist()[0]\n",
        "text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "    pred = model(**inputs)\n",
        "    answer_start_scores, answer_end_scores = pred['start_logits'][0] ,pred['end_logits'][0]\n",
        "\n",
        "    answer_start = torch.argmax(\n",
        "        answer_start_scores\n",
        "    )  # Get the most likely beginning of answer with the argmax of the score\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1  # Get the most likely end of answer with the argmax of the score\n",
        "answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
        "print(f\"Question: {question}\")\n",
        "    print(f\"Answer: {answer}\\n\")\n"
      ],
      "metadata": {
        "id": "3WWiieKY_zMt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}