{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pipeline_work.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNMVdZMMLDPgcuznp+nxaYQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment1/pipeline_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERvBOEn0wss6"
      },
      "source": [
        "!pip install spacy  nltk spacymoji huggingface -q       >/dev/null 2>&1         \n",
        "!pip install -q -U tensorflow-text                      >/dev/null 2>&1\n",
        "!pip install -q tf-models-official                      >/dev/null 2>&1\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "!pip install transformers                               >/dev/null 2>&1\n",
        "\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "\n",
        "!pip install mlxtend                                    >/dev/null 2>&1\n",
        "!pip install imblearn                                   >/dev/null 2>&1\n",
        "\n",
        "# handling emojis\n",
        "!pip install demoji                                     >/dev/null 2>&1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-DAX9VLZlw3"
      },
      "source": [
        "import requests\n",
        "def get_train_test_files():\n",
        "    TRAIN_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Test_For_Evaluation.csv'\n",
        "    TRAIN_FILE_LOCAL = 'Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE_LOCAL = 'Assessment1_Toxic_Test.csv'\n",
        "\n",
        "    def download(url, localfile):\n",
        "        with open(localfile, 'wb') as f:\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            f.write(r.content)\n",
        "\n",
        "    download(TRAIN_FILE, TRAIN_FILE_LOCAL)\n",
        "    download(TEST_FILE, TEST_FILE_LOCAL)\n",
        "\n",
        "    return TRAIN_FILE_LOCAL, TEST_FILE_LOCAL\n",
        "\n",
        "sklearn.set_config(display=\"diagram\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrlJ1H1maDxM"
      },
      "source": [
        "# Functions to read the CSV and do basic cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVfYulLnuGVJ"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import demoji\n",
        "\n",
        "def remove_roles(line:str)->str:\n",
        "    # Remove texts like @USER, @MODERATOR etc\n",
        "    pat = re.compile(u'\\@[A-Za-z]+')\n",
        "    return re.sub(pat, '', line)\n",
        "\n",
        "def get_train_test_df():\n",
        "    train_csv, test_csv = get_train_test_files()\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "    return train_df, test_df\n",
        "\n",
        "def remove_emojis(line:str)->str:\n",
        "    # Replace emojis with their description, eg __thumbs_down__\n",
        "    demoji_str = demoji.replace_with_desc(line, sep=\" ::: \")\n",
        "    if (demoji_str == line):\n",
        "        return line\n",
        "    \n",
        "    inEmoji = False\n",
        "    currentEmojiWords = []\n",
        "    allWords = []\n",
        "\n",
        "    def accumulate(word:str)->None:\n",
        "        nonlocal inEmoji\n",
        "        nonlocal currentEmojiWords\n",
        "        nonlocal allWords\n",
        "        if not inEmoji and word != \":::\":\n",
        "            allWords.append(word)\n",
        "        elif inEmoji:\n",
        "            if word == ':::':\n",
        "                currentEmoji = \"_\".join(currentEmojiWords)\n",
        "                currentEmoji = \"__\" + currentEmoji + \"__\"\n",
        "                allWords.append(currentEmoji)\n",
        "                currentEmojiWords = []\n",
        "            else:\n",
        "                currentEmojiWords.append(word)\n",
        "        else: # Not in emoji but ::: is true\n",
        "            inEmoji = True\n",
        "\n",
        "    [accumulate(word) for word in demoji_str.split()]\n",
        "\n",
        "    sentence = \" \".join(allWords)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def remove_ellipses(line:str)->str:\n",
        "    pat = re.compile(u'\\.\\.+')\n",
        "    return re.sub(pat, ' ', line)\n",
        "\n",
        "def to_lower(line:str)->str:\n",
        "    return line.lower()\n",
        "\n",
        "def replace_number_with_tag(line:str)->str:\n",
        "    line = re.sub(\"\\s\\d*((\\.|\\,)\\d+)?\\s\", \" nummer \", line)\n",
        "    line = re.sub('\\s\\d+$', '', line)\n",
        "    line = re.sub('^\\d+\\s', '', line)\n",
        "    return line\n",
        "\n",
        "def remove_urls(line:str)->str:\n",
        "    return re.sub('https?:\\/\\/\\S+', ' hyperlink ', line)\n",
        "\n",
        "def basic_clean(s:pd.Series)->pd.Series:\n",
        "    return s.map(to_lower)                                                  \\\n",
        "            .map(remove_emojis)                                             \\\n",
        "            .map(remove_roles)                                              \\\n",
        "            .map(remove_ellipses)                                           \\\n",
        "            .map(replace_number_with_tag)                                   \\\n",
        "            .map(remove_urls)\n",
        "\n",
        "def get_clean_train_test_df()->tuple:\n",
        "    train_df, test_df = get_train_test_df()\n",
        "    train_df['comment_text'] = basic_clean(train_df['comment_text'])\n",
        "    test_df['comment_text'] = basic_clean(test_df['comment_text'])\n",
        "    return train_df, test_df\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6_dto2JaR2f"
      },
      "source": [
        "# Clean using Spacy and Enrich"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8YSZZ4vxTk5"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string\n",
        "import spacy\n",
        "from spacymoji import Emoji\n",
        "import  de_core_news_sm\n",
        "\n",
        "def is_punct_only(token:str)->bool:\n",
        "    for c in list(token):\n",
        "        if c not in string.punctuation:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_same(l1:list, l2:list)->bool:\n",
        "    if (len(l1) != len(l2)):\n",
        "        return False\n",
        "    for x, y in zip(l1, l2):\n",
        "        if x != y:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def get_num_of_allcap_words(s:str)->int:\n",
        "    def is_allcaps(s:str)->bool:\n",
        "        if (len(s) < 3):\n",
        "            return False\n",
        "        for c in list(s):\n",
        "            if not (\\\n",
        "                    (ord(c) <=ord('Z') and ord(c) >= ord('A')) or           \\\n",
        "                    (ord(c) >= ord('0') and ord(c) <= ord('9'))             \\\n",
        "                    ):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    if len(s) < 3:\n",
        "        return 0\n",
        "    tokens = [w.strip() for w in s.split()]\n",
        "    return sum([1 for t in tokens if is_allcaps(t)])\n",
        "\n",
        "def get_percentage_of_excalamations(s:str)->float:\n",
        "    if len(s) == 0:\n",
        "        return 0.0\n",
        "    exclamation_count = sum([1 for c in list(s) if c == '!'])\n",
        "    return exclamation_count / len(s)\n",
        "\n",
        "\n",
        "def is_empty_string(s:str)->bool:\n",
        "    if s == '' or s == None:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def do_basic_nlp_cleaning(line:str)->str:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(line)\n",
        "\n",
        "    # Some tokens start with a punctuation, remove the first one\n",
        "    def remove_first_punctuation(tok:str)->str:\n",
        "        return                                                              \\\n",
        "            tok[1:]                                                         \\\n",
        "            if tok[0] in set(string.punctuation) and len(tok) != 0          \\\n",
        "            else tok\n",
        "\n",
        "    tokens = [remove_first_punctuation(w) for w in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"german\"))\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [w for w in tokens if not is_punct_only(w)]\n",
        "\n",
        "    # Stem words\n",
        "    stem = SnowballStemmer('german')\n",
        "    tokens = [stem.stem(w) for w in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def get_cleaning_function():\n",
        "    #nlp = spacy.load(\"de_dep_news_trf\")\n",
        "    #nlp = spacy.load(\"de_core_news_sm\")\n",
        "    nlp = de_core_news_sm.load()\n",
        "    emoji = Emoji(nlp)\n",
        "    nlp.add_pipe(emoji, first=True)\n",
        "    stopwords = spacy.lang.de.stop_words.STOP_WORDS\n",
        "\n",
        "    def do_basic_nlp_cleaning(line:str)->str:\n",
        "        def is_interesting_token(token):\n",
        "            if token.pos_ in set(['NUM', 'SYM']):\n",
        "                return False\n",
        "            if token.text in stopwords:\n",
        "                return False\n",
        "            if (token.is_punct):\n",
        "                return False\n",
        "            #if token._.is_emoji:\n",
        "            #    return False\n",
        "            return True\n",
        "\n",
        "        def remove_terminal_punctuations(word):\n",
        "            word = word.strip()\n",
        "            while word != \"\" and word[0] in list(string.punctuation):\n",
        "                word = word[1:]\n",
        "            while word != \"\" and word[-1] in list(string.punctuation):\n",
        "                word = word[:-1]\n",
        "            return word\n",
        "\n",
        "        doc = nlp(line)\n",
        "        words = [tok.lemma_.lower() for tok in doc if is_interesting_token(tok)]\n",
        "        words = [remove_terminal_punctuations(word) for word in words]\n",
        "        words = [word for word in words if word != \"\"]\n",
        "        return  \" \".join(words)\n",
        "\n",
        "    return do_basic_nlp_cleaning\n",
        "\n",
        "def get_enriched_dataset(df):\n",
        "    df['cleaned_comment_text'] = df['comment_text'].map(cleaning_fn)\n",
        "    df['n_all_caps'] = df['comment_text'].map(get_num_of_allcap_words)\n",
        "    df['perc_exclamations'] = df['comment_text'].map(get_percentage_of_excalamations)\n",
        "    df['num_exclamations'] = df['comment_text'].map(lambda s: sum([1 for x in list(s) if x == '!']))\n",
        "    return df\n",
        "\n",
        "cleaning_fn = get_cleaning_function()\n",
        "train_df, test_df = get_clean_train_test_df()\n",
        "train_df = get_enriched_dataset(train_df)\n",
        "test_df = get_enriched_dataset(test_df)\n",
        "empty_rows = train_df['cleaned_comment_text'].map(is_empty_string)\n",
        "train_df = train_df[~ empty_rows]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWTtxHw0aa4N"
      },
      "source": [
        "# Print Enriched Training DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "n3q-TKbwZRBJ",
        "outputId": "fa823d16-a6bb-494b-9ccd-5a3ab8b8d7ff"
      },
      "source": [
        "train_df"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>Sub1_Toxic</th>\n",
              "      <th>Sub2_Engaging</th>\n",
              "      <th>Sub3_FactClaiming</th>\n",
              "      <th>cleaned_comment_text</th>\n",
              "      <th>n_all_caps</th>\n",
              "      <th>perc_exclamations</th>\n",
              "      <th>num_exclamations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gestern bei illner, montag bei nummer  ist das...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>gestern illner montag nummer langsam ör-partei...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mein gott der war erst gestern bei illner. die...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>gott gestern illner redaktionen versagen</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>die cdu lässt das so wie so nicht zu . sagen ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>cdu lässt sagen reich bekommen nummer milliard...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bei meiner beschissenen rente als 2x geschiede...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>beschissen rente geschieden mann steuern krank...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wer nummer jahre zum mindestlohn arbeiten muß,...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>nummer mindestlohn arbeiten erhalten € rente n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.005025</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3189</th>\n",
              "      <td>hier mal eine info. flüchtlinge werden nummer ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>mal info flüchtlinge nummer km küste schlauchb...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3190</th>\n",
              "      <td>.aha .mal abwarten kommt bei uns auch .firmen ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>aha mal abwarten firmen entlassen mitarbeiter ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3191</th>\n",
              "      <td>.so ist es</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>so</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3192</th>\n",
              "      <td>.die warten da</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>die warten</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3193</th>\n",
              "      <td>.das bekommen die gesagt wie sich verhalten s...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>das bekommen verhalten kameras richten tut ers...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3149 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           comment_text  ...  num_exclamations\n",
              "0     gestern bei illner, montag bei nummer  ist das...  ...                 0\n",
              "1     mein gott der war erst gestern bei illner. die...  ...                 0\n",
              "2      die cdu lässt das so wie so nicht zu . sagen ...  ...                 0\n",
              "3     bei meiner beschissenen rente als 2x geschiede...  ...                 0\n",
              "4     wer nummer jahre zum mindestlohn arbeiten muß,...  ...                 3\n",
              "...                                                 ...  ...               ...\n",
              "3189  hier mal eine info. flüchtlinge werden nummer ...  ...                 0\n",
              "3190  .aha .mal abwarten kommt bei uns auch .firmen ...  ...                 0\n",
              "3191                                         .so ist es  ...                 0\n",
              "3192                                     .die warten da  ...                 0\n",
              "3193   .das bekommen die gesagt wie sich verhalten s...  ...                 0\n",
              "\n",
              "[3149 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68DFX_fjYgQz"
      },
      "source": [
        "# Multinomial NB (original)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 671
        },
        "id": "zL5Boe5qYfF4",
        "outputId": "a962155f-4702-4e8c-bba2-33b1b9a19737"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.pipeline import Pipeline\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE \n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.preprocessing import DenseTransformer\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "def get_feature_column_names(df):\n",
        "    return [cname for cname in df.columns if not cname.startswith('Sub')]\n",
        "\n",
        "def get_target_column_names(df):\n",
        "    return [cname for cname in df.columns if cname.startswith('Sub')]\n",
        "\n",
        "def is_text_column(colname:str)->bool:\n",
        "    if 'text' in colname:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_text_columns(df)->list:\n",
        "    return [cn for cn in df.columns if is_text_column(cn)]\n",
        "\n",
        "def get_nontext_columns(df)->list:\n",
        "    return [cn for cn in df.columns if not is_text_column(cn)]\n",
        "\n",
        "def run_classification(                                                     \\\n",
        "                       dataset:pd.DataFrame,                                \\\n",
        "                       target_column:str,                                   \\\n",
        "                       clf_gen_fn,                                          \\\n",
        "                       use_smote=False)->tuple:\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'n_all_caps']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'num_exclamations']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'perc_exclamations']]\n",
        "    X = dataset[get_feature_column_names(dataset)]\n",
        "    y = dataset[target_column]\n",
        "    trainX, testX, trainY, testY = train_test_split(X, y, random_state=0)\n",
        "\n",
        "    def get_text_pipeline():\n",
        "        return Pipeline(                                                    \\\n",
        "                        [                                                   \\\n",
        "                            ('cv', CountVectorizer(),),                     \\\n",
        "                            ('tfid', TfidfVectorizer()),                    \\\n",
        "                        ])\n",
        "\n",
        "                        \n",
        "    column_trans = make_column_transformer(                                 \\\n",
        "                            (CountVectorizer(ngram_range=(1,1)), 'cleaned_comment_text'),   \\\n",
        "                            (TfidfVectorizer(use_idf=True), 'cleaned_comment_text'),    \\\n",
        "                            (TfidfVectorizer(use_idf=False), 'cleaned_comment_text'),    \\\n",
        "                            remainder=MinMaxScaler(),                       \\\n",
        "                        )\n",
        "\n",
        "    if use_smote:\n",
        "        classif_pipeline = Pipeline(                                        \\\n",
        "                                [                                           \\\n",
        "                                    ('column_transformer', column_trans),   \\\n",
        "                                    ('dense', DenseTransformer()),          \\\n",
        "                                    ('smote', SMOTE(n_jobs=-1)),            \\\n",
        "                                    ('clf', clf_gen_fn()),                  \\\n",
        "                                ])\n",
        "    else:\n",
        "        classif_pipeline = Pipeline(                                        \\\n",
        "                                [                                           \\\n",
        "                                    ('column_transformer', column_trans),   \\\n",
        "                                    ('dense', DenseTransformer()),          \\\n",
        "                                    ('clf', clf_gen_fn()),                  \\\n",
        "                                ])\n",
        "    \n",
        "    \n",
        "    classif_pipeline.fit(trainX, trainY)\n",
        "    y_pred = classif_pipeline.predict(testX)\n",
        "\n",
        "    return accuracy_score(testY, y_pred), f1_score(testY, y_pred), classif_pipeline\n",
        "\n",
        "def run_classifiers():\n",
        "\n",
        "    classifiers = {\n",
        "        \"LinearSVC_nosmote\": (False, lambda: LinearSVC(),),\n",
        "        \"LinearSVC\": (True, lambda: LinearSVC(),),\n",
        "        \"AdaBoost\": (True, lambda: AdaBoostClassifier(),),\n",
        "        \"AdaBoost_nosmote\": (False, lambda: AdaBoostClassifier(),),\n",
        "        \"MultinomialNB_nosmote\": (False, lambda: MultinomialNB(),),\n",
        "        \"RandomForestClassifier\": (True, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "        \"RandomForestClassifier_nosmote\": (False, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "        \"BernoulliNB_nosmote\": (False, lambda: BernoulliNB(),),\n",
        "    }\n",
        "\n",
        "    result_df = pd.DataFrame(                                                           \\\n",
        "                    {                                                       \\\n",
        "                        'classifier': pd.Series('str'),                     \\\n",
        "                        'task_name': pd.Series('str'),                      \\\n",
        "                        'metric': pd.Series('str'),                         \\\n",
        "                        'smote': pd.Series(int),                            \\\n",
        "                        'value': pd.Series(float),                          \\\n",
        "                    })\n",
        "\n",
        "    for clfname, value in classifiers.items():\n",
        "        use_smote, clfgen = value\n",
        "        for colname in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "            accuracy, f1, model = run_classification(train_df, colname, clfgen, use_smote)\n",
        "            print(f\"{clfname:20.20s} {colname:20.20s} accuracy={accuracy:1.3f}              f1={f1:1.3f}   smote={use_smote}\")\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'accuracy',\n",
        "                'value': accuracy,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'f1_score',\n",
        "                'value': f1,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "\n",
        "        \n",
        "    return result_df, model\n",
        "\n",
        "result_df, model = run_classifiers()\n",
        "\n",
        "model\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC_nosmote    Sub1_Toxic           accuracy=0.648              f1=0.438   smote=False\n",
            "LinearSVC_nosmote    Sub2_Engaging        accuracy=0.780              f1=0.541   smote=False\n",
            "LinearSVC_nosmote    Sub3_FactClaiming    accuracy=0.716              f1=0.535   smote=False\n",
            "LinearSVC            Sub1_Toxic           accuracy=0.621              f1=0.475   smote=True\n",
            "LinearSVC            Sub2_Engaging        accuracy=0.777              f1=0.539   smote=True\n",
            "LinearSVC            Sub3_FactClaiming    accuracy=0.711              f1=0.535   smote=True\n",
            "AdaBoost             Sub1_Toxic           accuracy=0.647              f1=0.374   smote=True\n",
            "AdaBoost             Sub2_Engaging        accuracy=0.758              f1=0.440   smote=True\n",
            "AdaBoost             Sub3_FactClaiming    accuracy=0.695              f1=0.447   smote=True\n",
            "AdaBoost_nosmote     Sub1_Toxic           accuracy=0.657              f1=0.372   smote=False\n",
            "AdaBoost_nosmote     Sub2_Engaging        accuracy=0.749              f1=0.393   smote=False\n",
            "AdaBoost_nosmote     Sub3_FactClaiming    accuracy=0.709              f1=0.476   smote=False\n",
            "MultinomialNB_nosmot Sub1_Toxic           accuracy=0.662              f1=0.429   smote=False\n",
            "MultinomialNB_nosmot Sub2_Engaging        accuracy=0.737              f1=0.486   smote=False\n",
            "MultinomialNB_nosmot Sub3_FactClaiming    accuracy=0.714              f1=0.593   smote=False\n",
            "RandomForestClassifi Sub1_Toxic           accuracy=0.647              f1=0.467   smote=True\n",
            "RandomForestClassifi Sub2_Engaging        accuracy=0.784              f1=0.426   smote=True\n",
            "RandomForestClassifi Sub3_FactClaiming    accuracy=0.731              f1=0.488   smote=True\n",
            "RandomForestClassifi Sub1_Toxic           accuracy=0.665              f1=0.468   smote=False\n",
            "RandomForestClassifi Sub2_Engaging        accuracy=0.770              f1=0.327   smote=False\n",
            "RandomForestClassifi Sub3_FactClaiming    accuracy=0.721              f1=0.433   smote=False\n",
            "BernoulliNB_nosmote  Sub1_Toxic           accuracy=0.657              f1=0.215   smote=False\n",
            "BernoulliNB_nosmote  Sub2_Engaging        accuracy=0.779              f1=0.379   smote=False\n",
            "BernoulliNB_nosmote  Sub3_FactClaiming    accuracy=0.695              f1=0.302   smote=False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style>#sk-85d2590c-7823-49eb-be41-10172563e578 {color: black;background-color: white;}#sk-85d2590c-7823-49eb-be41-10172563e578 pre{padding: 0;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-toggleable {background-color: white;}#sk-85d2590c-7823-49eb-be41-10172563e578 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-85d2590c-7823-49eb-be41-10172563e578 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-85d2590c-7823-49eb-be41-10172563e578 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-estimator:hover {background-color: #d4ebff;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-item {z-index: 1;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-parallel-item:only-child::after {width: 0;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-85d2590c-7823-49eb-be41-10172563e578 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-85d2590c-7823-49eb-be41-10172563e578\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"88986466-c8ab-40d1-8b31-559d14432b75\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"88986466-c8ab-40d1-8b31-559d14432b75\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('column_transformer',\n",
              "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                                   transformers=[('countvectorizer',\n",
              "                                                  CountVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-1',\n",
              "                                                  TfidfVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-2',\n",
              "                                                  TfidfVectorizer(use_idf=False),\n",
              "                                                  'cleaned_comment_text')])),\n",
              "                ('dense', DenseTransformer()), ('clf', BernoulliNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c1d94f63-a4bc-48a0-b346-ac680dd61836\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"c1d94f63-a4bc-48a0-b346-ac680dd61836\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                  transformers=[('countvectorizer', CountVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-1', TfidfVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-2',\n",
              "                                 TfidfVectorizer(use_idf=False),\n",
              "                                 'cleaned_comment_text')])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"9d5cfaa3-d30f-49ab-8e8e-6416c3196f46\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"9d5cfaa3-d30f-49ab-8e8e-6416c3196f46\">countvectorizer</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c698f4d8-0159-4b08-91f0-8c92f9d54600\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"c698f4d8-0159-4b08-91f0-8c92f9d54600\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c045aece-8da8-4125-a4e2-c8db477c33e3\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"c045aece-8da8-4125-a4e2-c8db477c33e3\">tfidfvectorizer-1</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"aa2d2880-1ef8-4994-a7d3-25737b3313e3\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"aa2d2880-1ef8-4994-a7d3-25737b3313e3\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"435825d7-37b1-4a49-9aeb-7daf7fa8d5f2\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"435825d7-37b1-4a49-9aeb-7daf7fa8d5f2\">tfidfvectorizer-2</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"d1f0946b-746f-4c0d-9ad6-2b43f8bb60e4\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"d1f0946b-746f-4c0d-9ad6-2b43f8bb60e4\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(use_idf=False)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"ae89ed9b-b814-4e69-aafd-8e3a953e1399\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"ae89ed9b-b814-4e69-aafd-8e3a953e1399\">remainder</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps', 'perc_exclamations', 'num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"4abbf9e2-969c-4805-a74a-9a2defeeea79\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"4abbf9e2-969c-4805-a74a-9a2defeeea79\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"fac1b068-05d9-47d1-88d8-1f2c68bd28fd\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"fac1b068-05d9-47d1-88d8-1f2c68bd28fd\">DenseTransformer</label><div class=\"sk-toggleable__content\"><pre>DenseTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"34577f3c-6889-4f6e-9c5a-e8ec29931323\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"34577f3c-6889-4f6e-9c5a-e8ec29931323\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('column_transformer',\n",
              "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                                   transformers=[('countvectorizer',\n",
              "                                                  CountVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-1',\n",
              "                                                  TfidfVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-2',\n",
              "                                                  TfidfVectorizer(use_idf=False),\n",
              "                                                  'cleaned_comment_text')])),\n",
              "                ('dense', DenseTransformer()), ('clf', BernoulliNB())])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a1buL1EozuH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "101950e8-1d46-498f-d5ce-04385772e122"
      },
      "source": [
        "def print_df(df, metric, task):\n",
        "    df = df[(df['metric'] == metric) & (df['task_name'] == task)]\n",
        "    df = df.sort_values(by=['value'], ascending=False)\n",
        "    print(df.head(3))\n",
        "    return df\n",
        "\n",
        "for task_name in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "    print('=' * 80)\n",
        "    print(task_name)\n",
        "    print('-' * len(task_name))\n",
        "    print()\n",
        "    for metric in ['accuracy', 'f1_score']:\n",
        "        print_df(result_df, metric, task_name)\n",
        "        print()\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Sub1_Toxic\n",
            "----------\n",
            "\n",
            "                        classifier   task_name    metric smote     value\n",
            "37  RandomForestClassifier_nosmote  Sub1_Toxic  accuracy     0  0.664975\n",
            "25           MultinomialNB_nosmote  Sub1_Toxic  accuracy     0  0.662437\n",
            "19                AdaBoost_nosmote  Sub1_Toxic  accuracy     0   0.65736\n",
            "\n",
            "                        classifier   task_name    metric smote     value\n",
            "8                        LinearSVC  Sub1_Toxic  f1_score     1  0.474517\n",
            "38  RandomForestClassifier_nosmote  Sub1_Toxic  f1_score     0  0.467742\n",
            "32          RandomForestClassifier  Sub1_Toxic  f1_score     1  0.467433\n",
            "\n",
            "================================================================================\n",
            "Sub2_Engaging\n",
            "-------------\n",
            "\n",
            "                classifier      task_name    metric smote     value\n",
            "33  RandomForestClassifier  Sub2_Engaging  accuracy     1  0.784264\n",
            "3        LinearSVC_nosmote  Sub2_Engaging  accuracy     0  0.780457\n",
            "45     BernoulliNB_nosmote  Sub2_Engaging  accuracy     0  0.779188\n",
            "\n",
            "               classifier      task_name    metric smote     value\n",
            "4       LinearSVC_nosmote  Sub2_Engaging  f1_score     0  0.541114\n",
            "10              LinearSVC  Sub2_Engaging  f1_score     1  0.539267\n",
            "28  MultinomialNB_nosmote  Sub2_Engaging  f1_score     0  0.486352\n",
            "\n",
            "================================================================================\n",
            "Sub3_FactClaiming\n",
            "-----------------\n",
            "\n",
            "                        classifier          task_name    metric smote     value\n",
            "35          RandomForestClassifier  Sub3_FactClaiming  accuracy     1  0.730964\n",
            "41  RandomForestClassifier_nosmote  Sub3_FactClaiming  accuracy     0  0.720812\n",
            "5                LinearSVC_nosmote  Sub3_FactClaiming  accuracy     0  0.715736\n",
            "\n",
            "               classifier          task_name    metric smote     value\n",
            "30  MultinomialNB_nosmote  Sub3_FactClaiming  f1_score     0  0.593128\n",
            "6       LinearSVC_nosmote  Sub3_FactClaiming  f1_score     0   0.53527\n",
            "12              LinearSVC  Sub3_FactClaiming  f1_score     1  0.534694\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzgub_1VakyE"
      },
      "source": [
        "# Multinomial NB pipeline (modified)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV_S-WOU3sQp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "PXqnV32AaoCL",
        "outputId": "5547b662-8535-46d5-e27a-06d6ece90ec6"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "#from imblearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.preprocessing import DenseTransformer\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "import sklearn\n",
        "import itertools\n",
        "\n",
        "MAX_COMBINATION_NUMBER=3\n",
        "\n",
        "def get_feature_column_names(df):\n",
        "    return [cname for cname in df.columns if not cname.startswith('Sub')]\n",
        "\n",
        "def get_target_column_names(df):\n",
        "    return [cname for cname in df.columns if cname.startswith('Sub')]\n",
        "\n",
        "def is_text_column(colname:str)->bool:\n",
        "    if 'text' in colname:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_text_columns(df)->list:\n",
        "    return [cn for cn in df.columns if is_text_column(cn)]\n",
        "\n",
        "def get_nontext_columns(df)->list:\n",
        "    return [cn for cn in df.columns if not is_text_column(cn)]\n",
        "\n",
        "class CustomTextProcessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, use_tfid=True):\n",
        "        vect = CountVectorizer() if use_tfid == False else TfidfVectorizer()\n",
        "        self.tweet_text_transformer = Pipeline(steps=[\n",
        "                                        ('vect', vect),\n",
        "                        ])\n",
        "\n",
        "class CustomCountVectorizer(CountVectorizer):\n",
        "    # The only difference here is that we don't return a sparse array\n",
        "    # So that this can work with a transformer\n",
        "    # Instead we return a dense array\n",
        "    def __init__(self):\n",
        "        super(CustomCountVectorizer, self).__init__()\n",
        "    \n",
        "    def fit_transform(self, X, y=None):\n",
        "        return super(CustomCountVectorizer, self).fit_transform(X.squeeze(), y).toarray()\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return super(CustomCountVectorizer, self).fit(X, y)\n",
        "\n",
        "    def transform(self, X):\n",
        "        return super(CustomCountVectorizer, self).transform(X.squeeze()).toarray()\n",
        "\n",
        "class CustomTfidVectorizer(TfidfVectorizer):\n",
        "    # The only difference here is that we don't return a sparse array\n",
        "    # So that this can work with a transformer\n",
        "    # Instead we return a dense array\n",
        "    def __init__(self):\n",
        "        super(CustomTfidVectorizer, self).__init__()\n",
        "    \n",
        "    def fit_transform(self, X, y=None):\n",
        "        return super(CustomTfidVectorizer, self).fit_transform(X.squeeze(), y).toarray()\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return super(CustomTfidVectorizer, self).fit(X, y)\n",
        "\n",
        "    def transform(self, X):\n",
        "        return super(CustomTfidVectorizer, self).transform(X.squeeze()).toarray()\n",
        "\n",
        "def run_classification(                                                     \\\n",
        "                       dataset:pd.DataFrame,                                \\\n",
        "                       target_column:str,                                   \\\n",
        "                       clf_gen_fn,                                          \\\n",
        "                       use_smote=False)->tuple:\n",
        "\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "\n",
        "    X = dataset[get_feature_column_names(dataset)]\n",
        "    y = dataset[target_column]\n",
        "    trainX, testX, trainY, testY = train_test_split(X, y, random_state=0)\n",
        "\n",
        "    text_columns = ['cleaned_comment_text']\n",
        "    numeric_columns = ['n_all_caps', 'perc_exclamations', 'num_exclamations']\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "                        remainder='drop',\n",
        "                        transformers =                                      \\\n",
        "                                    [                                       \\\n",
        "                                        ('text', CustomCountVectorizer(), text_columns), \\\n",
        "                                        ('text2', CustomTfidVectorizer(), text_columns), \\\n",
        "                                        ('num', StandardScaler(with_mean=False, with_std=False), numeric_columns)\n",
        "                                    ])\n",
        "    classif_pipeline = Pipeline([('prep', preprocessor), ('classif', clf_gen_fn())])\n",
        "\n",
        "    classif_pipeline.fit(trainX, trainY)\n",
        "    y_pred = classif_pipeline.predict(testX)\n",
        "\n",
        "    return accuracy_score(testY, y_pred), f1_score(testY, y_pred), classif_pipeline\n",
        "\n",
        "\n",
        "\n",
        "def run_classifiers():\n",
        "\n",
        "    classifiers = {\n",
        "        \"LinearSVC_nosmote\": (False, lambda: LinearSVC(),),\n",
        "        \"MultinomialNB_nosmote\": (False, lambda: MultinomialNB(),),\n",
        "        \"RandomForestClassifier\": (True, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "    }\n",
        "\n",
        "    result_df = pd.DataFrame(                                                           \\\n",
        "                    {                                                       \\\n",
        "                        'classifier': pd.Series('str'),                     \\\n",
        "                        'task_name': pd.Series('str'),                      \\\n",
        "                        'metric': pd.Series('str'),                         \\\n",
        "                        'smote': pd.Series(int),                            \\\n",
        "                        'value': pd.Series(float),                          \\\n",
        "                    })\n",
        "\n",
        "    model = None\n",
        "    for clfname, value in classifiers.items():\n",
        "        use_smote, clfgen = value\n",
        "        for colname in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "            accuracy, f1, model = run_classification(train_df, colname, clfgen, use_smote)\n",
        "            print(f\"{clfname:20.20s} {colname:20.20s} accuracy={accuracy:1.3f}              f1={f1:1.3f}   smote={use_smote}\")\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'accuracy',\n",
        "                'value': accuracy,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'f1_score',\n",
        "                'value': f1,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "\n",
        "        \n",
        "    return result_df, model\n",
        "\n",
        "result_df, model = run_classifiers()\n",
        "\n",
        "model\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC_nosmote    Sub1_Toxic           accuracy=0.654              f1=0.444   smote=False\n",
            "LinearSVC_nosmote    Sub2_Engaging        accuracy=0.780              f1=0.534   smote=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC_nosmote    Sub3_FactClaiming    accuracy=0.716              f1=0.535   smote=False\n",
            "MultinomialNB_nosmot Sub1_Toxic           accuracy=0.678              f1=0.464   smote=False\n",
            "MultinomialNB_nosmot Sub2_Engaging        accuracy=0.741              f1=0.482   smote=False\n",
            "MultinomialNB_nosmot Sub3_FactClaiming    accuracy=0.713              f1=0.578   smote=False\n",
            "RandomForestClassifi Sub1_Toxic           accuracy=0.669              f1=0.468   smote=True\n",
            "RandomForestClassifi Sub2_Engaging        accuracy=0.774              f1=0.346   smote=True\n",
            "RandomForestClassifi Sub3_FactClaiming    accuracy=0.726              f1=0.449   smote=True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style>#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 {color: black;background-color: white;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 pre{padding: 0;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-toggleable {background-color: white;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-estimator:hover {background-color: #d4ebff;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-item {z-index: 1;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-parallel-item:only-child::after {width: 0;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-0b5e08d2-42df-4815-b916-5f503ce80f38 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-0b5e08d2-42df-4815-b916-5f503ce80f38\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"35c814e9-22d8-4e1b-b21a-0c1922c38b96\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"35c814e9-22d8-4e1b-b21a-0c1922c38b96\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('prep',\n",
              "                 ColumnTransformer(transformers=[('text',\n",
              "                                                  CustomCountVectorizer(),\n",
              "                                                  ['cleaned_comment_text']),\n",
              "                                                 ('text2',\n",
              "                                                  CustomTfidVectorizer(),\n",
              "                                                  ['cleaned_comment_text']),\n",
              "                                                 ('num',\n",
              "                                                  StandardScaler(with_mean=False,\n",
              "                                                                 with_std=False),\n",
              "                                                  ['n_all_caps',\n",
              "                                                   'perc_exclamations',\n",
              "                                                   'num_exclamations'])])),\n",
              "                ('classif', RandomForestClassifier(n_jobs=-1))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"413ec237-e904-440d-a69e-c6f1892363ef\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"413ec237-e904-440d-a69e-c6f1892363ef\">prep: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('text', CustomCountVectorizer(),\n",
              "                                 ['cleaned_comment_text']),\n",
              "                                ('text2', CustomTfidVectorizer(),\n",
              "                                 ['cleaned_comment_text']),\n",
              "                                ('num',\n",
              "                                 StandardScaler(with_mean=False,\n",
              "                                                with_std=False),\n",
              "                                 ['n_all_caps', 'perc_exclamations',\n",
              "                                  'num_exclamations'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c32894cf-9e71-4ec5-a080-ce5a9711a625\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"c32894cf-9e71-4ec5-a080-ce5a9711a625\">text</label><div class=\"sk-toggleable__content\"><pre>['cleaned_comment_text']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"33711a6d-5293-40f1-b61d-e182e5d4a4db\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"33711a6d-5293-40f1-b61d-e182e5d4a4db\">CustomCountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CustomCountVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"cb59a495-0b96-4639-a593-9d6839b36e1d\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"cb59a495-0b96-4639-a593-9d6839b36e1d\">text2</label><div class=\"sk-toggleable__content\"><pre>['cleaned_comment_text']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5d526c89-78ab-4296-9aef-eb35d22ffbde\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"5d526c89-78ab-4296-9aef-eb35d22ffbde\">CustomTfidVectorizer</label><div class=\"sk-toggleable__content\"><pre>CustomTfidVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"4e43a6cf-bdc7-40b0-ac35-d5c5a6fdc38d\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"4e43a6cf-bdc7-40b0-ac35-d5c5a6fdc38d\">num</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps', 'perc_exclamations', 'num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"fc394fce-5e7e-46fe-b2b3-5b6e218941d0\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"fc394fce-5e7e-46fe-b2b3-5b6e218941d0\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler(with_mean=False, with_std=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b833c8f6-90af-4606-ac4b-23a777661c75\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"b833c8f6-90af-4606-ac4b-23a777661c75\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=-1)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('prep',\n",
              "                 ColumnTransformer(transformers=[('text',\n",
              "                                                  CustomCountVectorizer(),\n",
              "                                                  ['cleaned_comment_text']),\n",
              "                                                 ('text2',\n",
              "                                                  CustomTfidVectorizer(),\n",
              "                                                  ['cleaned_comment_text']),\n",
              "                                                 ('num',\n",
              "                                                  StandardScaler(with_mean=False,\n",
              "                                                                 with_std=False),\n",
              "                                                  ['n_all_caps',\n",
              "                                                   'perc_exclamations',\n",
              "                                                   'num_exclamations'])])),\n",
              "                ('classif', RandomForestClassifier(n_jobs=-1))])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EiRR4fd9sb7",
        "outputId": "cf13e67d-54db-42ca-b7bb-3be2a746b0d0"
      },
      "source": [
        "def print_df(df, metric, task):\n",
        "    df = df[(df['metric'] == metric) & (df['task_name'] == task)]\n",
        "    df = df.sort_values(by=['value'], ascending=False)\n",
        "    print(df.head(3))\n",
        "    return df\n",
        "\n",
        "for task_name in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "    print('=' * 80)\n",
        "    print(task_name)\n",
        "    print('-' * len(task_name))\n",
        "    print()\n",
        "    for metric in ['accuracy', 'f1_score']:\n",
        "        print_df(result_df, metric, task_name)\n",
        "        print()\n"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Sub1_Toxic\n",
            "----------\n",
            "\n",
            "                classifier   task_name    metric smote     value\n",
            "7    MultinomialNB_nosmote  Sub1_Toxic  accuracy     0   0.67132\n",
            "13  RandomForestClassifier  Sub1_Toxic  accuracy     1  0.662437\n",
            "1        LinearSVC_nosmote  Sub1_Toxic  accuracy     0  0.643401\n",
            "\n",
            "                classifier   task_name    metric smote     value\n",
            "8    MultinomialNB_nosmote  Sub1_Toxic  f1_score     0  0.470348\n",
            "14  RandomForestClassifier  Sub1_Toxic  f1_score     1  0.454918\n",
            "2        LinearSVC_nosmote  Sub1_Toxic  f1_score     0    0.4158\n",
            "\n",
            "================================================================================\n",
            "Sub2_Engaging\n",
            "-------------\n",
            "\n",
            "                classifier      task_name    metric smote     value\n",
            "3        LinearSVC_nosmote  Sub2_Engaging  accuracy     0  0.772843\n",
            "15  RandomForestClassifier  Sub2_Engaging  accuracy     1  0.770305\n",
            "9    MultinomialNB_nosmote  Sub2_Engaging  accuracy     0  0.742386\n",
            "\n",
            "                classifier      task_name    metric smote     value\n",
            "4        LinearSVC_nosmote  Sub2_Engaging  f1_score     0   0.51752\n",
            "10   MultinomialNB_nosmote  Sub2_Engaging  f1_score     0   0.46438\n",
            "16  RandomForestClassifier  Sub2_Engaging  f1_score     1  0.316981\n",
            "\n",
            "================================================================================\n",
            "Sub3_FactClaiming\n",
            "-----------------\n",
            "\n",
            "                classifier          task_name    metric smote     value\n",
            "17  RandomForestClassifier  Sub3_FactClaiming  accuracy     1  0.730964\n",
            "5        LinearSVC_nosmote  Sub3_FactClaiming  accuracy     0  0.718274\n",
            "11   MultinomialNB_nosmote  Sub3_FactClaiming  accuracy     0  0.715736\n",
            "\n",
            "                classifier          task_name    metric smote     value\n",
            "12   MultinomialNB_nosmote  Sub3_FactClaiming  f1_score     0  0.564202\n",
            "6        LinearSVC_nosmote  Sub3_FactClaiming  f1_score     0  0.535565\n",
            "18  RandomForestClassifier  Sub3_FactClaiming  f1_score     1  0.472637\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mvNlGAA-eN9"
      },
      "source": [
        "#result_df\n",
        "result_df[result_df['metric'] == 'f1_score'].sort_values(by='value', ascending=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}