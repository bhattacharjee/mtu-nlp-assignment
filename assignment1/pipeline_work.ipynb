{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pipeline_work.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPqKtJZLQWHVVkSeqGbvHpP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment1/pipeline_work.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERvBOEn0wss6"
      },
      "source": [
        "!pip install spacy  nltk spacymoji huggingface -q       >/dev/null 2>&1         \n",
        "!pip install -q -U tensorflow-text                      >/dev/null 2>&1\n",
        "!pip install -q tf-models-official                      >/dev/null 2>&1\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "!pip install transformers                               >/dev/null 2>&1\n",
        "\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "\n",
        "!pip install mlxtend                                    >/dev/null 2>&1\n",
        "!pip install imblearn                                   >/dev/null 2>&1\n",
        "\n",
        "# handling emojis\n",
        "!pip install demoji                                     >/dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-DAX9VLZlw3"
      },
      "source": [
        "import requests\n",
        "from functools import lru_cache\n",
        "import sklearn\n",
        "\n",
        "@lru_cache(maxsize=10)\n",
        "def get_train_test_files():\n",
        "    TRAIN_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Test_For_Evaluation.csv'\n",
        "    TRAIN_FILE_LOCAL = 'Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE_LOCAL = 'Assessment1_Toxic_Test.csv'\n",
        "\n",
        "    def download(url, localfile):\n",
        "        with open(localfile, 'wb') as f:\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            f.write(r.content)\n",
        "\n",
        "    download(TRAIN_FILE, TRAIN_FILE_LOCAL)\n",
        "    download(TEST_FILE, TEST_FILE_LOCAL)\n",
        "\n",
        "    return TRAIN_FILE_LOCAL, TEST_FILE_LOCAL\n",
        "\n",
        "def seed_random():\n",
        "    import numpy as np\n",
        "    import random\n",
        "    np.random.seed(0)\n",
        "    random.seed(0)\n",
        "\n",
        "sklearn.set_config(display=\"diagram\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrlJ1H1maDxM"
      },
      "source": [
        "# Functions to read the CSV and do basic cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVfYulLnuGVJ"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import demoji\n",
        "\n",
        "def remove_roles(line:str)->str:\n",
        "    # Remove texts like @USER, @MODERATOR etc\n",
        "    pat = re.compile(u'\\@[A-Za-z]+')\n",
        "    return re.sub(pat, '', line)\n",
        "\n",
        "def get_train_test_df():\n",
        "    train_csv, test_csv = get_train_test_files()\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "    return train_df, test_df\n",
        "\n",
        "def remove_emojis(line:str)->str:\n",
        "    # Replace emojis with their description, eg __thumbs_down__\n",
        "    demoji_str = demoji.replace_with_desc(line, sep=\" ::: \")\n",
        "    if (demoji_str == line):\n",
        "        return line\n",
        "    \n",
        "    inEmoji = False\n",
        "    currentEmojiWords = []\n",
        "    allWords = []\n",
        "\n",
        "    def accumulate(word:str)->None:\n",
        "        nonlocal inEmoji\n",
        "        nonlocal currentEmojiWords\n",
        "        nonlocal allWords\n",
        "        if not inEmoji and word != \":::\":\n",
        "            allWords.append(word)\n",
        "        elif inEmoji:\n",
        "            if word == ':::':\n",
        "                currentEmoji = \"_\".join(currentEmojiWords)\n",
        "                currentEmoji = \"__\" + currentEmoji + \"__\"\n",
        "                allWords.append(currentEmoji)\n",
        "                currentEmojiWords = []\n",
        "            else:\n",
        "                currentEmojiWords.append(word)\n",
        "        else: # Not in emoji but ::: is true\n",
        "            inEmoji = True\n",
        "\n",
        "    [accumulate(word) for word in demoji_str.split()]\n",
        "\n",
        "    sentence = \" \".join(allWords)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def remove_ellipses(line:str)->str:\n",
        "    pat = re.compile(u'\\.\\.+')\n",
        "    return re.sub(pat, ' ', line)\n",
        "\n",
        "def to_lower(line:str)->str:\n",
        "    return line.lower()\n",
        "\n",
        "def replace_number_with_tag(line:str)->str:\n",
        "    line = re.sub(\"\\s\\d*((\\.|\\,)\\d+)?\\s\", \" nummer \", line)\n",
        "    line = re.sub('\\s\\d+$', '', line)\n",
        "    line = re.sub('^\\d+\\s', '', line)\n",
        "    return line\n",
        "\n",
        "def remove_urls(line:str)->str:\n",
        "    return re.sub('https?:\\/\\/\\S+', ' hyperlink ', line)\n",
        "\n",
        "def basic_clean(s:pd.Series)->pd.Series:\n",
        "    return s.map(to_lower)                                                  \\\n",
        "            .map(remove_emojis)                                             \\\n",
        "            .map(remove_roles)                                              \\\n",
        "            .map(remove_ellipses)                                           \\\n",
        "            .map(replace_number_with_tag)                                   \\\n",
        "            .map(remove_urls)\n",
        "\n",
        "def get_clean_train_test_df()->tuple:\n",
        "    train_df, test_df = get_train_test_df()\n",
        "    train_df['comment_text'] = basic_clean(train_df['comment_text'])\n",
        "    test_df['comment_text'] = basic_clean(test_df['comment_text'])\n",
        "    return train_df, test_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6_dto2JaR2f"
      },
      "source": [
        "# Clean using Spacy and Enrich"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8YSZZ4vxTk5"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string\n",
        "import spacy\n",
        "from spacymoji import Emoji\n",
        "import  de_core_news_sm\n",
        "\n",
        "def is_punct_only(token:str)->bool:\n",
        "    for c in list(token):\n",
        "        if c not in string.punctuation:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_same(l1:list, l2:list)->bool:\n",
        "    if (len(l1) != len(l2)):\n",
        "        return False\n",
        "    for x, y in zip(l1, l2):\n",
        "        if x != y:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def get_num_of_allcap_words(s:str)->int:\n",
        "    def is_allcaps(s:str)->bool:\n",
        "        if (len(s) < 3):\n",
        "            return False\n",
        "        for c in list(s):\n",
        "            if not (\\\n",
        "                    (ord(c) <=ord('Z') and ord(c) >= ord('A')) or           \\\n",
        "                    (ord(c) >= ord('0') and ord(c) <= ord('9'))             \\\n",
        "                    ):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    if len(s) < 3:\n",
        "        return 0\n",
        "    tokens = [w.strip() for w in s.split()]\n",
        "    return sum([1 for t in tokens if is_allcaps(t)])\n",
        "\n",
        "def get_percentage_of_excalamations(s:str)->float:\n",
        "    if len(s) == 0:\n",
        "        return 0.0\n",
        "    exclamation_count = sum([1 for c in list(s) if c == '!'])\n",
        "    return exclamation_count / len(s)\n",
        "\n",
        "\n",
        "def is_empty_string(s:str)->bool:\n",
        "    if s == '' or s == None:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def do_basic_nlp_cleaning(line:str)->str:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(line)\n",
        "\n",
        "    # Some tokens start with a punctuation, remove the first one\n",
        "    def remove_first_punctuation(tok:str)->str:\n",
        "        return                                                              \\\n",
        "            tok[1:]                                                         \\\n",
        "            if tok[0] in set(string.punctuation) and len(tok) != 0          \\\n",
        "            else tok\n",
        "\n",
        "    tokens = [remove_first_punctuation(w) for w in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"german\"))\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [w for w in tokens if not is_punct_only(w)]\n",
        "\n",
        "    # Stem words\n",
        "    stem = SnowballStemmer('german')\n",
        "    tokens = [stem.stem(w) for w in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def get_cleaning_function(remove_named_ents:bool=True, pos_tagging:bool=False):\n",
        "    #nlp = spacy.load(\"de_dep_news_trf\")\n",
        "    #nlp = spacy.load(\"de_core_news_sm\")\n",
        "    nlp = de_core_news_sm.load()\n",
        "    emoji = Emoji(nlp)\n",
        "    nlp.add_pipe(emoji, first=True)\n",
        "    stopwords = spacy.lang.de.stop_words.STOP_WORDS\n",
        "\n",
        "    def do_basic_nlp_cleaning(line:str)->str:\n",
        "        def is_interesting_token(token, doc):\n",
        "            if token.pos_ in set(['NUM', 'SYM']):\n",
        "                return False\n",
        "            if remove_named_ents:\n",
        "                for e in doc.ents:\n",
        "                    for t in e:\n",
        "                        if token.text == t.text:\n",
        "                            return False\n",
        "            if token.text in stopwords:\n",
        "                return False\n",
        "            if (token.is_punct):\n",
        "                return False\n",
        "            #if token._.is_emoji:\n",
        "            #    return False\n",
        "            return True\n",
        "\n",
        "        def remove_terminal_punctuations(word):\n",
        "            word = word.strip()\n",
        "            while word != \"\" and word[0] in list(string.punctuation):\n",
        "                word = word[1:]\n",
        "            while word != \"\" and word[-1] in list(string.punctuation):\n",
        "                word = word[:-1]\n",
        "            return word\n",
        "\n",
        "        def get_final_string(tok, doc):\n",
        "            lemma = tok.lemma_.lower()\n",
        "            if pos_tagging:\n",
        "                lemma = lemma + \":\" + tok.pos_\n",
        "                lemma = lemma + \":\" + tok.tag_\n",
        "            return lemma\n",
        "\n",
        "        doc = nlp(line)\n",
        "        words = [get_final_string(tok, doc) for tok in doc if is_interesting_token(tok, doc)]\n",
        "        words = [remove_terminal_punctuations(word) for word in words]\n",
        "        words = [word for word in words if word != \"\"]\n",
        "        return  \" \".join(words)\n",
        "\n",
        "    return do_basic_nlp_cleaning\n",
        "\n",
        "def get_enriched_dataset(df):\n",
        "    df['cleaned_comment_text'] = df['comment_text'].map(cleaning_fn)\n",
        "    df['n_all_caps'] = df['comment_text'].map(get_num_of_allcap_words)\n",
        "    df['perc_exclamations'] = df['comment_text'].map(get_percentage_of_excalamations)\n",
        "    df['num_exclamations'] = df['comment_text'].map(lambda s: sum([1 for x in list(s) if x == '!']))\n",
        "    return df\n",
        "\n",
        "cleaning_fn = get_cleaning_function(remove_named_ents=True, pos_tagging=True)\n",
        "train_df, test_df = get_clean_train_test_df()\n",
        "train_df = get_enriched_dataset(train_df)\n",
        "test_df = get_enriched_dataset(test_df)\n",
        "#empty_rows = train_df['cleaned_comment_text'].map(is_empty_string)\n",
        "#train_df = train_df[~ empty_rows]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWTtxHw0aa4N"
      },
      "source": [
        "# Print Enriched Training DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "n3q-TKbwZRBJ",
        "outputId": "88c45ad1-e9e4-4c5a-9796-af1cd89364cf"
      },
      "source": [
        "train_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>Sub1_Toxic</th>\n",
              "      <th>Sub2_Engaging</th>\n",
              "      <th>Sub3_FactClaiming</th>\n",
              "      <th>cleaned_comment_text</th>\n",
              "      <th>n_all_caps</th>\n",
              "      <th>perc_exclamations</th>\n",
              "      <th>num_exclamations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gestern bei illner, montag bei nummer  ist das...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>gestern:ADV:ADV illner:ADJ:ADJA montag:NOUN:NN...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mein gott der war erst gestern bei illner. die...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>gestern:ADV:ADV redaktionen:NOUN:NN versagen:V...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>die cdu lässt das so wie so nicht zu . sagen ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>SPACE:_SP lässt:VERB:VVFIN sagen:VERB:VVFIN re...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bei meiner beschissenen rente als 2x geschiede...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>beschissen:ADJ:ADJA rente:NOUN:NN geschieden:A...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wer nummer jahre zum mindestlohn arbeiten muß,...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>nummer:ADJ:ADJA mindestlohn:NOUN:NN arbeiten:V...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.005025</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3189</th>\n",
              "      <td>hier mal eine info. flüchtlinge werden nummer ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>mal:ADV:ADV info:NOUN:NN flüchtlinge:NOUN:NN n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3190</th>\n",
              "      <td>.aha .mal abwarten kommt bei uns auch .firmen ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>aha:X:XY mal:X:XY abwarten:NOUN:NN entlassen:P...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3191</th>\n",
              "      <td>.so ist es</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SPACE:_SP so:PROPN:NE</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3192</th>\n",
              "      <td>.die warten da</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SPACE:_SP die:X:XY warten:NOUN:NN</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3193</th>\n",
              "      <td>.das bekommen die gesagt wie sich verhalten s...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>SPACE:_SP bekommen:VERB:VVFIN verhalten:VERB:V...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3194 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           comment_text  ...  num_exclamations\n",
              "0     gestern bei illner, montag bei nummer  ist das...  ...                 0\n",
              "1     mein gott der war erst gestern bei illner. die...  ...                 0\n",
              "2      die cdu lässt das so wie so nicht zu . sagen ...  ...                 0\n",
              "3     bei meiner beschissenen rente als 2x geschiede...  ...                 0\n",
              "4     wer nummer jahre zum mindestlohn arbeiten muß,...  ...                 3\n",
              "...                                                 ...  ...               ...\n",
              "3189  hier mal eine info. flüchtlinge werden nummer ...  ...                 0\n",
              "3190  .aha .mal abwarten kommt bei uns auch .firmen ...  ...                 0\n",
              "3191                                         .so ist es  ...                 0\n",
              "3192                                     .die warten da  ...                 0\n",
              "3193   .das bekommen die gesagt wie sich verhalten s...  ...                 0\n",
              "\n",
              "[3194 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68DFX_fjYgQz"
      },
      "source": [
        "# Multinomial NB (original)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 781
        },
        "id": "zL5Boe5qYfF4",
        "outputId": "e65cb9ef-3891-435f-ed43-a7c05c648f5e"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.pipeline import Pipeline\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE \n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.preprocessing import DenseTransformer\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "def get_feature_column_names(df):\n",
        "    return [cname for cname in df.columns if not cname.startswith('Sub')]\n",
        "\n",
        "def get_target_column_names(df):\n",
        "    return [cname for cname in df.columns if cname.startswith('Sub')]\n",
        "\n",
        "def is_text_column(colname:str)->bool:\n",
        "    if 'text' in colname:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_text_columns(df)->list:\n",
        "    return [cn for cn in df.columns if is_text_column(cn)]\n",
        "\n",
        "def get_nontext_columns(df)->list:\n",
        "    return [cn for cn in df.columns if not is_text_column(cn)]\n",
        "\n",
        "def run_classification(                                                     \\\n",
        "                       dataset:pd.DataFrame,                                \\\n",
        "                       target_column:str,                                   \\\n",
        "                       clf_gen_fn,                                          \\\n",
        "                       use_smote=False)->tuple:\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'n_all_caps']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'num_exclamations']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'perc_exclamations']]\n",
        "    X = dataset[get_feature_column_names(dataset)]\n",
        "    y = dataset[target_column]\n",
        "    trainX, testX, trainY, testY = train_test_split(X, y, random_state=0)\n",
        "\n",
        "    def get_text_pipeline():\n",
        "        return Pipeline(                                                    \\\n",
        "                        [                                                   \\\n",
        "                            ('cv', CountVectorizer(),),                     \\\n",
        "                            ('tfid', TfidfVectorizer()),                    \\\n",
        "                        ])\n",
        "\n",
        "                        \n",
        "    column_trans = make_column_transformer(                                 \\\n",
        "                            (CountVectorizer(ngram_range=(1,1)), 'cleaned_comment_text'),   \\\n",
        "                            (TfidfVectorizer(use_idf=True), 'cleaned_comment_text'),    \\\n",
        "                            (TfidfVectorizer(use_idf=False), 'cleaned_comment_text'),    \\\n",
        "                            remainder=MinMaxScaler(),                       \\\n",
        "                        )\n",
        "\n",
        "    if use_smote:\n",
        "        classif_pipeline = Pipeline(                                        \\\n",
        "                                [                                           \\\n",
        "                                    ('column_transformer', column_trans),   \\\n",
        "                                    ('dense', DenseTransformer()),          \\\n",
        "                                    ('smote', SMOTE(n_jobs=-1)),            \\\n",
        "                                    ('clf', clf_gen_fn()),                  \\\n",
        "                                ])\n",
        "    else:\n",
        "        classif_pipeline = Pipeline(                                        \\\n",
        "                                [                                           \\\n",
        "                                    ('column_transformer', column_trans),   \\\n",
        "                                    ('dense', DenseTransformer()),          \\\n",
        "                                    ('clf', clf_gen_fn()),                  \\\n",
        "                                ])\n",
        "    \n",
        "    \n",
        "    classif_pipeline.fit(trainX, trainY)\n",
        "    y_pred = classif_pipeline.predict(testX)\n",
        "\n",
        "    return accuracy_score(testY, y_pred), f1_score(testY, y_pred), classif_pipeline\n",
        "\n",
        "def run_classifiers():\n",
        "\n",
        "    classifiers = {\n",
        "        \"LinearSVC_nosmote\": (False, lambda: LinearSVC(),),\n",
        "        \"LinearSVC\": (True, lambda: LinearSVC(),),\n",
        "        #\"AdaBoost\": (True, lambda: AdaBoostClassifier(),),\n",
        "        #\"AdaBoost_nosmote\": (False, lambda: AdaBoostClassifier(),),\n",
        "        \"MultinomialNB_nosmote\": (False, lambda: MultinomialNB(),),\n",
        "        \"RandomForestClassifier\": (True, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "        \"RandomForestClassifier_nosmote\": (False, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "        \"BernoulliNB_nosmote\": (False, lambda: BernoulliNB(),),\n",
        "    }\n",
        "\n",
        "    result_df = pd.DataFrame(                                                           \\\n",
        "                    {                                                       \\\n",
        "                        'classifier': pd.Series('str'),                     \\\n",
        "                        'task_name': pd.Series('str'),                      \\\n",
        "                        'metric': pd.Series('str'),                         \\\n",
        "                        'smote': pd.Series(int),                            \\\n",
        "                        'value': pd.Series(float),                          \\\n",
        "                    })\n",
        "\n",
        "    for clfname, value in classifiers.items():\n",
        "        use_smote, clfgen = value\n",
        "        for colname in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "            accuracy, f1, model = run_classification(train_df, colname, clfgen, use_smote)\n",
        "            print(f\"{clfname:20.20s} {colname:20.20s} accuracy={accuracy:1.3f}              f1={f1:1.3f}   smote={use_smote}\")\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'accuracy',\n",
        "                'value': accuracy,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'f1_score',\n",
        "                'value': f1,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "\n",
        "        \n",
        "    return result_df, model\n",
        "\n",
        "seed_random()\n",
        "result_df, model = run_classifiers()\n",
        "\n",
        "model\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC_nosmote    Sub1_Toxic           accuracy=0.662              f1=0.471   smote=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC_nosmote    Sub2_Engaging        accuracy=0.816              f1=0.584   smote=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC_nosmote    Sub3_FactClaiming    accuracy=0.741              f1=0.578   smote=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC            Sub1_Toxic           accuracy=0.645              f1=0.470   smote=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC            Sub2_Engaging        accuracy=0.810              f1=0.582   smote=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC            Sub3_FactClaiming    accuracy=0.745              f1=0.587   smote=True\n",
            "MultinomialNB_nosmot Sub1_Toxic           accuracy=0.660              f1=0.205   smote=False\n",
            "MultinomialNB_nosmot Sub2_Engaging        accuracy=0.797              f1=0.580   smote=False\n",
            "MultinomialNB_nosmot Sub3_FactClaiming    accuracy=0.735              f1=0.600   smote=False\n",
            "RandomForestClassifi Sub1_Toxic           accuracy=0.658              f1=0.244   smote=True\n",
            "RandomForestClassifi Sub2_Engaging        accuracy=0.831              f1=0.620   smote=True\n",
            "RandomForestClassifi Sub3_FactClaiming    accuracy=0.747              f1=0.570   smote=True\n",
            "RandomForestClassifi Sub1_Toxic           accuracy=0.662              f1=0.182   smote=False\n",
            "RandomForestClassifi Sub2_Engaging        accuracy=0.834              f1=0.578   smote=False\n",
            "RandomForestClassifi Sub3_FactClaiming    accuracy=0.755              f1=0.522   smote=False\n",
            "BernoulliNB_nosmote  Sub1_Toxic           accuracy=0.672              f1=0.220   smote=False\n",
            "BernoulliNB_nosmote  Sub2_Engaging        accuracy=0.815              f1=0.493   smote=False\n",
            "BernoulliNB_nosmote  Sub3_FactClaiming    accuracy=0.737              f1=0.456   smote=False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style>#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 {color: black;background-color: white;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 pre{padding: 0;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-toggleable {background-color: white;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-estimator:hover {background-color: #d4ebff;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-item {z-index: 1;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-parallel-item:only-child::after {width: 0;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-ed6e0821-92bd-45a5-84fb-4b6181f97619 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-ed6e0821-92bd-45a5-84fb-4b6181f97619\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"3a7969e5-b9c0-4fb0-9645-dee817240413\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"3a7969e5-b9c0-4fb0-9645-dee817240413\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('column_transformer',\n",
              "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                                   transformers=[('countvectorizer',\n",
              "                                                  CountVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-1',\n",
              "                                                  TfidfVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-2',\n",
              "                                                  TfidfVectorizer(use_idf=False),\n",
              "                                                  'cleaned_comment_text')])),\n",
              "                ('dense', DenseTransformer()), ('clf', BernoulliNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"4f42418f-5e95-44cc-af56-35684f359e68\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"4f42418f-5e95-44cc-af56-35684f359e68\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                  transformers=[('countvectorizer', CountVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-1', TfidfVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-2',\n",
              "                                 TfidfVectorizer(use_idf=False),\n",
              "                                 'cleaned_comment_text')])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"a8f9cfb3-39cc-490f-812b-0515248ae535\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"a8f9cfb3-39cc-490f-812b-0515248ae535\">countvectorizer</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"73abe902-8847-476b-8339-ce580373ffbe\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"73abe902-8847-476b-8339-ce580373ffbe\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"1bce929a-50cf-40f3-b9dd-866ac7fbc327\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"1bce929a-50cf-40f3-b9dd-866ac7fbc327\">tfidfvectorizer-1</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"840cfe6d-c9a3-4b70-b58e-d7bdada3e9a3\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"840cfe6d-c9a3-4b70-b58e-d7bdada3e9a3\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"0333bbd6-682c-4334-ab0c-d94a38fa77d8\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"0333bbd6-682c-4334-ab0c-d94a38fa77d8\">tfidfvectorizer-2</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f8fbe2da-18e0-4cb7-b3b2-829036820669\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"f8fbe2da-18e0-4cb7-b3b2-829036820669\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(use_idf=False)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"3f41ced7-b176-421a-8a26-4c0f33a62214\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"3f41ced7-b176-421a-8a26-4c0f33a62214\">remainder</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps', 'perc_exclamations', 'num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"41c608cf-6757-4893-b08f-7c266a80ccc0\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"41c608cf-6757-4893-b08f-7c266a80ccc0\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"6a465a59-7761-4091-aa85-5fb625e5a68c\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"6a465a59-7761-4091-aa85-5fb625e5a68c\">DenseTransformer</label><div class=\"sk-toggleable__content\"><pre>DenseTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"3d551385-864c-4c92-afc5-ad64ade17c3e\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"3d551385-864c-4c92-afc5-ad64ade17c3e\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('column_transformer',\n",
              "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                                   transformers=[('countvectorizer',\n",
              "                                                  CountVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-1',\n",
              "                                                  TfidfVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-2',\n",
              "                                                  TfidfVectorizer(use_idf=False),\n",
              "                                                  'cleaned_comment_text')])),\n",
              "                ('dense', DenseTransformer()), ('clf', BernoulliNB())])"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a1buL1EozuH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09a36f10-c433-4b7a-a381-1ff62bb8ab03"
      },
      "source": [
        "def print_df(df, metric, task):\n",
        "    df = df[(df['metric'] == metric) & (df['task_name'] == task)]\n",
        "    df = df.sort_values(by=['value'], ascending=False)\n",
        "    print(df.head(3))\n",
        "    return df\n",
        "\n",
        "for task_name in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "    print('=' * 80)\n",
        "    print(task_name)\n",
        "    print('-' * len(task_name))\n",
        "    print()\n",
        "    for metric in ['accuracy', 'f1_score']:\n",
        "        print_df(result_df, metric, task_name)\n",
        "        print()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Sub1_Toxic\n",
            "----------\n",
            "\n",
            "                        classifier   task_name    metric smote     value\n",
            "31             BernoulliNB_nosmote  Sub1_Toxic  accuracy     0   0.67209\n",
            "1                LinearSVC_nosmote  Sub1_Toxic  accuracy     0  0.662078\n",
            "25  RandomForestClassifier_nosmote  Sub1_Toxic  accuracy     0  0.662078\n",
            "\n",
            "                classifier   task_name    metric smote     value\n",
            "2        LinearSVC_nosmote  Sub1_Toxic  f1_score     0  0.470588\n",
            "8                LinearSVC  Sub1_Toxic  f1_score     1  0.470149\n",
            "20  RandomForestClassifier  Sub1_Toxic  f1_score     1  0.243767\n",
            "\n",
            "================================================================================\n",
            "Sub2_Engaging\n",
            "-------------\n",
            "\n",
            "                        classifier      task_name    metric smote     value\n",
            "27  RandomForestClassifier_nosmote  Sub2_Engaging  accuracy     0  0.833542\n",
            "21          RandomForestClassifier  Sub2_Engaging  accuracy     1  0.831039\n",
            "3                LinearSVC_nosmote  Sub2_Engaging  accuracy     0   0.81602\n",
            "\n",
            "                classifier      task_name    metric smote     value\n",
            "22  RandomForestClassifier  Sub2_Engaging  f1_score     1  0.619718\n",
            "4        LinearSVC_nosmote  Sub2_Engaging  f1_score     0  0.583569\n",
            "10               LinearSVC  Sub2_Engaging  f1_score     1  0.582418\n",
            "\n",
            "================================================================================\n",
            "Sub3_FactClaiming\n",
            "-----------------\n",
            "\n",
            "                        classifier          task_name    metric smote     value\n",
            "29  RandomForestClassifier_nosmote  Sub3_FactClaiming  accuracy     0  0.754693\n",
            "23          RandomForestClassifier  Sub3_FactClaiming  accuracy     1  0.747184\n",
            "11                       LinearSVC  Sub3_FactClaiming  accuracy     1  0.744681\n",
            "\n",
            "               classifier          task_name    metric smote     value\n",
            "18  MultinomialNB_nosmote  Sub3_FactClaiming  f1_score     0       0.6\n",
            "12              LinearSVC  Sub3_FactClaiming  f1_score     1  0.587045\n",
            "6       LinearSVC_nosmote  Sub3_FactClaiming  f1_score     0  0.578411\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzgub_1VakyE"
      },
      "source": [
        "# Multinomial NB pipeline (modified)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV_S-WOU3sQp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "PXqnV32AaoCL",
        "outputId": "e08728f2-8a1e-4f48-9fe0-4b5aff03fac2"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "#from imblearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.preprocessing import DenseTransformer\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "import sklearn\n",
        "import itertools\n",
        "\n",
        "MAX_COMBINATION_NUMBER=3\n",
        "\n",
        "def get_feature_column_names(df):\n",
        "    return [cname for cname in df.columns if not cname.startswith('Sub')]\n",
        "\n",
        "def get_target_column_names(df):\n",
        "    return [cname for cname in df.columns if cname.startswith('Sub')]\n",
        "\n",
        "def is_text_column(colname:str)->bool:\n",
        "    if 'text' in colname:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_text_columns(df)->list:\n",
        "    return [cn for cn in df.columns if is_text_column(cn)]\n",
        "\n",
        "def get_nontext_columns(df)->list:\n",
        "    return [cn for cn in df.columns if not is_text_column(cn)]\n",
        "\n",
        "class CustomTextProcessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, use_tfid=True):\n",
        "        vect = CountVectorizer() if use_tfid == False else TfidfVectorizer()\n",
        "        self.tweet_text_transformer = Pipeline(steps=[\n",
        "                                        ('vect', vect),\n",
        "                        ])\n",
        "\n",
        "class CustomCountVectorizer(CountVectorizer):\n",
        "    # The only difference here is that we don't return a sparse array\n",
        "    # So that this can work with a transformer\n",
        "    # Instead we return a dense array\n",
        "    def __init__(self):\n",
        "        super(CustomCountVectorizer, self).__init__()\n",
        "    \n",
        "    def fit_transform(self, X, y=None):\n",
        "        return super(CustomCountVectorizer, self).fit_transform(X.squeeze(), y).toarray()\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return super(CustomCountVectorizer, self).fit(X, y)\n",
        "\n",
        "    def transform(self, X):\n",
        "        return super(CustomCountVectorizer, self).transform(X.squeeze()).toarray()\n",
        "\n",
        "class CustomTfidVectorizer(TfidfVectorizer):\n",
        "    # The only difference here is that we don't return a sparse array\n",
        "    # So that this can work with a transformer\n",
        "    # Instead we return a dense array\n",
        "    def __init__(self):\n",
        "        super(CustomTfidVectorizer, self).__init__()\n",
        "    \n",
        "    def fit_transform(self, X, y=None):\n",
        "        return super(CustomTfidVectorizer, self).fit_transform(X.squeeze(), y).toarray()\n",
        "    \n",
        "    def fit(self, X, y=None):\n",
        "        return super(CustomTfidVectorizer, self).fit(X, y)\n",
        "\n",
        "    def transform(self, X):\n",
        "        return super(CustomTfidVectorizer, self).transform(X.squeeze()).toarray()\n",
        "\n",
        "def run_classification(                                                     \\\n",
        "                       dataset:pd.DataFrame,                                \\\n",
        "                       target_column:str,                                   \\\n",
        "                       clf_gen_fn,                                          \\\n",
        "                       use_smote=False)->tuple:\n",
        "\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "\n",
        "    X = dataset[get_feature_column_names(dataset)]\n",
        "    y = dataset[target_column]\n",
        "    trainX, testX, trainY, testY = train_test_split(X, y, random_state=0)\n",
        "\n",
        "    text_columns = ['cleaned_comment_text']\n",
        "    numeric_columns = ['n_all_caps', 'perc_exclamations', 'num_exclamations']\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "                        remainder='drop',\n",
        "                        transformers =                                      \\\n",
        "                                    [                                       \\\n",
        "                                        ('text', CustomCountVectorizer(), text_columns), \\\n",
        "                                        ('text2', CustomTfidVectorizer(), text_columns), \\\n",
        "                                        ('num', StandardScaler(with_mean=False, with_std=False), numeric_columns)\n",
        "                                    ])\n",
        "    classif_pipeline = Pipeline([('prep', preprocessor), ('classif', clf_gen_fn())])\n",
        "\n",
        "    classif_pipeline.fit(trainX, trainY)\n",
        "    y_pred = classif_pipeline.predict(testX)\n",
        "\n",
        "    return accuracy_score(testY, y_pred), f1_score(testY, y_pred), classif_pipeline\n",
        "\n",
        "\n",
        "\n",
        "def run_classifiers():\n",
        "\n",
        "    classifiers = {\n",
        "        \"LinearSVC_nosmote\": (False, lambda: LinearSVC(),),\n",
        "        \"MultinomialNB_nosmote\": (False, lambda: MultinomialNB(),),\n",
        "        \"RandomForestClassifier\": (True, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "    }\n",
        "\n",
        "    result_df = pd.DataFrame(                                                           \\\n",
        "                    {                                                       \\\n",
        "                        'classifier': pd.Series('str'),                     \\\n",
        "                        'task_name': pd.Series('str'),                      \\\n",
        "                        'metric': pd.Series('str'),                         \\\n",
        "                        'smote': pd.Series(int),                            \\\n",
        "                        'value': pd.Series(float),                          \\\n",
        "                    })\n",
        "\n",
        "    model = None\n",
        "    for clfname, value in classifiers.items():\n",
        "        use_smote, clfgen = value\n",
        "        for colname in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "            accuracy, f1, model = run_classification(train_df, colname, clfgen, use_smote)\n",
        "            print(f\"{clfname:20.20s} {colname:20.20s} accuracy={accuracy:1.3f}              f1={f1:1.3f}   smote={use_smote}\")\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'accuracy',\n",
        "                'value': accuracy,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'f1_score',\n",
        "                'value': f1,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "\n",
        "        \n",
        "    return result_df, model\n",
        "\n",
        "seed_random()\n",
        "result_df, model = run_classifiers()\n",
        "\n",
        "model\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC_nosmote    Sub1_Toxic           accuracy=0.663              f1=0.476   smote=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC_nosmote    Sub2_Engaging        accuracy=0.821              f1=0.590   smote=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC_nosmote    Sub3_FactClaiming    accuracy=0.740              f1=0.574   smote=False\n",
            "MultinomialNB_nosmot Sub1_Toxic           accuracy=0.666              f1=0.303   smote=False\n",
            "MultinomialNB_nosmot Sub2_Engaging        accuracy=0.762              f1=0.541   smote=False\n",
            "MultinomialNB_nosmot Sub3_FactClaiming    accuracy=0.710              f1=0.577   smote=False\n",
            "RandomForestClassifi Sub1_Toxic           accuracy=0.662              f1=0.177   smote=True\n",
            "RandomForestClassifi Sub2_Engaging        accuracy=0.839              f1=0.583   smote=True\n",
            "RandomForestClassifi Sub3_FactClaiming    accuracy=0.757              f1=0.531   smote=True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style>#sk-a4184e36-5d69-419e-ada6-51333b088155 {color: black;background-color: white;}#sk-a4184e36-5d69-419e-ada6-51333b088155 pre{padding: 0;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-toggleable {background-color: white;}#sk-a4184e36-5d69-419e-ada6-51333b088155 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-a4184e36-5d69-419e-ada6-51333b088155 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-a4184e36-5d69-419e-ada6-51333b088155 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-estimator:hover {background-color: #d4ebff;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-item {z-index: 1;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-parallel-item:only-child::after {width: 0;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-a4184e36-5d69-419e-ada6-51333b088155 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-a4184e36-5d69-419e-ada6-51333b088155\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"11a57a2e-372d-4f71-8032-c943eadbc11a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"11a57a2e-372d-4f71-8032-c943eadbc11a\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('prep',\n",
              "                 ColumnTransformer(transformers=[('text',\n",
              "                                                  CustomCountVectorizer(),\n",
              "                                                  ['cleaned_comment_text']),\n",
              "                                                 ('text2',\n",
              "                                                  CustomTfidVectorizer(),\n",
              "                                                  ['cleaned_comment_text']),\n",
              "                                                 ('num',\n",
              "                                                  StandardScaler(with_mean=False,\n",
              "                                                                 with_std=False),\n",
              "                                                  ['n_all_caps',\n",
              "                                                   'perc_exclamations',\n",
              "                                                   'num_exclamations'])])),\n",
              "                ('classif', RandomForestClassifier(n_jobs=-1))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b08f7b78-836f-4160-9dc3-3972e2c7c7af\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"b08f7b78-836f-4160-9dc3-3972e2c7c7af\">prep: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('text', CustomCountVectorizer(),\n",
              "                                 ['cleaned_comment_text']),\n",
              "                                ('text2', CustomTfidVectorizer(),\n",
              "                                 ['cleaned_comment_text']),\n",
              "                                ('num',\n",
              "                                 StandardScaler(with_mean=False,\n",
              "                                                with_std=False),\n",
              "                                 ['n_all_caps', 'perc_exclamations',\n",
              "                                  'num_exclamations'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e150d5b0-23cc-4364-9569-2ffd5a3b81f4\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e150d5b0-23cc-4364-9569-2ffd5a3b81f4\">text</label><div class=\"sk-toggleable__content\"><pre>['cleaned_comment_text']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"860d961a-4c0a-4181-b691-e026bd6e884f\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"860d961a-4c0a-4181-b691-e026bd6e884f\">CustomCountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CustomCountVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"6a0b1667-051a-4a61-b1f9-8b78a35a2f4b\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"6a0b1667-051a-4a61-b1f9-8b78a35a2f4b\">text2</label><div class=\"sk-toggleable__content\"><pre>['cleaned_comment_text']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"beeaa4a7-32bb-4c24-ac7a-2e2d9d9de474\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"beeaa4a7-32bb-4c24-ac7a-2e2d9d9de474\">CustomTfidVectorizer</label><div class=\"sk-toggleable__content\"><pre>CustomTfidVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"9c5c8d72-6c0e-45c4-9cac-abe8caa37219\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"9c5c8d72-6c0e-45c4-9cac-abe8caa37219\">num</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps', 'perc_exclamations', 'num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"9f63f538-527a-4f94-9346-bebfea72319a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"9f63f538-527a-4f94-9346-bebfea72319a\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler(with_mean=False, with_std=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"dc816bf9-e19b-4124-a37a-cd4c94bfa428\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"dc816bf9-e19b-4124-a37a-cd4c94bfa428\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=-1)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('prep',\n",
              "                 ColumnTransformer(transformers=[('text',\n",
              "                                                  CustomCountVectorizer(),\n",
              "                                                  ['cleaned_comment_text']),\n",
              "                                                 ('text2',\n",
              "                                                  CustomTfidVectorizer(),\n",
              "                                                  ['cleaned_comment_text']),\n",
              "                                                 ('num',\n",
              "                                                  StandardScaler(with_mean=False,\n",
              "                                                                 with_std=False),\n",
              "                                                  ['n_all_caps',\n",
              "                                                   'perc_exclamations',\n",
              "                                                   'num_exclamations'])])),\n",
              "                ('classif', RandomForestClassifier(n_jobs=-1))])"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EiRR4fd9sb7",
        "outputId": "2a3c435c-5e63-46a9-9b1d-60b1a2140902"
      },
      "source": [
        "def print_df(df, metric, task):\n",
        "    df = df[(df['metric'] == metric) & (df['task_name'] == task)]\n",
        "    df = df.sort_values(by=['value'], ascending=False)\n",
        "    print(df.head(3))\n",
        "    return df\n",
        "\n",
        "for task_name in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "    print('=' * 80)\n",
        "    print(task_name)\n",
        "    print('-' * len(task_name))\n",
        "    print()\n",
        "    for metric in ['accuracy', 'f1_score']:\n",
        "        print_df(result_df, metric, task_name)\n",
        "        print()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Sub1_Toxic\n",
            "----------\n",
            "\n",
            "                classifier   task_name    metric smote     value\n",
            "7    MultinomialNB_nosmote  Sub1_Toxic  accuracy     0  0.665832\n",
            "1        LinearSVC_nosmote  Sub1_Toxic  accuracy     0  0.663329\n",
            "13  RandomForestClassifier  Sub1_Toxic  accuracy     1  0.662078\n",
            "\n",
            "                classifier   task_name    metric smote     value\n",
            "2        LinearSVC_nosmote  Sub1_Toxic  f1_score     0  0.475634\n",
            "8    MultinomialNB_nosmote  Sub1_Toxic  f1_score     0  0.302872\n",
            "14  RandomForestClassifier  Sub1_Toxic  f1_score     1  0.176829\n",
            "\n",
            "================================================================================\n",
            "Sub2_Engaging\n",
            "-------------\n",
            "\n",
            "                classifier      task_name    metric smote     value\n",
            "15  RandomForestClassifier  Sub2_Engaging  accuracy     1  0.838548\n",
            "3        LinearSVC_nosmote  Sub2_Engaging  accuracy     0  0.821026\n",
            "9    MultinomialNB_nosmote  Sub2_Engaging  accuracy     0  0.762203\n",
            "\n",
            "                classifier      task_name    metric smote     value\n",
            "4        LinearSVC_nosmote  Sub2_Engaging  f1_score     0  0.590258\n",
            "16  RandomForestClassifier  Sub2_Engaging  f1_score     1  0.582524\n",
            "10   MultinomialNB_nosmote  Sub2_Engaging  f1_score     0  0.541063\n",
            "\n",
            "================================================================================\n",
            "Sub3_FactClaiming\n",
            "-----------------\n",
            "\n",
            "                classifier          task_name    metric smote     value\n",
            "17  RandomForestClassifier  Sub3_FactClaiming  accuracy     1  0.757196\n",
            "5        LinearSVC_nosmote  Sub3_FactClaiming  accuracy     0  0.739675\n",
            "11   MultinomialNB_nosmote  Sub3_FactClaiming  accuracy     0  0.709637\n",
            "\n",
            "                classifier          task_name    metric smote     value\n",
            "12   MultinomialNB_nosmote  Sub3_FactClaiming  f1_score     0  0.576642\n",
            "6        LinearSVC_nosmote  Sub3_FactClaiming  f1_score     0   0.57377\n",
            "18  RandomForestClassifier  Sub3_FactClaiming  f1_score     1  0.531401\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mvNlGAA-eN9"
      },
      "source": [
        "#result_df\n",
        "result_df[result_df['metric'] == 'f1_score'].sort_values(by='value', ascending=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}