{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMeQ2PNKYxiZoxvQj9sVDNN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment1/NLP_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERvBOEn0wss6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d37ccb36-9c4c-4d69-98c6-4939f073bc9a"
      },
      "source": [
        "!pip install spacy  nltk spacymoji huggingface -q       >/dev/null 2>&1         \n",
        "!pip install -q -U tensorflow-text                      >/dev/null 2>&1\n",
        "!pip install -q tf-models-official                      >/dev/null 2>&1\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "!pip install transformers                               >/dev/null 2>&1\n",
        "\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "\n",
        "!pip install mlxtend                                    >/dev/null 2>&1\n",
        "!pip install imblearn                                   >/dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (from imblearn) (0.4.3)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->imbalanced-learn->imblearn) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-DAX9VLZlw3"
      },
      "source": [
        "import requests\n",
        "def get_train_test_files():\n",
        "    TRAIN_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Test_For_Evaluation.csv'\n",
        "    TRAIN_FILE_LOCAL = 'Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE_LOCAL = 'Assessment1_Toxic_Test.csv'\n",
        "\n",
        "    def download(url, localfile):\n",
        "        with open(localfile, 'wb') as f:\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            f.write(r.content)\n",
        "\n",
        "    download(TRAIN_FILE, TRAIN_FILE_LOCAL)\n",
        "    download(TEST_FILE, TEST_FILE_LOCAL)\n",
        "\n",
        "    return TRAIN_FILE_LOCAL, TEST_FILE_LOCAL\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrlJ1H1maDxM"
      },
      "source": [
        "# Functions to read the CSV and do basic cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVfYulLnuGVJ"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "def remove_roles(line:str)->str:\n",
        "    # Remove texts like @USER, @MODERATOR etc\n",
        "    pat = re.compile(u'\\@[A-Za-z]+')\n",
        "    return re.sub(pat, '', line)\n",
        "\n",
        "def get_train_test_df():\n",
        "    train_csv, test_csv = get_train_test_files()\n",
        "\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "def remove_emojis(line:str)->str:\n",
        "    pat = re.compile(\"[\"\n",
        "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                u\"\\U00002702-\\U000027B0\"\n",
        "                u\"\\U000024C2-\\U0001F251\"\n",
        "                u\"\\U0001f926-\\U0001f937\"\n",
        "                u'\\U00010000-\\U0010ffff'\n",
        "                u\"\\u200d\"\n",
        "                u\"\\u2640-\\u2642\"\n",
        "                u\"\\u2600-\\u2B55\"\n",
        "                u\"\\u23cf\"\n",
        "                u\"\\u23e9\"\n",
        "                u\"\\u231a\"\n",
        "                u\"\\u3030\"\n",
        "                u\"\\ufe0f\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return re.sub(pat, '', line)\n",
        "\n",
        "def remove_ellipses(line:str)->str:\n",
        "    pat = re.compile(u'\\.\\.+')\n",
        "    return re.sub(pat, ' ', line)\n",
        "\n",
        "def to_lower(line:str)->str:\n",
        "    return line.lower()\n",
        "\n",
        "def replace_number_with_tag(line:str)->str:\n",
        "    line = re.sub(\"\\s\\d*((\\.|\\,)\\d+)?\\s\", \" nummer \", line)\n",
        "    line = re.sub('\\s\\d+$', '', line)\n",
        "    line = re.sub('^\\d+\\s', '', line)\n",
        "    return line\n",
        "\n",
        "def remove_urls(line:str)->str:\n",
        "    return re.sub('https?:\\/\\/\\S+', ' hyperlink ', line)\n",
        "\n",
        "def basic_clean(s:pd.Series)->pd.Series:\n",
        "    return s.map(to_lower)                                                  \\\n",
        "            .map(remove_emojis)                                             \\\n",
        "            .map(remove_roles)                                              \\\n",
        "            .map(remove_ellipses)                                           \\\n",
        "            .map(replace_number_with_tag)                                   \\\n",
        "            .map(remove_urls)\n",
        "\n",
        "def get_clean_train_test_df()->tuple:\n",
        "    train_df, test_df = get_train_test_df()\n",
        "    train_df['comment_text'] = basic_clean(train_df['comment_text'])\n",
        "    test_df['comment_text'] = basic_clean(test_df['comment_text'])\n",
        "    return train_df, test_df\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6_dto2JaR2f"
      },
      "source": [
        "# Clean using Spacy and Enrich"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8YSZZ4vxTk5"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string\n",
        "import spacy\n",
        "from spacymoji import Emoji\n",
        "import  de_core_news_sm\n",
        "\n",
        "def is_punct_only(token:str)->bool:\n",
        "    for c in list(token):\n",
        "        if c not in string.punctuation:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_same(l1:list, l2:list)->bool:\n",
        "    if (len(l1) != len(l2)):\n",
        "        return False\n",
        "    for x, y in zip(l1, l2):\n",
        "        if x != y:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def get_num_of_allcap_words(s:str)->int:\n",
        "    def is_allcaps(s:str)->bool:\n",
        "        if (len(s) < 3):\n",
        "            return False\n",
        "        for c in list(s):\n",
        "            if not (\\\n",
        "                    (ord(c) <=ord('Z') and ord(c) >= ord('A')) or           \\\n",
        "                    (ord(c) >= ord('0') and ord(c) <= ord('9'))             \\\n",
        "                    ):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    if len(s) < 3:\n",
        "        return 0\n",
        "    tokens = [w.strip() for w in s.split()]\n",
        "    return sum([1 for t in tokens if is_allcaps(t)])\n",
        "\n",
        "def get_percentage_of_excalamations(s:str)->float:\n",
        "    if len(s) == 0:\n",
        "        return 0.0\n",
        "    exclamation_count = sum([1 for c in list(s) if c == '!'])\n",
        "    return exclamation_count / len(s)\n",
        "\n",
        "\n",
        "def is_empty_string(s:str)->bool:\n",
        "    if s == '' or s == None:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def do_basic_nlp_cleaning(line:str)->str:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(line)\n",
        "\n",
        "    # Some tokens start with a punctuation, remove the first one\n",
        "    def remove_first_punctuation(tok:str)->str:\n",
        "        return                                                              \\\n",
        "            tok[1:]                                                         \\\n",
        "            if tok[0] in set(string.punctuation) and len(tok) != 0          \\\n",
        "            else tok\n",
        "\n",
        "    tokens = [remove_first_punctuation(w) for w in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"german\"))\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [w for w in tokens if not is_punct_only(w)]\n",
        "\n",
        "    # Stem words\n",
        "    stem = SnowballStemmer('german')\n",
        "    tokens = [stem.stem(w) for w in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def get_cleaning_function():\n",
        "    #nlp = spacy.load(\"de_dep_news_trf\")\n",
        "    #nlp = spacy.load(\"de_core_news_sm\")\n",
        "    nlp = de_core_news_sm.load()\n",
        "    emoji = Emoji(nlp)\n",
        "    nlp.add_pipe(emoji, first=True)\n",
        "    stopwords = spacy.lang.de.stop_words.STOP_WORDS\n",
        "\n",
        "    def do_basic_nlp_cleaning(line:str)->str:\n",
        "        def is_interesting_token(token):\n",
        "            if token.pos_ in set(['NUM', 'SYM']):\n",
        "                return False\n",
        "            if token.text in stopwords:\n",
        "                return False\n",
        "            if (token.is_punct):\n",
        "                return False\n",
        "            #if token._.is_emoji:\n",
        "            #    return False\n",
        "            return True\n",
        "\n",
        "        def remove_terminal_punctuations(word):\n",
        "            word = word.strip()\n",
        "            while word != \"\" and word[0] in list(string.punctuation):\n",
        "                word = word[1:]\n",
        "            while word != \"\" and word[-1] in list(string.punctuation):\n",
        "                word = word[:-1]\n",
        "            return word\n",
        "\n",
        "        doc = nlp(line)\n",
        "        words = [tok.lemma_.lower() for tok in doc if is_interesting_token(tok)]\n",
        "        words = [remove_terminal_punctuations(word) for word in words]\n",
        "        words = [word for word in words if word != \"\"]\n",
        "        return  \" \".join(words)\n",
        "\n",
        "    return do_basic_nlp_cleaning\n",
        "\n",
        "def get_enriched_dataset(df):\n",
        "    df['cleaned_comment_text'] = df['comment_text'].map(cleaning_fn)\n",
        "    df['n_all_caps'] = df['comment_text'].map(get_num_of_allcap_words)\n",
        "    df['perc_exclamations'] = df['comment_text'].map(get_percentage_of_excalamations)\n",
        "    df['num_exclamations'] = df['comment_text'].map(lambda s: sum([1 for x in list(s) if x == '!']))\n",
        "    return df\n",
        "\n",
        "cleaning_fn = get_cleaning_function()\n",
        "train_df, test_df = get_clean_train_test_df()\n",
        "train_df = get_enriched_dataset(train_df)\n",
        "test_df = get_enriched_dataset(test_df)\n",
        "empty_rows = train_df['cleaned_comment_text'].map(is_empty_string)\n",
        "train_df = train_df[~ empty_rows]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWTtxHw0aa4N"
      },
      "source": [
        "# Print Enriched Training DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "n3q-TKbwZRBJ",
        "outputId": "7b9f8469-63ff-4276-c1ef-761c07b0a784"
      },
      "source": [
        "train_df"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>Sub1_Toxic</th>\n",
              "      <th>Sub2_Engaging</th>\n",
              "      <th>Sub3_FactClaiming</th>\n",
              "      <th>cleaned_comment_text</th>\n",
              "      <th>n_all_caps</th>\n",
              "      <th>perc_exclamations</th>\n",
              "      <th>num_exclamations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gestern bei illner, montag bei nummer  ist das...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>gestern illner montag nummer langsam ör-partei...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mein gott der war erst gestern bei illner. die...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>gott gestern illner redaktionen versagen</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>die cdu lässt das so wie so nicht zu . sagen ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>cdu lässt sagen reich bekommen nummer milliard...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bei meiner beschissenen rente als 2x geschiede...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>beschissen rente geschieden mann steuern krank...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wer nummer jahre zum mindestlohn arbeiten muß,...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>nummer mindestlohn arbeiten erhalten € rente n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00495</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3189</th>\n",
              "      <td>hier mal eine info. flüchtlinge werden nummer ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>mal info flüchtlinge nummer km küste schlauchb...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3190</th>\n",
              "      <td>.aha .mal abwarten kommt bei uns auch .firmen ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>aha mal abwarten firmen entlassen mitarbeiter ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3191</th>\n",
              "      <td>.so ist es</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>so</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3192</th>\n",
              "      <td>.die warten da</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>die warten</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3193</th>\n",
              "      <td>.das bekommen die gesagt wie sich verhalten s...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>das bekommen verhalten kameras richten tut ers...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3131 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           comment_text  ...  num_exclamations\n",
              "0     gestern bei illner, montag bei nummer  ist das...  ...                 0\n",
              "1     mein gott der war erst gestern bei illner. die...  ...                 0\n",
              "2      die cdu lässt das so wie so nicht zu . sagen ...  ...                 0\n",
              "3     bei meiner beschissenen rente als 2x geschiede...  ...                 0\n",
              "4     wer nummer jahre zum mindestlohn arbeiten muß,...  ...                 3\n",
              "...                                                 ...  ...               ...\n",
              "3189  hier mal eine info. flüchtlinge werden nummer ...  ...                 0\n",
              "3190  .aha .mal abwarten kommt bei uns auch .firmen ...  ...                 0\n",
              "3191                                         .so ist es  ...                 0\n",
              "3192                                     .die warten da  ...                 0\n",
              "3193   .das bekommen die gesagt wie sich verhalten s...  ...                 0\n",
              "\n",
              "[3131 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzgub_1VakyE"
      },
      "source": [
        "# Multinomial NB pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV_S-WOU3sQp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXqnV32AaoCL",
        "outputId": "6a1c1176-db95-41f6-a6ff-d98088e4e0f6"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.pipeline import Pipeline\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE \n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.preprocessing import DenseTransformer\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "def get_feature_column_names(df):\n",
        "    return [cname for cname in df.columns if not cname.startswith('Sub')]\n",
        "\n",
        "def get_target_column_names(df):\n",
        "    return [cname for cname in df.columns if cname.startswith('Sub')]\n",
        "\n",
        "def is_text_column(colname:str)->bool:\n",
        "    if 'text' in colname:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_text_columns(df)->list:\n",
        "    return [cn for cn in df.columns if is_text_column(cn)]\n",
        "\n",
        "def get_nontext_columns(df)->list:\n",
        "    return [cn for cn in df.columns if not is_text_column(cn)]\n",
        "\n",
        "def run_classification(                                                     \\\n",
        "                       dataset:pd.DataFrame,                                \\\n",
        "                       target_column:str,                                   \\\n",
        "                       clf_gen_fn,                                          \\\n",
        "                       use_smote=False)->tuple:\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'n_all_caps']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'num_exclamations']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'perc_exclamations']]\n",
        "    X = dataset[get_feature_column_names(dataset)]\n",
        "    y = dataset[target_column]\n",
        "    trainX, testX, trainY, testY = train_test_split(X, y, random_state=0)\n",
        "\n",
        "    def get_text_pipeline():\n",
        "        return Pipeline(                                                    \\\n",
        "                        [                                                   \\\n",
        "                            ('cv', CountVectorizer(),),                     \\\n",
        "                            ('tfid', TfidfVectorizer()),                    \\\n",
        "                        ])\n",
        "\n",
        "                        \n",
        "    column_trans = make_column_transformer(                                 \\\n",
        "                            (CountVectorizer(ngram_range=(1,1)), 'cleaned_comment_text'),   \\\n",
        "                            (TfidfVectorizer(), 'cleaned_comment_text'),    \\\n",
        "                            remainder=MinMaxScaler(),                       \\\n",
        "                        )\n",
        "\n",
        "    if use_smote:\n",
        "        classif_pipeline = Pipeline(                                        \\\n",
        "                                [                                           \\\n",
        "                                    ('column_transformer', column_trans),   \\\n",
        "                                    ('dense', DenseTransformer()),          \\\n",
        "                                    ('smote', SMOTE(n_jobs=-1)),            \\\n",
        "                                    ('clf', clf_gen_fn()),                  \\\n",
        "                                ])\n",
        "    else:\n",
        "        classif_pipeline = Pipeline(                                        \\\n",
        "                                [                                           \\\n",
        "                                    ('column_transformer', column_trans),   \\\n",
        "                                    ('dense', DenseTransformer()),          \\\n",
        "                                    ('clf', clf_gen_fn()),                  \\\n",
        "                                ])\n",
        "    \n",
        "    \n",
        "    classif_pipeline.fit(trainX, trainY)\n",
        "    y_pred = classif_pipeline.predict(testX)\n",
        "\n",
        "    return accuracy_score(testY, y_pred), f1_score(testY, y_pred)\n",
        "\n",
        "def run_classifiers():\n",
        "\n",
        "    classifiers = {\n",
        "        \"LinearSVC_nosmote\": (False, lambda: LinearSVC(),),\n",
        "        \"LinearSVC\": (True, lambda: LinearSVC(),),\n",
        "        \"AdaBoost\": (True, lambda: AdaBoostClassifier(),),\n",
        "        \"AdaBoost_nosmote\": (False, lambda: AdaBoostClassifier(),),\n",
        "        \"MultinomialNB_nosmote\": (False, lambda: MultinomialNB(),),\n",
        "        \"RandomForestClassifier\": (True, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "        \"RandomForestClassifier_nosmote\": (False, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "        \"BernoulliNB_nosmote\": (False, lambda: BernoulliNB(),),\n",
        "    }\n",
        "\n",
        "    result_df = pd.DataFrame(                                                           \\\n",
        "                    {                                                       \\\n",
        "                        'classifier': pd.Series('str'),                     \\\n",
        "                        'task_name': pd.Series('str'),                      \\\n",
        "                        'metric': pd.Series('str'),                         \\\n",
        "                        'smote': pd.Series(int),                            \\\n",
        "                        'value': pd.Series(float),                          \\\n",
        "                    })\n",
        "\n",
        "    for clfname, value in classifiers.items():\n",
        "        use_smote, clfgen = value\n",
        "        for colname in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "            accuracy, f1 = run_classification(train_df, colname, clfgen, use_smote)\n",
        "            print(f\"{clfname:20.20s} {colname:20.20s} accuracy={accuracy:1.3f}              f1={f1:1.3f}   smote={use_smote}\")\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'accuracy',\n",
        "                'value': accuracy,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'f1_score',\n",
        "                'value': f1,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "\n",
        "        \n",
        "    return result_df\n",
        "\n",
        "result_df = run_classifiers()\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC_nosmote    Sub1_Toxic           accuracy=0.655              f1=0.428   smote=False\n",
            "LinearSVC_nosmote    Sub2_Engaging        accuracy=0.807              f1=0.552   smote=False\n",
            "LinearSVC_nosmote    Sub3_FactClaiming    accuracy=0.754              f1=0.583   smote=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC            Sub1_Toxic           accuracy=0.645              f1=0.442   smote=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC            Sub2_Engaging        accuracy=0.801              f1=0.544   smote=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC            Sub3_FactClaiming    accuracy=0.751              f1=0.582   smote=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost             Sub1_Toxic           accuracy=0.665              f1=0.382   smote=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost             Sub2_Engaging        accuracy=0.784              f1=0.483   smote=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost             Sub3_FactClaiming    accuracy=0.739              f1=0.521   smote=True\n",
            "AdaBoost_nosmote     Sub1_Toxic           accuracy=0.665              f1=0.355   smote=False\n",
            "AdaBoost_nosmote     Sub2_Engaging        accuracy=0.778              f1=0.428   smote=False\n",
            "AdaBoost_nosmote     Sub3_FactClaiming    accuracy=0.739              f1=0.514   smote=False\n",
            "MultinomialNB_nosmot Sub1_Toxic           accuracy=0.662              f1=0.430   smote=False\n",
            "MultinomialNB_nosmot Sub2_Engaging        accuracy=0.750              f1=0.503   smote=False\n",
            "MultinomialNB_nosmot Sub3_FactClaiming    accuracy=0.724              f1=0.589   smote=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifi Sub1_Toxic           accuracy=0.702              f1=0.395   smote=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifi Sub2_Engaging        accuracy=0.817              f1=0.464   smote=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifi Sub3_FactClaiming    accuracy=0.765              f1=0.526   smote=True\n",
            "RandomForestClassifi Sub1_Toxic           accuracy=0.691              f1=0.328   smote=False\n",
            "RandomForestClassifi Sub2_Engaging        accuracy=0.797              f1=0.351   smote=False\n",
            "RandomForestClassifi Sub3_FactClaiming    accuracy=0.750              f1=0.479   smote=False\n",
            "BernoulliNB_nosmote  Sub1_Toxic           accuracy=0.678              f1=0.192   smote=False\n",
            "BernoulliNB_nosmote  Sub2_Engaging        accuracy=0.785              f1=0.359   smote=False\n",
            "BernoulliNB_nosmote  Sub3_FactClaiming    accuracy=0.733              f1=0.394   smote=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EiRR4fd9sb7",
        "outputId": "c020262a-1406-4a3d-8bdb-6cbe07e5fb16"
      },
      "source": [
        "def print_df(df, metric, task):\n",
        "    df = df[(df['metric'] == metric) & (df['task_name'] == task)]\n",
        "    df = df.sort_values(by=['value'], ascending=False)\n",
        "    print(df.head(3))\n",
        "    return df\n",
        "\n",
        "for task_name in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "    print('=' * 80)\n",
        "    print(task_name)\n",
        "    print('-' * len(task_name))\n",
        "    print()\n",
        "    for metric in ['accuracy', 'f1_score']:\n",
        "        print_df(result_df, metric, task_name)\n",
        "        print()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Sub1_Toxic\n",
            "----------\n",
            "\n",
            "                        classifier   task_name    metric smote     value\n",
            "31          RandomForestClassifier  Sub1_Toxic  accuracy     1  0.702427\n",
            "37  RandomForestClassifier_nosmote  Sub1_Toxic  accuracy     0  0.690932\n",
            "43             BernoulliNB_nosmote  Sub1_Toxic  accuracy     0  0.678161\n",
            "\n",
            "               classifier   task_name    metric smote     value\n",
            "8               LinearSVC  Sub1_Toxic  f1_score     1  0.441767\n",
            "26  MultinomialNB_nosmote  Sub1_Toxic  f1_score     0  0.430108\n",
            "2       LinearSVC_nosmote  Sub1_Toxic  f1_score     0  0.427966\n",
            "\n",
            "================================================================================\n",
            "Sub2_Engaging\n",
            "-------------\n",
            "\n",
            "                classifier      task_name    metric smote     value\n",
            "33  RandomForestClassifier  Sub2_Engaging  accuracy     1  0.817369\n",
            "3        LinearSVC_nosmote  Sub2_Engaging  accuracy     0  0.807152\n",
            "9                LinearSVC  Sub2_Engaging  accuracy     1  0.800766\n",
            "\n",
            "               classifier      task_name    metric smote     value\n",
            "4       LinearSVC_nosmote  Sub2_Engaging  f1_score     0  0.551929\n",
            "10              LinearSVC  Sub2_Engaging  f1_score     1   0.54386\n",
            "28  MultinomialNB_nosmote  Sub2_Engaging  f1_score     0  0.502538\n",
            "\n",
            "================================================================================\n",
            "Sub3_FactClaiming\n",
            "-----------------\n",
            "\n",
            "                classifier          task_name    metric smote     value\n",
            "35  RandomForestClassifier  Sub3_FactClaiming  accuracy     1  0.765006\n",
            "5        LinearSVC_nosmote  Sub3_FactClaiming  accuracy     0  0.753512\n",
            "11               LinearSVC  Sub3_FactClaiming  accuracy     1  0.750958\n",
            "\n",
            "               classifier          task_name    metric smote     value\n",
            "30  MultinomialNB_nosmote  Sub3_FactClaiming  f1_score     0  0.589354\n",
            "6       LinearSVC_nosmote  Sub3_FactClaiming  f1_score     0  0.583153\n",
            "12              LinearSVC  Sub3_FactClaiming  f1_score     1  0.582441\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "id": "5mvNlGAA-eN9",
        "outputId": "e20aec4a-aa54-4c95-b1c2-201a76048db5"
      },
      "source": [
        "#result_df\n",
        "result_df[result_df['metric'] == 'f1_score'].sort_values(by='value', ascending=False)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classifier</th>\n",
              "      <th>task_name</th>\n",
              "      <th>metric</th>\n",
              "      <th>smote</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>MultinomialNB_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.589354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LinearSVC_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.583153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.582441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LinearSVC_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.551929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.54386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.525773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.521127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AdaBoost_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.514286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>MultinomialNB_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.502538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.48318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>RandomForestClassifier_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.478723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.464419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.441767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>MultinomialNB_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.430108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LinearSVC_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.427966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>AdaBoost_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.427632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.394805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>BernoulliNB_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.394203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.382075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>BernoulliNB_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.358779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>AdaBoost_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.35468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>RandomForestClassifier_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.35102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>RandomForestClassifier_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.327778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>BernoulliNB_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.192308</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        classifier          task_name    metric smote     value\n",
              "30           MultinomialNB_nosmote  Sub3_FactClaiming  f1_score     0  0.589354\n",
              "6                LinearSVC_nosmote  Sub3_FactClaiming  f1_score     0  0.583153\n",
              "12                       LinearSVC  Sub3_FactClaiming  f1_score     1  0.582441\n",
              "4                LinearSVC_nosmote      Sub2_Engaging  f1_score     0  0.551929\n",
              "10                       LinearSVC      Sub2_Engaging  f1_score     1   0.54386\n",
              "36          RandomForestClassifier  Sub3_FactClaiming  f1_score     1  0.525773\n",
              "18                        AdaBoost  Sub3_FactClaiming  f1_score     1  0.521127\n",
              "24                AdaBoost_nosmote  Sub3_FactClaiming  f1_score     0  0.514286\n",
              "28           MultinomialNB_nosmote      Sub2_Engaging  f1_score     0  0.502538\n",
              "16                        AdaBoost      Sub2_Engaging  f1_score     1   0.48318\n",
              "42  RandomForestClassifier_nosmote  Sub3_FactClaiming  f1_score     0  0.478723\n",
              "34          RandomForestClassifier      Sub2_Engaging  f1_score     1  0.464419\n",
              "8                        LinearSVC         Sub1_Toxic  f1_score     1  0.441767\n",
              "26           MultinomialNB_nosmote         Sub1_Toxic  f1_score     0  0.430108\n",
              "2                LinearSVC_nosmote         Sub1_Toxic  f1_score     0  0.427966\n",
              "22                AdaBoost_nosmote      Sub2_Engaging  f1_score     0  0.427632\n",
              "32          RandomForestClassifier         Sub1_Toxic  f1_score     1  0.394805\n",
              "48             BernoulliNB_nosmote  Sub3_FactClaiming  f1_score     0  0.394203\n",
              "14                        AdaBoost         Sub1_Toxic  f1_score     1  0.382075\n",
              "46             BernoulliNB_nosmote      Sub2_Engaging  f1_score     0  0.358779\n",
              "20                AdaBoost_nosmote         Sub1_Toxic  f1_score     0   0.35468\n",
              "40  RandomForestClassifier_nosmote      Sub2_Engaging  f1_score     0   0.35102\n",
              "38  RandomForestClassifier_nosmote         Sub1_Toxic  f1_score     0  0.327778\n",
              "44             BernoulliNB_nosmote         Sub1_Toxic  f1_score     0  0.192308"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbcLIE7oC2ML"
      },
      "source": [
        "## BERT MODEL\n",
        "\n",
        "Use code from https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9TbWSQ0c7C_"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
        "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
        "\n",
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEzTyC_sru6w"
      },
      "source": [
        "## HUGGING FACE TRANSFORMERS : GERMAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zul78K-dKK3h",
        "outputId": "f4967889-3c28-4d08-fa98-708cd2f854c4"
      },
      "source": [
        "from transformers import AutoTokenizer,TFAutoModel\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, TFAutoModelForMaskedLM\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import IPython\n",
        "\n",
        "SEQUENCE_LENGTH = 512\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
        "\n",
        "def tokenize_input_with_bert(bert_tokenizer, input:str):\n",
        "    tokens = bert_tokenizer.encode_plus(\n",
        "                    input,\n",
        "                    max_length=SEQUENCE_LENGTH,\n",
        "                    truncation=True,\n",
        "                    padding='max_length',\n",
        "                    add_special_tokens=True,\n",
        "                    return_attention_mask=True,\n",
        "                    return_token_type_ids=False,\n",
        "                    return_tensors='np'\n",
        "                )\n",
        "    return tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "def tokenize_input_with_bert(bert_tokenizer, input:str, seq_len:int):\n",
        "    def pad(t):\n",
        "        try:\n",
        "            t = t.reshape((t.shape[1],))\n",
        "        except:\n",
        "            pass\n",
        "        t = t[:seq_len]\n",
        "        pad_len = max(0, seq_len - t.shape[0])\n",
        "        t = np.pad(t, (0, pad_len))\n",
        "        t = t.reshape((1, t.shape[0]))\n",
        "        return t\n",
        "\n",
        "    tokens = bert_tokenizer(\n",
        "                    input,\n",
        "                    truncation=True,\n",
        "                    add_special_tokens=True,\n",
        "                    return_attention_mask=True,\n",
        "                    return_token_type_ids=False,\n",
        "                    return_tensors='np')\n",
        "\n",
        "    return pad(tokens['input_ids']), pad(tokens['attention_mask'])\n",
        "\n",
        "def tokenize_dataframe_of_text(df, seq_len:int):\n",
        "    x_ids = np.zeros((0, seq_len))\n",
        "    x_mask = np.zeros((0, seq_len))\n",
        "\n",
        "    for i, text in enumerate(df):\n",
        "        id, mask = tokenize_input_with_bert(bert_tokenizer, text, seq_len)\n",
        "        x_ids = np.append(x_ids, id, axis=0)\n",
        "        x_mask = np.append(x_mask, mask, axis=0)\n",
        "\n",
        "    return x_ids, x_mask\n",
        "\n",
        "def build_model(seq_len):\n",
        "    \n",
        "    input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
        "    mask = tf.keras.layers.Input(shape=(seq_len,), name='attn_mask', dtype='int32')\n",
        "\n",
        "    bert_model = TFAutoModelForMaskedLM.from_pretrained(\"bert-base-german-cased\")\n",
        "\n",
        "    X = bert_model(input_ids, attention_mask=mask)\n",
        "    X = X[0]\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
        "    X = tf.keras.layers.Dropout(0.1)(X)\n",
        "    X = tf.keras.layers.Flatten()(X)\n",
        "    y = tf.keras.layers.Dense(1, activation='softmax', name='outputs')(X)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "\n",
        "    bert_model.trainable = False\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def fit_model(model, X, y):\n",
        "    X_ids, X_masks = tokenize_dataframe_of_text(X, SEQUENCE_LENGTH)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X_ids, X_masks, y))\n",
        "\n",
        "    def map_fn(input_ids, masks, labels):\n",
        "        return {'input_ids': input_ids, 'attn_mask': masks}, labels\n",
        "    ds = ds.map(map_fn)\n",
        "\n",
        "    ds = ds.shuffle(10000).batch(32)\n",
        "\n",
        "    train_ds = ds.take((len(ds) * 8) // 10)\n",
        "    val_ds = ds.skip((len(ds) * 8) // 10)\n",
        "\n",
        "    model = build_model(SEQUENCE_LENGTH)\n",
        "\n",
        "    history = model.fit(train_ds, validation_data=val_ds, epochs=20)\n",
        "\n",
        "    return history\n",
        "\n",
        "def create_model():\n",
        "    return build_model(SEQUENCE_LENGTH)\n",
        "\n",
        "model = create_model()\n",
        "train_df, test_df = get_clean_train_test_df()\n",
        "history = fit_model(model, train_df['comment_text'], train_df['Sub3_FactClaiming'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-german-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-german-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "80/80 [==============================] - 918s 11s/step - loss: 11.7735 - accuracy: 0.3371 - val_loss: 10.7914 - val_accuracy: 0.3549\n",
            "Epoch 2/20\n",
            "80/80 [==============================] - 916s 11s/step - loss: 16.1275 - accuracy: 0.3484 - val_loss: 9.1900 - val_accuracy: 0.3644\n",
            "Epoch 3/20\n",
            "80/80 [==============================] - 905s 11s/step - loss: 9.7318 - accuracy: 0.3391 - val_loss: 12.2273 - val_accuracy: 0.3628\n",
            "Epoch 4/20\n",
            "80/80 [==============================] - 902s 11s/step - loss: 7.8757 - accuracy: 0.3406 - val_loss: 5.4716 - val_accuracy: 0.3549\n",
            "Epoch 5/20\n",
            "80/80 [==============================] - 902s 11s/step - loss: 3.0658 - accuracy: 0.3410 - val_loss: 1.6754 - val_accuracy: 0.3580\n",
            "Epoch 6/20\n",
            "80/80 [==============================] - 901s 11s/step - loss: 2.2503 - accuracy: 0.3457 - val_loss: 0.6449 - val_accuracy: 0.3502\n",
            "Epoch 7/20\n",
            "80/80 [==============================] - 894s 11s/step - loss: 1.0822 - accuracy: 0.3379 - val_loss: 0.5585 - val_accuracy: 0.3281\n",
            "Epoch 8/20\n",
            "80/80 [==============================] - 904s 11s/step - loss: 0.9531 - accuracy: 0.3383 - val_loss: 0.6451 - val_accuracy: 0.3438\n",
            "Epoch 9/20\n",
            "80/80 [==============================] - 897s 11s/step - loss: 0.7708 - accuracy: 0.3375 - val_loss: 0.2565 - val_accuracy: 0.3265\n",
            "Epoch 10/20\n",
            "80/80 [==============================] - 901s 11s/step - loss: 0.7691 - accuracy: 0.3352 - val_loss: 0.2079 - val_accuracy: 0.3565\n",
            "Epoch 11/20\n",
            "80/80 [==============================] - 902s 11s/step - loss: 0.7401 - accuracy: 0.3434 - val_loss: 0.2741 - val_accuracy: 0.3407\n",
            "Epoch 12/20\n",
            "80/80 [==============================] - 899s 11s/step - loss: 0.6107 - accuracy: 0.3449 - val_loss: 0.2575 - val_accuracy: 0.3580\n",
            "Epoch 13/20\n",
            "80/80 [==============================] - 895s 11s/step - loss: 0.5235 - accuracy: 0.3355 - val_loss: 0.2401 - val_accuracy: 0.3281\n",
            "Epoch 14/20\n",
            "80/80 [==============================] - 890s 11s/step - loss: 0.7208 - accuracy: 0.3371 - val_loss: 0.2019 - val_accuracy: 0.3344\n",
            "Epoch 15/20\n",
            "80/80 [==============================] - 898s 11s/step - loss: 0.5936 - accuracy: 0.3406 - val_loss: 0.2444 - val_accuracy: 0.3596\n",
            "Epoch 16/20\n",
            "80/80 [==============================] - 891s 11s/step - loss: 0.7424 - accuracy: 0.3398 - val_loss: 0.3053 - val_accuracy: 0.3628\n",
            "Epoch 17/20\n",
            "80/80 [==============================] - 892s 11s/step - loss: 0.8034 - accuracy: 0.3387 - val_loss: 0.2235 - val_accuracy: 0.3423\n",
            "Epoch 18/20\n",
            "80/80 [==============================] - 907s 11s/step - loss: 0.6434 - accuracy: 0.3387 - val_loss: 0.2895 - val_accuracy: 0.3628\n",
            "Epoch 19/20\n",
            "12/80 [===>..........................] - ETA: 10:34 - loss: 0.6248 - accuracy: 0.3646"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS8NgaRs-k8O"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(history.history)\n",
        "plt.plot(history.history['loss'], label='loss', color='b')\n",
        "plt.plot(history.history['val_loss'], label='val_loss', color='r')\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='acc', color='g')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc', color='cyan')\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_OUi7BXtbzo"
      },
      "source": [
        "import IPython\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjV2E3i_0f3i"
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGhWSvlj2Ywc"
      },
      "source": [
        "def build_model(seq_len):\n",
        "    \n",
        "    input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
        "    mask = tf.keras.layers.Input(shape=(seq_len,), name='attn_mask', dtype='int32')\n",
        "\n",
        "    bert_model = TFAutoModelForMaskedLM.from_pretrained(\"bert-base-german-cased\")\n",
        "\n",
        "    X = bert_model(input_ids, attention_mask=mask)\n",
        "    X = X[0]\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "\n",
        "\n",
        "    X = tf.keras.layers.Conv1D(64, (4,), padding='same',  activation='relu')(X)\n",
        "    X = tf.keras.layers.MaxPooling1D(2,)(X)\n",
        "    \n",
        "    X = tf.keras.layers.Conv1D(128, (4,), padding='same',  activation='relu')(X)\n",
        "    X = tf.keras.layers.MaxPooling1D(2,)(X)\n",
        "\n",
        "    X = tf.keras.layers.Conv1D(256, (4,), padding='same',  activation='relu')(X)\n",
        "    X = tf.keras.layers.MaxPooling1D(2,)(X)\n",
        "\n",
        "    X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
        "    X = tf.keras.layers.Dropout(0.1)(X)\n",
        "    X = tf.keras.layers.Flatten()(X)\n",
        "    X = tf.keras.layers.Dense(512, activation='relu')(X)\n",
        "    X = tf.keras.layers.Dense(64, activation='relu')(X)\n",
        "    X = tf.keras.layers.Dense(8, activation='relu')(X)\n",
        "    y = tf.keras.layers.Dense(1, activation='softmax', name='outputs')(X)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "\n",
        "    bert_model.trainable = False\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    return build_model(SEQUENCE_LENGTH)\n",
        "\n",
        "\n",
        "m1 = create_model()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd4lPVS60r2-"
      },
      "source": [
        "\n",
        "tf.keras.utils.plot_model(m1, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlYEb8mO4Zrv"
      },
      "source": [
        "print(m1.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbANX9ah4yGv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}