{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNtBtU0qNWRDkEg2pSS/nPm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment1/NLP_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-DAX9VLZlw3"
      },
      "source": [
        "import requests\n",
        "def get_train_test_files():\n",
        "    TRAIN_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Test_For_Evaluation.csv'\n",
        "    TRAIN_FILE_LOCAL = 'Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE_LOCAL = 'Assessment1_Toxic_Test.csv'\n",
        "\n",
        "    def download(url, localfile):\n",
        "        with open(localfile, 'wb') as f:\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            f.write(r.content)\n",
        "\n",
        "    download(TRAIN_FILE, TRAIN_FILE_LOCAL)\n",
        "    download(TEST_FILE, TEST_FILE_LOCAL)\n",
        "\n",
        "    return TRAIN_FILE_LOCAL, TEST_FILE_LOCAL\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERvBOEn0wss6"
      },
      "source": [
        "!pip install spacy  nltk spacymoji huggingface -q       >/dev/null 2>&1         \n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "!pip install -q -U tensorflow-text                      >/dev/null 2>&1\n",
        "!pip install -q tf-models-official                      >/dev/null 2>&1"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v6ohiY0s5N1"
      },
      "source": [
        "import pandas as pd\n",
        "def get_train_test_df():\n",
        "    train_csv, test_csv = get_train_test_files()\n",
        "\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "\n",
        "    return train_df, test_df"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVfYulLnuGVJ"
      },
      "source": [
        "import re\n",
        "def remove_roles(line:str)->str:\n",
        "    # Remove texts like @USER, @MODERATOR etc\n",
        "    pat = re.compile(u'\\@[A-Za-z]+')\n",
        "    return re.sub(pat, '', line)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nU-BO8guUcM"
      },
      "source": [
        "import re\n",
        "def remove_emojis(line:str)->str:\n",
        "    pat = re.compile(\"[\"\n",
        "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                u\"\\U00002702-\\U000027B0\"\n",
        "                u\"\\U000024C2-\\U0001F251\"\n",
        "                u\"\\U0001f926-\\U0001f937\"\n",
        "                u'\\U00010000-\\U0010ffff'\n",
        "                u\"\\u200d\"\n",
        "                u\"\\u2640-\\u2642\"\n",
        "                u\"\\u2600-\\u2B55\"\n",
        "                u\"\\u23cf\"\n",
        "                u\"\\u23e9\"\n",
        "                u\"\\u231a\"\n",
        "                u\"\\u3030\"\n",
        "                u\"\\ufe0f\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return re.sub(pat, '', line)\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Hi-K1gSDZW"
      },
      "source": [
        "import re\n",
        "def remove_ellipses(line:str)->str:\n",
        "    pat = re.compile(u'\\.\\.+')\n",
        "    return re.sub(pat, ' ', line)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6JFm16r7M3E"
      },
      "source": [
        "def to_lower(line:str)->str:\n",
        "    return line.lower()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSDv8R1lbfFP"
      },
      "source": [
        "def replace_number_with_tag(line:str)->str:\n",
        "    line = re.sub(\"\\s\\d*((\\.|\\,)\\d+)?\\s\", \" nummer \", line)\n",
        "    line = re.sub('\\s\\d+$', '', line)\n",
        "    line = re.sub('^\\d+\\s', '', line)\n",
        "    return line"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgSXlzZ4fAkF"
      },
      "source": [
        "def remove_urls(line:str)->str:\n",
        "    return re.sub('https?:\\/\\/\\S+', ' hyperlink ', line)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_TxMNrC7doY"
      },
      "source": [
        "def basic_clean(s:pd.Series)->pd.Series:\n",
        "    return s.map(to_lower)                                                  \\\n",
        "            .map(remove_emojis)                                             \\\n",
        "            .map(remove_roles)                                              \\\n",
        "            .map(remove_ellipses)                                           \\\n",
        "            .map(replace_number_with_tag)                                   \\\n",
        "            .map(remove_urls)\n",
        "\n",
        "def get_clean_train_test_df()->tuple:\n",
        "    train_df, test_df = get_train_test_df()\n",
        "    train_df['comment_text'] = basic_clean(train_df['comment_text'])\n",
        "    test_df['comment_text'] = basic_clean(test_df['comment_text'])\n",
        "    return train_df, test_df\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8YSZZ4vxTk5"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string\n",
        "import spacy\n",
        "from spacymoji import Emoji\n",
        "\n",
        "\n",
        "def is_punct_only(token:str)->bool:\n",
        "    for c in list(token):\n",
        "        if c not in string.punctuation:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_same(l1:list, l2:list)->bool:\n",
        "    if (len(l1) != len(l2)):\n",
        "        return False\n",
        "    for x, y in zip(l1, l2):\n",
        "        if x != y:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def do_basic_nlp_cleaning(line:str)->str:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(line)\n",
        "\n",
        "    # Some tokens start with a punctuation, remove the first one\n",
        "    def remove_first_punctuation(tok:str)->str:\n",
        "        return                                                              \\\n",
        "            tok[1:]                                                         \\\n",
        "            if tok[0] in set(string.punctuation) and len(tok) != 0          \\\n",
        "            else tok\n",
        "\n",
        "    tokens = [remove_first_punctuation(w) for w in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"german\"))\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [w for w in tokens if not is_punct_only(w)]\n",
        "\n",
        "    # Stem words\n",
        "    stem = SnowballStemmer('german')\n",
        "    tokens = [stem.stem(w) for w in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "import spacy\n",
        "def get_cleaning_function():\n",
        "    #nlp = spacy.load(\"de_dep_news_trf\")\n",
        "    nlp = spacy.load(\"de_core_news_sm\")\n",
        "    emoji = Emoji(nlp)\n",
        "    nlp.add_pipe(emoji, first=True)\n",
        "    stopwords = spacy.lang.de.stop_words.STOP_WORDS\n",
        "\n",
        "    def do_basic_nlp_cleaning(line:str)->str:\n",
        "        def is_interesting_token(token):\n",
        "            if token.pos_ in set(['NUM', 'SYM']):\n",
        "                return False\n",
        "            if token.text in stopwords:\n",
        "                return False\n",
        "            if (token.is_punct):\n",
        "                return False\n",
        "            #if token._.is_emoji:\n",
        "            #    return False\n",
        "            return True\n",
        "\n",
        "        def remove_terminal_punctuations(word):\n",
        "            word = word.strip()\n",
        "            while word != \"\" and word[0] in list(string.punctuation):\n",
        "                word = word[1:]\n",
        "            while word != \"\" and word[-1] in list(string.punctuation):\n",
        "                word = word[:-1]\n",
        "            return word\n",
        "\n",
        "        doc = nlp(line)\n",
        "        words = [tok.lemma_.lower() for tok in doc if is_interesting_token(tok)]\n",
        "        words = [remove_terminal_punctuations(word) for word in words]\n",
        "        words = [word for word in words if word != \"\"]\n",
        "        return  \" \".join(words)\n",
        "\n",
        "    return do_basic_nlp_cleaning\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def is_empty_string(s:str)->bool:\n",
        "    if s == '' or s == None:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "cleaning_fn = get_cleaning_function()\n",
        "train_df, test_df = get_clean_train_test_df()\n",
        "train_df['comment_text'] = train_df['comment_text'].map(cleaning_fn)\n",
        "empty_rows = train_df['comment_text'].map(is_empty_string)\n",
        "train_df = train_df[~ empty_rows]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZotXwWTZm7CB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7666d208-6db4-4223-f3bb-81273b254e33"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def get_initial_feel(model, vectorizer, y_column:str):\n",
        "    print('*' * 80)\n",
        "    print(f\"Classifying for : {y_column}\")\n",
        "    \n",
        "    trainX, testX, trainY, testY = train_test_split(\\\n",
        "                        train_df['comment_text'], train_df[y_column])\n",
        "\n",
        "    cv =  vectorizer.fit(trainX)\n",
        "    trainX = cv.transform(trainX)\n",
        "    testX = cv.transform(testX)\n",
        "\n",
        "    model.fit(trainX, trainY)\n",
        "\n",
        "    predY = model.predict(testX)\n",
        "\n",
        "    print(confusion_matrix(testY, predY))\n",
        "    print(f\"Accuracy: {accuracy_score(testY, predY)}\")\n",
        "    print(f\"f1-score: {f1_score(testY, predY)}\")\n",
        "\n",
        "def tryout(model, vectorizer):\n",
        "    print(repr(model), repr(vectorizer))\n",
        "    print()\n",
        "    get_initial_feel(model, vectorizer, 'Sub1_Toxic')\n",
        "    get_initial_feel(model, vectorizer, 'Sub2_Engaging')\n",
        "    get_initial_feel(model, vectorizer, 'Sub3_FactClaiming')\n",
        "    print()\n",
        "\n",
        "\n",
        "tryout(MultinomialNB(), CountVectorizer(ngram_range=(1, 3)))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True) CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)\n",
            "\n",
            "********************************************************************************\n",
            "Classifying for : Sub1_Toxic\n",
            "[[438  54]\n",
            " [211  80]]\n",
            "Accuracy: 0.6615581098339719\n",
            "f1-score: 0.3764705882352941\n",
            "********************************************************************************\n",
            "Classifying for : Sub2_Engaging\n",
            "[[488  84]\n",
            " [128  83]]\n",
            "Accuracy: 0.7292464878671775\n",
            "f1-score: 0.4391534391534392\n",
            "********************************************************************************\n",
            "Classifying for : Sub3_FactClaiming\n",
            "[[417 106]\n",
            " [100 160]]\n",
            "Accuracy: 0.7369093231162197\n",
            "f1-score: 0.6083650190114068\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbcLIE7oC2ML"
      },
      "source": [
        "## BERT MODEL\n",
        "\n",
        "Use code from https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9TbWSQ0c7C_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8826b8df-35d7-49f2-c22f-8f861bf3652e"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
        "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
        "\n",
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "text_test = [\"\"\"this is such an amazing movie! evaluationScriptGermeval2018.pl\n",
        "\n",
        "represents the evaluation tool. It can be used to evaluated the predictions for both tasks of the shared task.\n",
        "\n",
        "In order to obtain further information on the usage of that tool please type:\n",
        "As discussed in Section 2, our pretrained language models will learn pre-existing biases from the\n",
        "training datasets. The main portion (89%) of our training data, namely the OSCAR dataset, uses texts\n",
        "scraped from the internet, which is in some respects problematic. First off, this dataset contains a lot of\n",
        "explicit and indecent material. While we filtered out many of these documents through keyword match-\n",
        "ing, we cannot guarantee that this method was successful in every case. Furthermore, many websites\n",
        "contain unverified information and any dataset containing this kind of text can lead to a skewed model\n",
        "that reflects commonly found lies and misconceptions. This includes gender, racial and religious biases\n",
        "which are found in textual data of all registers and so we advise that anyone using our model to recognise\n",
        "that it will not always build true and accurate representation of real world concepts. We implore users\n",
        "of the model to seriously consider these issues before deploying it in a production setting, especially\n",
        "in situations where impartiality matter, such as journalism, and institutional decision making like job\n",
        "\n",
        "The underground cities were well designed for protection against attacks. The few entrances were hidden by foliage and not easily spotted from outside. Inside, they took the form of a labyrinth of passageways which were unnavigable for outsiders, and could be sealed with large rock doors, around a metre high and shaped like mill-stones. These doors were built such that they could be rolled into a closed position relatively easily, but could not be moved from the outside. They had a hole in the centre which was probably used as a kind of peephole. In some cities there were holes in ceiling above, through which the enemy could be attacked with spears.[6] The cities descended up to twelve stories – over 100 metres – under the ground and had everything necessary for a long siege. The upper stories were largely used as stables and storerooms, with a constant temperature of around 10 °C. In the walls of the caverns there were receptacles for various kinds of food, as well as hollows for vessels in which liquids could be stored. Further down, were the living and working spaces, where furniture, including seats, tables, and beds were carved out of the rock. Working spaces include a wine press at Derinkuyu, a copper foundry in Kaymakli, as well as cisterns and wells which ensured a supply of drinking water during a long siege.[9] There were also prisons and toilets.\n",
        "applications or insurance assessments\n",
        "\"\"\"]\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "#print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "#print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "#print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "#print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
        "#print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')\n",
        "\n",
        "\n",
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "#print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled Outputs Shape:(1, 512)\n",
            "Pooled Outputs Values:[ 0.9107527   0.4435962  -0.0445334   0.07857981  0.13065211  0.95631254\n",
            "  0.9006179  -0.79465264 -0.56049114 -0.94223607 -0.3001277  -0.973857  ]\n",
            "Sequence Outputs Shape:(1, 128, 512)\n",
            "Sequence Outputs Values:[[ 0.01486441  0.7203154   0.14484876 ... -0.8022307   0.06363815\n",
            "   0.95406747]\n",
            " [-0.65695393  0.08590551  0.15920955 ...  0.64430267 -0.19999222\n",
            "   0.7745597 ]\n",
            " [-0.81394494  0.74874014 -0.43530738 ...  0.07674243 -0.4267268\n",
            "   0.6159239 ]\n",
            " ...\n",
            " [-0.65857285  1.4859799   0.7598989  ... -0.02287899  0.23170051\n",
            "   0.85067797]\n",
            " [-0.77028626  1.3962332   0.19560781 ... -0.1806682   0.76875293\n",
            "   1.0642036 ]\n",
            " [ 0.44916624  0.7581736   0.42707348 ... -0.8116752   1.2785095\n",
            "   1.1305085 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qI8qAcpDYo3",
        "outputId": "fc36060f-9734-471a-832a-7a8af2857581"
      },
      "source": [
        "print(bert_results.keys())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['sequence_output', 'encoder_outputs', 'default', 'pooled_output'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt-7fa8Ip0TM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09225998-1cdf-4238-aafe-208bbfc77468"
      },
      "source": [
        "print(text_preprocessed['input_mask'])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "  1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]], shape=(1, 128), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEzTyC_sru6w"
      },
      "source": [
        "## HUGGING FACE TRANSFORMERS : GERMAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV64NvetqfkS"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BUIUip_rWrh",
        "outputId": "1b95edbf-9dc4-455c-d3f9-ff67e993b1f9"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "import tensorflow as tf\n",
        "  \n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
        "\n",
        "bert_model = AutoModelForMaskedLM.from_pretrained(\"bert-base-german-cased\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICvn-q-Vrzlb"
      },
      "source": [
        "def build_classifier_model():\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessed_input = bert_tokenizer(text_input)\n",
        "  outputs = bert_model(preprocessed_input)\n",
        "  net = outputs['pooled_output']\n",
        "  net = tf.keras.layers.Dropout(0.1)(net)\n",
        "  net = tf.keras.layers.Dense(1, activation=None, name='classifier')(net)\n",
        "  return tf.keras.Model(text_input, net)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "G5j860RDr_cf",
        "outputId": "2673a1b7-b71f-41ca-89e6-2364fccb7800"
      },
      "source": [
        "full_model = build_classifier_model()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-95fa54bbf7a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfull_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_classifier_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-c95fca320395>\u001b[0m in \u001b[0;36mbuild_classifier_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_classifier_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mtext_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mpreprocessed_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pooled_output'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_is_valid_text_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2378\u001b[0m             raise ValueError(\n\u001b[0;32m-> 2379\u001b[0;31m                 \u001b[0;34m\"text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2380\u001b[0m                 \u001b[0;34m\"or `List[List[str]]` (batch of pretokenized examples).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m             )\n",
            "\u001b[0;31mValueError\u001b[0m: text input must of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPLQFo2FstuR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}