{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6ga1UPDE4UJLyM/zfOrf0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment1/NLP_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERvBOEn0wss6"
      },
      "source": [
        "!pip install spacy  nltk spacymoji huggingface -q       >/dev/null 2>&1         \n",
        "!pip install -q -U tensorflow-text                      >/dev/null 2>&1\n",
        "!pip install -q tf-models-official                      >/dev/null 2>&1\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "!pip install transformers                               >/dev/null 2>&1\n",
        "\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "\n",
        "!pip install mlxtend                                    >/dev/null 2>&1\n",
        "!pip install imblearn                                   >/dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-DAX9VLZlw3"
      },
      "source": [
        "import requests\n",
        "def get_train_test_files():\n",
        "    TRAIN_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Test_For_Evaluation.csv'\n",
        "    TRAIN_FILE_LOCAL = 'Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE_LOCAL = 'Assessment1_Toxic_Test.csv'\n",
        "\n",
        "    def download(url, localfile):\n",
        "        with open(localfile, 'wb') as f:\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            f.write(r.content)\n",
        "\n",
        "    download(TRAIN_FILE, TRAIN_FILE_LOCAL)\n",
        "    download(TEST_FILE, TEST_FILE_LOCAL)\n",
        "\n",
        "    return TRAIN_FILE_LOCAL, TEST_FILE_LOCAL\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrlJ1H1maDxM"
      },
      "source": [
        "# Functions to read the CSV and do basic cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVfYulLnuGVJ"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "def remove_roles(line:str)->str:\n",
        "    # Remove texts like @USER, @MODERATOR etc\n",
        "    pat = re.compile(u'\\@[A-Za-z]+')\n",
        "    return re.sub(pat, '', line)\n",
        "\n",
        "def get_train_test_df():\n",
        "    train_csv, test_csv = get_train_test_files()\n",
        "\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "def remove_emojis(line:str)->str:\n",
        "    pat = re.compile(\"[\"\n",
        "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                u\"\\U00002702-\\U000027B0\"\n",
        "                u\"\\U000024C2-\\U0001F251\"\n",
        "                u\"\\U0001f926-\\U0001f937\"\n",
        "                u'\\U00010000-\\U0010ffff'\n",
        "                u\"\\u200d\"\n",
        "                u\"\\u2640-\\u2642\"\n",
        "                u\"\\u2600-\\u2B55\"\n",
        "                u\"\\u23cf\"\n",
        "                u\"\\u23e9\"\n",
        "                u\"\\u231a\"\n",
        "                u\"\\u3030\"\n",
        "                u\"\\ufe0f\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return re.sub(pat, '', line)\n",
        "\n",
        "def remove_ellipses(line:str)->str:\n",
        "    pat = re.compile(u'\\.\\.+')\n",
        "    return re.sub(pat, ' ', line)\n",
        "\n",
        "def to_lower(line:str)->str:\n",
        "    return line.lower()\n",
        "\n",
        "def replace_number_with_tag(line:str)->str:\n",
        "    line = re.sub(\"\\s\\d*((\\.|\\,)\\d+)?\\s\", \" nummer \", line)\n",
        "    line = re.sub('\\s\\d+$', '', line)\n",
        "    line = re.sub('^\\d+\\s', '', line)\n",
        "    return line\n",
        "\n",
        "def remove_urls(line:str)->str:\n",
        "    return re.sub('https?:\\/\\/\\S+', ' hyperlink ', line)\n",
        "\n",
        "def basic_clean(s:pd.Series)->pd.Series:\n",
        "    return s.map(to_lower)                                                  \\\n",
        "            .map(remove_emojis)                                             \\\n",
        "            .map(remove_roles)                                              \\\n",
        "            .map(remove_ellipses)                                           \\\n",
        "            .map(replace_number_with_tag)                                   \\\n",
        "            .map(remove_urls)\n",
        "\n",
        "def get_clean_train_test_df()->tuple:\n",
        "    train_df, test_df = get_train_test_df()\n",
        "    train_df['comment_text'] = basic_clean(train_df['comment_text'])\n",
        "    test_df['comment_text'] = basic_clean(test_df['comment_text'])\n",
        "    return train_df, test_df\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6_dto2JaR2f"
      },
      "source": [
        "# Clean using Spacy and Enrich"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8YSZZ4vxTk5"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string\n",
        "import spacy\n",
        "from spacymoji import Emoji\n",
        "import  de_core_news_sm\n",
        "\n",
        "def is_punct_only(token:str)->bool:\n",
        "    for c in list(token):\n",
        "        if c not in string.punctuation:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_same(l1:list, l2:list)->bool:\n",
        "    if (len(l1) != len(l2)):\n",
        "        return False\n",
        "    for x, y in zip(l1, l2):\n",
        "        if x != y:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def get_num_of_allcap_words(s:str)->int:\n",
        "    def is_allcaps(s:str)->bool:\n",
        "        if (len(s) < 3):\n",
        "            return False\n",
        "        for c in list(s):\n",
        "            if not (\\\n",
        "                    (ord(c) <=ord('Z') and ord(c) >= ord('A')) or           \\\n",
        "                    (ord(c) >= ord('0') and ord(c) <= ord('9'))             \\\n",
        "                    ):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    if len(s) < 3:\n",
        "        return 0\n",
        "    tokens = [w.strip() for w in s.split()]\n",
        "    return sum([1 for t in tokens if is_allcaps(t)])\n",
        "\n",
        "def get_percentage_of_excalamations(s:str)->float:\n",
        "    if len(s) == 0:\n",
        "        return 0.0\n",
        "    exclamation_count = sum([1 for c in list(s) if c == '!'])\n",
        "    return exclamation_count / len(s)\n",
        "\n",
        "\n",
        "def is_empty_string(s:str)->bool:\n",
        "    if s == '' or s == None:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def do_basic_nlp_cleaning(line:str)->str:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(line)\n",
        "\n",
        "    # Some tokens start with a punctuation, remove the first one\n",
        "    def remove_first_punctuation(tok:str)->str:\n",
        "        return                                                              \\\n",
        "            tok[1:]                                                         \\\n",
        "            if tok[0] in set(string.punctuation) and len(tok) != 0          \\\n",
        "            else tok\n",
        "\n",
        "    tokens = [remove_first_punctuation(w) for w in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"german\"))\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [w for w in tokens if not is_punct_only(w)]\n",
        "\n",
        "    # Stem words\n",
        "    stem = SnowballStemmer('german')\n",
        "    tokens = [stem.stem(w) for w in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def get_cleaning_function():\n",
        "    #nlp = spacy.load(\"de_dep_news_trf\")\n",
        "    #nlp = spacy.load(\"de_core_news_sm\")\n",
        "    nlp = de_core_news_sm.load()\n",
        "    emoji = Emoji(nlp)\n",
        "    nlp.add_pipe(emoji, first=True)\n",
        "    stopwords = spacy.lang.de.stop_words.STOP_WORDS\n",
        "\n",
        "    def do_basic_nlp_cleaning(line:str)->str:\n",
        "        def is_interesting_token(token):\n",
        "            if token.pos_ in set(['NUM', 'SYM']):\n",
        "                return False\n",
        "            if token.text in stopwords:\n",
        "                return False\n",
        "            if (token.is_punct):\n",
        "                return False\n",
        "            #if token._.is_emoji:\n",
        "            #    return False\n",
        "            return True\n",
        "\n",
        "        def remove_terminal_punctuations(word):\n",
        "            word = word.strip()\n",
        "            while word != \"\" and word[0] in list(string.punctuation):\n",
        "                word = word[1:]\n",
        "            while word != \"\" and word[-1] in list(string.punctuation):\n",
        "                word = word[:-1]\n",
        "            return word\n",
        "\n",
        "        doc = nlp(line)\n",
        "        words = [tok.lemma_.lower() for tok in doc if is_interesting_token(tok)]\n",
        "        words = [remove_terminal_punctuations(word) for word in words]\n",
        "        words = [word for word in words if word != \"\"]\n",
        "        return  \" \".join(words)\n",
        "\n",
        "    return do_basic_nlp_cleaning\n",
        "\n",
        "def get_enriched_dataset(df):\n",
        "    df['cleaned_comment_text'] = df['comment_text'].map(cleaning_fn)\n",
        "    df['n_all_caps'] = df['comment_text'].map(get_num_of_allcap_words)\n",
        "    df['perc_exclamations'] = df['comment_text'].map(get_percentage_of_excalamations)\n",
        "    df['num_exclamations'] = df['comment_text'].map(lambda s: sum([1 for x in list(s) if x == '!']))\n",
        "    return df\n",
        "\n",
        "cleaning_fn = get_cleaning_function()\n",
        "train_df, test_df = get_clean_train_test_df()\n",
        "train_df = get_enriched_dataset(train_df)\n",
        "test_df = get_enriched_dataset(test_df)\n",
        "empty_rows = train_df['cleaned_comment_text'].map(is_empty_string)\n",
        "train_df = train_df[~ empty_rows]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWTtxHw0aa4N"
      },
      "source": [
        "# Print Enriched Training DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "n3q-TKbwZRBJ",
        "outputId": "36af8c7f-baa1-4c46-877a-30a4d8032445"
      },
      "source": [
        "train_df"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>Sub1_Toxic</th>\n",
              "      <th>Sub2_Engaging</th>\n",
              "      <th>Sub3_FactClaiming</th>\n",
              "      <th>cleaned_comment_text</th>\n",
              "      <th>n_all_caps</th>\n",
              "      <th>perc_exclamations</th>\n",
              "      <th>num_exclamations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gestern bei illner, montag bei nummer  ist das...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>gestern illner montag nummer langsam ör-partei...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mein gott der war erst gestern bei illner. die...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>gott gestern illner redaktionen versagen</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>die cdu lässt das so wie so nicht zu . sagen ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>cdu lässt sagen reich bekommen nummer milliard...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bei meiner beschissenen rente als 2x geschiede...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>beschissen rente geschieden mann steuern krank...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wer nummer jahre zum mindestlohn arbeiten muß,...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>nummer mindestlohn arbeiten erhalten € rente n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00495</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3189</th>\n",
              "      <td>hier mal eine info. flüchtlinge werden nummer ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>mal info flüchtlinge nummer km küste schlauchb...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3190</th>\n",
              "      <td>.aha .mal abwarten kommt bei uns auch .firmen ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>aha mal abwarten firmen entlassen mitarbeiter ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3191</th>\n",
              "      <td>.so ist es</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>so</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3192</th>\n",
              "      <td>.die warten da</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>die warten</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3193</th>\n",
              "      <td>.das bekommen die gesagt wie sich verhalten s...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>das bekommen verhalten kameras richten tut ers...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3131 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           comment_text  ...  num_exclamations\n",
              "0     gestern bei illner, montag bei nummer  ist das...  ...                 0\n",
              "1     mein gott der war erst gestern bei illner. die...  ...                 0\n",
              "2      die cdu lässt das so wie so nicht zu . sagen ...  ...                 0\n",
              "3     bei meiner beschissenen rente als 2x geschiede...  ...                 0\n",
              "4     wer nummer jahre zum mindestlohn arbeiten muß,...  ...                 3\n",
              "...                                                 ...  ...               ...\n",
              "3189  hier mal eine info. flüchtlinge werden nummer ...  ...                 0\n",
              "3190  .aha .mal abwarten kommt bei uns auch .firmen ...  ...                 0\n",
              "3191                                         .so ist es  ...                 0\n",
              "3192                                     .die warten da  ...                 0\n",
              "3193   .das bekommen die gesagt wie sich verhalten s...  ...                 0\n",
              "\n",
              "[3131 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68DFX_fjYgQz"
      },
      "source": [
        "# Multinomial NB (original)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "zL5Boe5qYfF4",
        "outputId": "2dd64d8b-af27-48a9-d418-b98d972c6dfd"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.pipeline import Pipeline\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE \n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.preprocessing import DenseTransformer\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "def get_feature_column_names(df):\n",
        "    return [cname for cname in df.columns if not cname.startswith('Sub')]\n",
        "\n",
        "def get_target_column_names(df):\n",
        "    return [cname for cname in df.columns if cname.startswith('Sub')]\n",
        "\n",
        "def is_text_column(colname:str)->bool:\n",
        "    if 'text' in colname:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_text_columns(df)->list:\n",
        "    return [cn for cn in df.columns if is_text_column(cn)]\n",
        "\n",
        "def get_nontext_columns(df)->list:\n",
        "    return [cn for cn in df.columns if not is_text_column(cn)]\n",
        "\n",
        "def run_classification(                                                     \\\n",
        "                       dataset:pd.DataFrame,                                \\\n",
        "                       target_column:str,                                   \\\n",
        "                       clf_gen_fn,                                          \\\n",
        "                       use_smote=False)->tuple:\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'n_all_caps']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'num_exclamations']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'perc_exclamations']]\n",
        "    X = dataset[get_feature_column_names(dataset)]\n",
        "    y = dataset[target_column]\n",
        "    trainX, testX, trainY, testY = train_test_split(X, y, random_state=0)\n",
        "\n",
        "    def get_text_pipeline():\n",
        "        return Pipeline(                                                    \\\n",
        "                        [                                                   \\\n",
        "                            ('cv', CountVectorizer(),),                     \\\n",
        "                            ('tfid', TfidfVectorizer()),                    \\\n",
        "                        ])\n",
        "\n",
        "                        \n",
        "    column_trans = make_column_transformer(                                 \\\n",
        "                            (CountVectorizer(ngram_range=(1,1)), 'cleaned_comment_text'),   \\\n",
        "                            (TfidfVectorizer(), 'cleaned_comment_text'),    \\\n",
        "                            remainder=MinMaxScaler(),                       \\\n",
        "                        )\n",
        "\n",
        "    if use_smote:\n",
        "        classif_pipeline = Pipeline(                                        \\\n",
        "                                [                                           \\\n",
        "                                    ('column_transformer', column_trans),   \\\n",
        "                                    ('dense', DenseTransformer()),          \\\n",
        "                                    ('smote', SMOTE(n_jobs=-1)),            \\\n",
        "                                    ('clf', clf_gen_fn()),                  \\\n",
        "                                ])\n",
        "    else:\n",
        "        classif_pipeline = Pipeline(                                        \\\n",
        "                                [                                           \\\n",
        "                                    ('column_transformer', column_trans),   \\\n",
        "                                    ('dense', DenseTransformer()),          \\\n",
        "                                    ('clf', clf_gen_fn()),                  \\\n",
        "                                ])\n",
        "    \n",
        "    \n",
        "    classif_pipeline.fit(trainX, trainY)\n",
        "    y_pred = classif_pipeline.predict(testX)\n",
        "\n",
        "    return accuracy_score(testY, y_pred), f1_score(testY, y_pred)\n",
        "\n",
        "def run_classifiers():\n",
        "\n",
        "    classifiers = {\n",
        "        \"LinearSVC_nosmote\": (False, lambda: LinearSVC(),),\n",
        "        \"LinearSVC\": (True, lambda: LinearSVC(),),\n",
        "        \"AdaBoost\": (True, lambda: AdaBoostClassifier(),),\n",
        "        \"AdaBoost_nosmote\": (False, lambda: AdaBoostClassifier(),),\n",
        "        \"MultinomialNB_nosmote\": (False, lambda: MultinomialNB(),),\n",
        "        \"RandomForestClassifier\": (True, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "        \"RandomForestClassifier_nosmote\": (False, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "        \"BernoulliNB_nosmote\": (False, lambda: BernoulliNB(),),\n",
        "    }\n",
        "\n",
        "    result_df = pd.DataFrame(                                                           \\\n",
        "                    {                                                       \\\n",
        "                        'classifier': pd.Series('str'),                     \\\n",
        "                        'task_name': pd.Series('str'),                      \\\n",
        "                        'metric': pd.Series('str'),                         \\\n",
        "                        'smote': pd.Series(int),                            \\\n",
        "                        'value': pd.Series(float),                          \\\n",
        "                    })\n",
        "\n",
        "    for clfname, value in classifiers.items():\n",
        "        use_smote, clfgen = value\n",
        "        for colname in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "            accuracy, f1 = run_classification(train_df, colname, clfgen, use_smote)\n",
        "            print(f\"{clfname:20.20s} {colname:20.20s} accuracy={accuracy:1.3f}              f1={f1:1.3f}   smote={use_smote}\")\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'accuracy',\n",
        "                'value': accuracy,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'f1_score',\n",
        "                'value': f1,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "\n",
        "        \n",
        "    return result_df\n",
        "\n",
        "result_df = run_classifiers()\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-dea513807721>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#from sklearn.pipeline import Pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \"\"\"\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFunctionSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'six' from 'sklearn.externals' (/usr/local/lib/python3.7/dist-packages/sklearn/externals/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzgub_1VakyE"
      },
      "source": [
        "# Multinomial NB pipeline (modified)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5pktZUUkIxo",
        "outputId": "52d88641-318e-4c99-bf9c-49a6dba1c0d3"
      },
      "source": [
        "!pip install --upgrade scikit-learn\n",
        "!pip install imblearn"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (from imblearn) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->imbalanced-learn->imblearn) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->imbalanced-learn->imblearn) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV_S-WOU3sQp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "PXqnV32AaoCL",
        "outputId": "815010ee-cc31-4111-eb34-c83f93a146f5"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "#from imblearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.preprocessing import DenseTransformer\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "import sklearn\n",
        "import itertools\n",
        "\n",
        "def get_feature_column_names(df):\n",
        "    return [cname for cname in df.columns if not cname.startswith('Sub')]\n",
        "\n",
        "def get_target_column_names(df):\n",
        "    return [cname for cname in df.columns if cname.startswith('Sub')]\n",
        "\n",
        "def is_text_column(colname:str)->bool:\n",
        "    if 'text' in colname:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_text_columns(df)->list:\n",
        "    return [cn for cn in df.columns if is_text_column(cn)]\n",
        "\n",
        "def get_nontext_columns(df)->list:\n",
        "    return [cn for cn in df.columns if not is_text_column(cn)]\n",
        "\n",
        "def run_classification(                                                     \\\n",
        "                       dataset:pd.DataFrame,                                \\\n",
        "                       target_column:str,                                   \\\n",
        "                       clf_gen_fn,                                          \\\n",
        "                       use_smote=False)->tuple:\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'n_all_caps']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'num_exclamations']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'perc_exclamations']]\n",
        "    X = dataset[get_feature_column_names(dataset)]\n",
        "    y = dataset[target_column]\n",
        "    trainX, testX, trainY, testY = train_test_split(X, y, random_state=0)\n",
        "\n",
        "    def get_text_pipeline():\n",
        "        return Pipeline(                                                    \\\n",
        "                        [                                                   \\\n",
        "                            ('cv', CountVectorizer(),),                     \\\n",
        "                            ('tfidt', TfidfVectorizer(),),                  \\\n",
        "                        ])\n",
        "\n",
        "                        \n",
        "    column_trans = make_column_transformer(                                 \\\n",
        "                            (CountVectorizer(ngram_range=(1,1)), 'cleaned_comment_text'),   \\\n",
        "                            (TfidfVectorizer(ngram_range=(1,1)), 'cleaned_comment_text'),    \\\n",
        "                            (TfidfVectorizer(ngram_range=(1,1), use_idf=False), 'cleaned_comment_text'),    \\\n",
        "                            remainder='drop',                               \\\n",
        "                        )\n",
        "\n",
        "    if use_smote:\n",
        "        classif_pipeline = Pipeline(                                        \\\n",
        "                                [                                           \\\n",
        "                                    ('column_transformer', column_trans),   \\\n",
        "                                    ('dense', DenseTransformer()),          \\\n",
        "                                    ('clf', clf_gen_fn()),                  \\\n",
        "                                ])\n",
        "    else:\n",
        "        classif_pipeline = Pipeline(                                        \\\n",
        "                                [                                           \\\n",
        "                                    ('column_transformer', column_trans),   \\\n",
        "                                    ('dense', DenseTransformer()),          \\\n",
        "                                    ('clf', clf_gen_fn()),                  \\\n",
        "                                ])\n",
        "\n",
        "    def get_dummy_classifier(cols):\n",
        "        col_selector = ColumnTransformer([('selector', 'passthrough', cols)], remainder='drop')\n",
        "        classif2_pipeline = Pipeline(                                           \\\n",
        "                                [                                               \\\n",
        "                                    ('colselector', col_selector),              \\\n",
        "                                    ('dummy', LinearSVC(max_iter=100_000)),                 \\\n",
        "                                ])\n",
        "        return classif2_pipeline\n",
        "\n",
        "    import itertools\n",
        "    col_arr = []\n",
        "    numericcols = get_nontext_columns(trainX)\n",
        "    for i in range(1, min(3, len(numericcols) + 1)):\n",
        "        for arr in itertools.combinations(numericcols, i):\n",
        "            col_arr.append(list(arr))\n",
        "\n",
        "    classifier_array = [('classifier1', classif_pipeline)]\n",
        "    for n, arr in enumerate(col_arr):\n",
        "        classifier_array.append((f'classifier_{n}', get_dummy_classifier(arr)))\n",
        "\n",
        "    classifier = StackingClassifier(classifier_array, final_estimator=RandomForestClassifier())\n",
        "       \n",
        "    classifier.fit(trainX, trainY)\n",
        "    y_pred = classifier.predict(testX)\n",
        "\n",
        "\n",
        "    return accuracy_score(testY, y_pred), f1_score(testY, y_pred), Pipeline([('pipeline', classifier)])\n",
        "\n",
        "\n",
        "def run_classifiers():\n",
        "\n",
        "    classifiers = {\n",
        "        #\"LinearSVC_nosmote\": (False, lambda: LinearSVC(),),\n",
        "        #\"LinearSVC\": (True, lambda: LinearSVC(),),\n",
        "        #\"AdaBoost\": (True, lambda: AdaBoostClassifier(),),\n",
        "        #\"AdaBoost_nosmote\": (False, lambda: AdaBoostClassifier(),),\n",
        "        \"MultinomialNB_nosmote\": (False, lambda: MultinomialNB(),),\n",
        "        #\"RandomForestClassifier\": (True, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "        #\"RandomForestClassifier_nosmote\": (False, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "        #\"BernoulliNB_nosmote\": (False, lambda: BernoulliNB(),),\n",
        "    }\n",
        "\n",
        "    result_df = pd.DataFrame(                                                           \\\n",
        "                    {                                                       \\\n",
        "                        'classifier': pd.Series('str'),                     \\\n",
        "                        'task_name': pd.Series('str'),                      \\\n",
        "                        'metric': pd.Series('str'),                         \\\n",
        "                        'smote': pd.Series(int),                            \\\n",
        "                        'value': pd.Series(float),                          \\\n",
        "                    })\n",
        "\n",
        "    model = None\n",
        "    for clfname, value in classifiers.items():\n",
        "        use_smote, clfgen = value\n",
        "        for colname in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "            accuracy, f1, model = run_classification(train_df, colname, clfgen, use_smote)\n",
        "            print(f\"{clfname:20.20s} {colname:20.20s} accuracy={accuracy:1.3f}              f1={f1:1.3f}   smote={use_smote}\")\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'accuracy',\n",
        "                'value': accuracy,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'f1_score',\n",
        "                'value': f1,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "\n",
        "        \n",
        "    return result_df, model\n",
        "\n",
        "result_df, model = run_classifiers()\n",
        "\n",
        "sklearn.set_config(display=\"diagram\")\n",
        "model\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultinomialNB_nosmot Sub1_Toxic           accuracy=0.670              f1=0.429   smote=False\n",
            "MultinomialNB_nosmot Sub2_Engaging        accuracy=0.748              f1=0.412   smote=False\n",
            "MultinomialNB_nosmot Sub3_FactClaiming    accuracy=0.746              f1=0.549   smote=False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style>#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 {color: black;background-color: white;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 pre{padding: 0;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-toggleable {background-color: white;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-estimator:hover {background-color: #d4ebff;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-item {z-index: 1;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-parallel-item:only-child::after {width: 0;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-8453bcb1-0aeb-4cdf-8ca5-80116642fe92\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"41d6e658-f18f-4413-b9c1-aebe5c5934f9\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"41d6e658-f18f-4413-b9c1-aebe5c5934f9\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('pipeline',\n",
              "                 StackingClassifier(estimators=[('classifier1',\n",
              "                                                 Pipeline(steps=[('column_transformer',\n",
              "                                                                  ColumnTransformer(transformers=[('countvectorizer',\n",
              "                                                                                                   CountVectorizer(),\n",
              "                                                                                                   'cleaned_comment_text'),\n",
              "                                                                                                  ('tfidfvectorizer-1',\n",
              "                                                                                                   TfidfVectorizer(),\n",
              "                                                                                                   'cleaned_comment_text'),\n",
              "                                                                                                  ('tfidfvectorizer-2',\n",
              "                                                                                                   TfidfVectorizer(use_idf=False),\n",
              "                                                                                                   'cleaned_comment_text')])),\n",
              "                                                                 ('d...\n",
              "                                                                  ColumnTransformer(transformers=[('selector',\n",
              "                                                                                                   'passthrough',\n",
              "                                                                                                   ['n_all_caps',\n",
              "                                                                                                    'num_exclamations'])])),\n",
              "                                                                 ('dummy',\n",
              "                                                                  LinearSVC(max_iter=100000))])),\n",
              "                                                ('classifier_5',\n",
              "                                                 Pipeline(steps=[('colselector',\n",
              "                                                                  ColumnTransformer(transformers=[('selector',\n",
              "                                                                                                   'passthrough',\n",
              "                                                                                                   ['perc_exclamations',\n",
              "                                                                                                    'num_exclamations'])])),\n",
              "                                                                 ('dummy',\n",
              "                                                                  LinearSVC(max_iter=100000))]))],\n",
              "                                    final_estimator=RandomForestClassifier()))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5aa687f2-3b01-46ab-9308-db3106d075c0\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"5aa687f2-3b01-46ab-9308-db3106d075c0\">pipeline: StackingClassifier</label><div class=\"sk-toggleable__content\"><pre>StackingClassifier(estimators=[('classifier1',\n",
              "                                Pipeline(steps=[('column_transformer',\n",
              "                                                 ColumnTransformer(transformers=[('countvectorizer',\n",
              "                                                                                  CountVectorizer(),\n",
              "                                                                                  'cleaned_comment_text'),\n",
              "                                                                                 ('tfidfvectorizer-1',\n",
              "                                                                                  TfidfVectorizer(),\n",
              "                                                                                  'cleaned_comment_text'),\n",
              "                                                                                 ('tfidfvectorizer-2',\n",
              "                                                                                  TfidfVectorizer(use_idf=False),\n",
              "                                                                                  'cleaned_comment_text')])),\n",
              "                                                ('dense', DenseTransformer()),\n",
              "                                                ('...\n",
              "                                                 ColumnTransformer(transformers=[('selector',\n",
              "                                                                                  'passthrough',\n",
              "                                                                                  ['n_all_caps',\n",
              "                                                                                   'num_exclamations'])])),\n",
              "                                                ('dummy',\n",
              "                                                 LinearSVC(max_iter=100000))])),\n",
              "                               ('classifier_5',\n",
              "                                Pipeline(steps=[('colselector',\n",
              "                                                 ColumnTransformer(transformers=[('selector',\n",
              "                                                                                  'passthrough',\n",
              "                                                                                  ['perc_exclamations',\n",
              "                                                                                   'num_exclamations'])])),\n",
              "                                                ('dummy',\n",
              "                                                 LinearSVC(max_iter=100000))]))],\n",
              "                   final_estimator=RandomForestClassifier())</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier1</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c6d3e1ba-bece-49b5-8a46-620a4553781a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"c6d3e1ba-bece-49b5-8a46-620a4553781a\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('countvectorizer', CountVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-1', TfidfVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-2',\n",
              "                                 TfidfVectorizer(use_idf=False),\n",
              "                                 'cleaned_comment_text')])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"768159b2-8601-4923-b82b-32447962631f\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"768159b2-8601-4923-b82b-32447962631f\">countvectorizer</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"3314cb99-85c9-44bc-8342-c5bee9f2c0fe\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"3314cb99-85c9-44bc-8342-c5bee9f2c0fe\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"1e1d121c-3760-49b3-b229-5253d396926a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"1e1d121c-3760-49b3-b229-5253d396926a\">tfidfvectorizer-1</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e931a99b-f8d3-4aec-8489-a4046b482dcf\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e931a99b-f8d3-4aec-8489-a4046b482dcf\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"cdb61c89-c181-4fe0-aad9-9b50dd64a367\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"cdb61c89-c181-4fe0-aad9-9b50dd64a367\">tfidfvectorizer-2</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"711177c7-a6d1-45fd-9955-e32c87e58477\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"711177c7-a6d1-45fd-9955-e32c87e58477\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(use_idf=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"419c902d-cf18-4031-af23-ef22fd134fb1\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"419c902d-cf18-4031-af23-ef22fd134fb1\">DenseTransformer</label><div class=\"sk-toggleable__content\"><pre>DenseTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"cc66daf7-0915-4301-85f5-bcacf23dd87a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"cc66daf7-0915-4301-85f5-bcacf23dd87a\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_0</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"df397caa-f68a-477f-bb16-ce44d94f64ae\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"df397caa-f68a-477f-bb16-ce44d94f64ae\">colselector: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('selector', 'passthrough', ['n_all_caps'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"9b0f45e4-b065-4941-8f63-0739a43cf675\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"9b0f45e4-b065-4941-8f63-0739a43cf675\">selector</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5b6d700d-4577-496c-96a6-d81e812679ee\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"5b6d700d-4577-496c-96a6-d81e812679ee\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"bb4c4bc4-8eef-4d1f-b30a-9f9fe62adf1e\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"bb4c4bc4-8eef-4d1f-b30a-9f9fe62adf1e\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(max_iter=100000)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_1</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c732a722-62db-4e8e-82db-34b6827ab6ea\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"c732a722-62db-4e8e-82db-34b6827ab6ea\">colselector: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('selector', 'passthrough',\n",
              "                                 ['perc_exclamations'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f635c61f-3679-4c28-9ebf-1dfada498413\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"f635c61f-3679-4c28-9ebf-1dfada498413\">selector</label><div class=\"sk-toggleable__content\"><pre>['perc_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e34f7f46-97f6-4aa3-a089-559d6cc0680a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e34f7f46-97f6-4aa3-a089-559d6cc0680a\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"1b279f81-7318-490b-a400-4b6359197820\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"1b279f81-7318-490b-a400-4b6359197820\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(max_iter=100000)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_2</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"d516b873-5cc0-4563-b94b-866b93e4a561\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"d516b873-5cc0-4563-b94b-866b93e4a561\">colselector: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('selector', 'passthrough',\n",
              "                                 ['num_exclamations'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"35ec14e5-206b-41d5-b459-c5cbc9267242\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"35ec14e5-206b-41d5-b459-c5cbc9267242\">selector</label><div class=\"sk-toggleable__content\"><pre>['num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"154f8aab-5f9b-4ddb-b885-ae9da6cab9de\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"154f8aab-5f9b-4ddb-b885-ae9da6cab9de\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"24a25132-d333-42ca-b046-aa0f39b30035\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"24a25132-d333-42ca-b046-aa0f39b30035\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(max_iter=100000)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_3</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"99b3e3eb-a12e-4ab9-9914-f0fe12a47e2a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"99b3e3eb-a12e-4ab9-9914-f0fe12a47e2a\">colselector: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('selector', 'passthrough',\n",
              "                                 ['n_all_caps', 'perc_exclamations'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"8b08936f-d136-4215-8361-29cfc3d9731a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"8b08936f-d136-4215-8361-29cfc3d9731a\">selector</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps', 'perc_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"7d19acb6-cc47-4aca-9bbe-a1c5c22e5f4c\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"7d19acb6-cc47-4aca-9bbe-a1c5c22e5f4c\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"cf043c54-4644-4af1-b4fe-765d9b3edc6c\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"cf043c54-4644-4af1-b4fe-765d9b3edc6c\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(max_iter=100000)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_4</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"0cb70391-0284-44f4-ab1c-393d8faff3d2\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"0cb70391-0284-44f4-ab1c-393d8faff3d2\">colselector: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('selector', 'passthrough',\n",
              "                                 ['n_all_caps', 'num_exclamations'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"afc10565-85a3-42de-a508-3f9fe17b9d61\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"afc10565-85a3-42de-a508-3f9fe17b9d61\">selector</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps', 'num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"899c3d1b-d31b-459f-b889-18bbe04edc26\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"899c3d1b-d31b-459f-b889-18bbe04edc26\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"3f81a4d4-b45e-42db-beb1-495017b5788e\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"3f81a4d4-b45e-42db-beb1-495017b5788e\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(max_iter=100000)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_5</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e1a93c3e-ce01-45d5-9784-44c2a2691c11\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e1a93c3e-ce01-45d5-9784-44c2a2691c11\">colselector: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('selector', 'passthrough',\n",
              "                                 ['perc_exclamations', 'num_exclamations'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"098731dc-cfa8-45c0-862c-84381a47e799\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"098731dc-cfa8-45c0-862c-84381a47e799\">selector</label><div class=\"sk-toggleable__content\"><pre>['perc_exclamations', 'num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"d56c2986-69b6-445d-a2dd-533ab331d6c2\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"d56c2986-69b6-445d-a2dd-533ab331d6c2\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2e857f04-b83d-4eb9-ba1f-a79b8aff2359\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"2e857f04-b83d-4eb9-ba1f-a79b8aff2359\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(max_iter=100000)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"85f51aab-4051-4656-a605-5052aa5eeca4\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"85f51aab-4051-4656-a605-5052aa5eeca4\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('pipeline',\n",
              "                 StackingClassifier(estimators=[('classifier1',\n",
              "                                                 Pipeline(steps=[('column_transformer',\n",
              "                                                                  ColumnTransformer(transformers=[('countvectorizer',\n",
              "                                                                                                   CountVectorizer(),\n",
              "                                                                                                   'cleaned_comment_text'),\n",
              "                                                                                                  ('tfidfvectorizer-1',\n",
              "                                                                                                   TfidfVectorizer(),\n",
              "                                                                                                   'cleaned_comment_text'),\n",
              "                                                                                                  ('tfidfvectorizer-2',\n",
              "                                                                                                   TfidfVectorizer(use_idf=False),\n",
              "                                                                                                   'cleaned_comment_text')])),\n",
              "                                                                 ('d...\n",
              "                                                                  ColumnTransformer(transformers=[('selector',\n",
              "                                                                                                   'passthrough',\n",
              "                                                                                                   ['n_all_caps',\n",
              "                                                                                                    'num_exclamations'])])),\n",
              "                                                                 ('dummy',\n",
              "                                                                  LinearSVC(max_iter=100000))])),\n",
              "                                                ('classifier_5',\n",
              "                                                 Pipeline(steps=[('colselector',\n",
              "                                                                  ColumnTransformer(transformers=[('selector',\n",
              "                                                                                                   'passthrough',\n",
              "                                                                                                   ['perc_exclamations',\n",
              "                                                                                                    'num_exclamations'])])),\n",
              "                                                                 ('dummy',\n",
              "                                                                  LinearSVC(max_iter=100000))]))],\n",
              "                                    final_estimator=RandomForestClassifier()))])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EiRR4fd9sb7",
        "outputId": "62ca147b-0aa6-435b-968b-9dc44bd8affa"
      },
      "source": [
        "def print_df(df, metric, task):\n",
        "    df = df[(df['metric'] == metric) & (df['task_name'] == task)]\n",
        "    df = df.sort_values(by=['value'], ascending=False)\n",
        "    print(df.head(3))\n",
        "    return df\n",
        "\n",
        "for task_name in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "    print('=' * 80)\n",
        "    print(task_name)\n",
        "    print('-' * len(task_name))\n",
        "    print()\n",
        "    for metric in ['accuracy', 'f1_score']:\n",
        "        print_df(result_df, metric, task_name)\n",
        "        print()\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Sub1_Toxic\n",
            "----------\n",
            "\n",
            "              classifier   task_name    metric smote     value\n",
            "1  MultinomialNB_nosmote  Sub1_Toxic  accuracy     0  0.657727\n",
            "\n",
            "              classifier   task_name    metric smote     value\n",
            "2  MultinomialNB_nosmote  Sub1_Toxic  f1_score     0  0.412281\n",
            "\n",
            "================================================================================\n",
            "Sub2_Engaging\n",
            "-------------\n",
            "\n",
            "              classifier      task_name    metric smote     value\n",
            "3  MultinomialNB_nosmote  Sub2_Engaging  accuracy     0  0.756066\n",
            "\n",
            "              classifier      task_name    metric smote     value\n",
            "4  MultinomialNB_nosmote  Sub2_Engaging  f1_score     0  0.381877\n",
            "\n",
            "================================================================================\n",
            "Sub3_FactClaiming\n",
            "-----------------\n",
            "\n",
            "              classifier          task_name    metric smote     value\n",
            "5  MultinomialNB_nosmote  Sub3_FactClaiming  accuracy     0  0.729246\n",
            "\n",
            "              classifier          task_name    metric smote     value\n",
            "6  MultinomialNB_nosmote  Sub3_FactClaiming  f1_score     0  0.547009\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "id": "5mvNlGAA-eN9",
        "outputId": "e20aec4a-aa54-4c95-b1c2-201a76048db5"
      },
      "source": [
        "#result_df\n",
        "result_df[result_df['metric'] == 'f1_score'].sort_values(by='value', ascending=False)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classifier</th>\n",
              "      <th>task_name</th>\n",
              "      <th>metric</th>\n",
              "      <th>smote</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>MultinomialNB_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.589354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LinearSVC_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.583153</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.582441</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LinearSVC_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.551929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.54386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.525773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.521127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AdaBoost_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.514286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>MultinomialNB_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.502538</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.48318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>RandomForestClassifier_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.478723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.464419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.441767</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>MultinomialNB_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.430108</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LinearSVC_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.427966</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>AdaBoost_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.427632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.394805</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>BernoulliNB_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.394203</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.382075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>BernoulliNB_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.358779</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>AdaBoost_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.35468</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>RandomForestClassifier_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.35102</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>RandomForestClassifier_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.327778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>BernoulliNB_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.192308</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        classifier          task_name    metric smote     value\n",
              "30           MultinomialNB_nosmote  Sub3_FactClaiming  f1_score     0  0.589354\n",
              "6                LinearSVC_nosmote  Sub3_FactClaiming  f1_score     0  0.583153\n",
              "12                       LinearSVC  Sub3_FactClaiming  f1_score     1  0.582441\n",
              "4                LinearSVC_nosmote      Sub2_Engaging  f1_score     0  0.551929\n",
              "10                       LinearSVC      Sub2_Engaging  f1_score     1   0.54386\n",
              "36          RandomForestClassifier  Sub3_FactClaiming  f1_score     1  0.525773\n",
              "18                        AdaBoost  Sub3_FactClaiming  f1_score     1  0.521127\n",
              "24                AdaBoost_nosmote  Sub3_FactClaiming  f1_score     0  0.514286\n",
              "28           MultinomialNB_nosmote      Sub2_Engaging  f1_score     0  0.502538\n",
              "16                        AdaBoost      Sub2_Engaging  f1_score     1   0.48318\n",
              "42  RandomForestClassifier_nosmote  Sub3_FactClaiming  f1_score     0  0.478723\n",
              "34          RandomForestClassifier      Sub2_Engaging  f1_score     1  0.464419\n",
              "8                        LinearSVC         Sub1_Toxic  f1_score     1  0.441767\n",
              "26           MultinomialNB_nosmote         Sub1_Toxic  f1_score     0  0.430108\n",
              "2                LinearSVC_nosmote         Sub1_Toxic  f1_score     0  0.427966\n",
              "22                AdaBoost_nosmote      Sub2_Engaging  f1_score     0  0.427632\n",
              "32          RandomForestClassifier         Sub1_Toxic  f1_score     1  0.394805\n",
              "48             BernoulliNB_nosmote  Sub3_FactClaiming  f1_score     0  0.394203\n",
              "14                        AdaBoost         Sub1_Toxic  f1_score     1  0.382075\n",
              "46             BernoulliNB_nosmote      Sub2_Engaging  f1_score     0  0.358779\n",
              "20                AdaBoost_nosmote         Sub1_Toxic  f1_score     0   0.35468\n",
              "40  RandomForestClassifier_nosmote      Sub2_Engaging  f1_score     0   0.35102\n",
              "38  RandomForestClassifier_nosmote         Sub1_Toxic  f1_score     0  0.327778\n",
              "44             BernoulliNB_nosmote         Sub1_Toxic  f1_score     0  0.192308"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbcLIE7oC2ML"
      },
      "source": [
        "## BERT MODEL\n",
        "\n",
        "Use code from https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9TbWSQ0c7C_"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
        "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
        "\n",
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEzTyC_sru6w"
      },
      "source": [
        "## HUGGING FACE TRANSFORMERS : GERMAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zul78K-dKK3h",
        "outputId": "f4967889-3c28-4d08-fa98-708cd2f854c4"
      },
      "source": [
        "from transformers import AutoTokenizer,TFAutoModel\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, TFAutoModelForMaskedLM\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import IPython\n",
        "\n",
        "SEQUENCE_LENGTH = 512\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
        "\n",
        "def tokenize_input_with_bert(bert_tokenizer, input:str):\n",
        "    tokens = bert_tokenizer.encode_plus(\n",
        "                    input,\n",
        "                    max_length=SEQUENCE_LENGTH,\n",
        "                    truncation=True,\n",
        "                    padding='max_length',\n",
        "                    add_special_tokens=True,\n",
        "                    return_attention_mask=True,\n",
        "                    return_token_type_ids=False,\n",
        "                    return_tensors='np'\n",
        "                )\n",
        "    return tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "def tokenize_input_with_bert(bert_tokenizer, input:str, seq_len:int):\n",
        "    def pad(t):\n",
        "        try:\n",
        "            t = t.reshape((t.shape[1],))\n",
        "        except:\n",
        "            pass\n",
        "        t = t[:seq_len]\n",
        "        pad_len = max(0, seq_len - t.shape[0])\n",
        "        t = np.pad(t, (0, pad_len))\n",
        "        t = t.reshape((1, t.shape[0]))\n",
        "        return t\n",
        "\n",
        "    tokens = bert_tokenizer(\n",
        "                    input,\n",
        "                    truncation=True,\n",
        "                    add_special_tokens=True,\n",
        "                    return_attention_mask=True,\n",
        "                    return_token_type_ids=False,\n",
        "                    return_tensors='np')\n",
        "\n",
        "    return pad(tokens['input_ids']), pad(tokens['attention_mask'])\n",
        "\n",
        "def tokenize_dataframe_of_text(df, seq_len:int):\n",
        "    x_ids = np.zeros((0, seq_len))\n",
        "    x_mask = np.zeros((0, seq_len))\n",
        "\n",
        "    for i, text in enumerate(df):\n",
        "        id, mask = tokenize_input_with_bert(bert_tokenizer, text, seq_len)\n",
        "        x_ids = np.append(x_ids, id, axis=0)\n",
        "        x_mask = np.append(x_mask, mask, axis=0)\n",
        "\n",
        "    return x_ids, x_mask\n",
        "\n",
        "def build_model(seq_len):\n",
        "    \n",
        "    input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
        "    mask = tf.keras.layers.Input(shape=(seq_len,), name='attn_mask', dtype='int32')\n",
        "\n",
        "    bert_model = TFAutoModelForMaskedLM.from_pretrained(\"bert-base-german-cased\")\n",
        "\n",
        "    X = bert_model(input_ids, attention_mask=mask)\n",
        "    X = X[0]\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
        "    X = tf.keras.layers.Dropout(0.1)(X)\n",
        "    X = tf.keras.layers.Flatten()(X)\n",
        "    y = tf.keras.layers.Dense(1, activation='softmax', name='outputs')(X)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "\n",
        "    bert_model.trainable = False\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def fit_model(model, X, y):\n",
        "    X_ids, X_masks = tokenize_dataframe_of_text(X, SEQUENCE_LENGTH)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X_ids, X_masks, y))\n",
        "\n",
        "    def map_fn(input_ids, masks, labels):\n",
        "        return {'input_ids': input_ids, 'attn_mask': masks}, labels\n",
        "    ds = ds.map(map_fn)\n",
        "\n",
        "    ds = ds.shuffle(10000).batch(32)\n",
        "\n",
        "    train_ds = ds.take((len(ds) * 8) // 10)\n",
        "    val_ds = ds.skip((len(ds) * 8) // 10)\n",
        "\n",
        "    model = build_model(SEQUENCE_LENGTH)\n",
        "\n",
        "    history = model.fit(train_ds, validation_data=val_ds, epochs=20)\n",
        "\n",
        "    return history\n",
        "\n",
        "def create_model():\n",
        "    return build_model(SEQUENCE_LENGTH)\n",
        "\n",
        "model = create_model()\n",
        "train_df, test_df = get_clean_train_test_df()\n",
        "history = fit_model(model, train_df['comment_text'], train_df['Sub3_FactClaiming'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-german-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-german-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "80/80 [==============================] - 918s 11s/step - loss: 11.7735 - accuracy: 0.3371 - val_loss: 10.7914 - val_accuracy: 0.3549\n",
            "Epoch 2/20\n",
            "80/80 [==============================] - 916s 11s/step - loss: 16.1275 - accuracy: 0.3484 - val_loss: 9.1900 - val_accuracy: 0.3644\n",
            "Epoch 3/20\n",
            "80/80 [==============================] - 905s 11s/step - loss: 9.7318 - accuracy: 0.3391 - val_loss: 12.2273 - val_accuracy: 0.3628\n",
            "Epoch 4/20\n",
            "80/80 [==============================] - 902s 11s/step - loss: 7.8757 - accuracy: 0.3406 - val_loss: 5.4716 - val_accuracy: 0.3549\n",
            "Epoch 5/20\n",
            "80/80 [==============================] - 902s 11s/step - loss: 3.0658 - accuracy: 0.3410 - val_loss: 1.6754 - val_accuracy: 0.3580\n",
            "Epoch 6/20\n",
            "80/80 [==============================] - 901s 11s/step - loss: 2.2503 - accuracy: 0.3457 - val_loss: 0.6449 - val_accuracy: 0.3502\n",
            "Epoch 7/20\n",
            "80/80 [==============================] - 894s 11s/step - loss: 1.0822 - accuracy: 0.3379 - val_loss: 0.5585 - val_accuracy: 0.3281\n",
            "Epoch 8/20\n",
            "80/80 [==============================] - 904s 11s/step - loss: 0.9531 - accuracy: 0.3383 - val_loss: 0.6451 - val_accuracy: 0.3438\n",
            "Epoch 9/20\n",
            "80/80 [==============================] - 897s 11s/step - loss: 0.7708 - accuracy: 0.3375 - val_loss: 0.2565 - val_accuracy: 0.3265\n",
            "Epoch 10/20\n",
            "80/80 [==============================] - 901s 11s/step - loss: 0.7691 - accuracy: 0.3352 - val_loss: 0.2079 - val_accuracy: 0.3565\n",
            "Epoch 11/20\n",
            "80/80 [==============================] - 902s 11s/step - loss: 0.7401 - accuracy: 0.3434 - val_loss: 0.2741 - val_accuracy: 0.3407\n",
            "Epoch 12/20\n",
            "80/80 [==============================] - 899s 11s/step - loss: 0.6107 - accuracy: 0.3449 - val_loss: 0.2575 - val_accuracy: 0.3580\n",
            "Epoch 13/20\n",
            "80/80 [==============================] - 895s 11s/step - loss: 0.5235 - accuracy: 0.3355 - val_loss: 0.2401 - val_accuracy: 0.3281\n",
            "Epoch 14/20\n",
            "80/80 [==============================] - 890s 11s/step - loss: 0.7208 - accuracy: 0.3371 - val_loss: 0.2019 - val_accuracy: 0.3344\n",
            "Epoch 15/20\n",
            "80/80 [==============================] - 898s 11s/step - loss: 0.5936 - accuracy: 0.3406 - val_loss: 0.2444 - val_accuracy: 0.3596\n",
            "Epoch 16/20\n",
            "80/80 [==============================] - 891s 11s/step - loss: 0.7424 - accuracy: 0.3398 - val_loss: 0.3053 - val_accuracy: 0.3628\n",
            "Epoch 17/20\n",
            "80/80 [==============================] - 892s 11s/step - loss: 0.8034 - accuracy: 0.3387 - val_loss: 0.2235 - val_accuracy: 0.3423\n",
            "Epoch 18/20\n",
            "80/80 [==============================] - 907s 11s/step - loss: 0.6434 - accuracy: 0.3387 - val_loss: 0.2895 - val_accuracy: 0.3628\n",
            "Epoch 19/20\n",
            "12/80 [===>..........................] - ETA: 10:34 - loss: 0.6248 - accuracy: 0.3646"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS8NgaRs-k8O"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(history.history)\n",
        "plt.plot(history.history['loss'], label='loss', color='b')\n",
        "plt.plot(history.history['val_loss'], label='val_loss', color='r')\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='acc', color='g')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc', color='cyan')\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_OUi7BXtbzo"
      },
      "source": [
        "import IPython\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjV2E3i_0f3i"
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGhWSvlj2Ywc"
      },
      "source": [
        "def build_model(seq_len):\n",
        "    \n",
        "    input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
        "    mask = tf.keras.layers.Input(shape=(seq_len,), name='attn_mask', dtype='int32')\n",
        "\n",
        "    bert_model = TFAutoModelForMaskedLM.from_pretrained(\"bert-base-german-cased\")\n",
        "\n",
        "    X = bert_model(input_ids, attention_mask=mask)\n",
        "    X = X[0]\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "\n",
        "\n",
        "    X = tf.keras.layers.Conv1D(64, (4,), padding='same',  activation='relu')(X)\n",
        "    X = tf.keras.layers.MaxPooling1D(2,)(X)\n",
        "    \n",
        "    X = tf.keras.layers.Conv1D(128, (4,), padding='same',  activation='relu')(X)\n",
        "    X = tf.keras.layers.MaxPooling1D(2,)(X)\n",
        "\n",
        "    X = tf.keras.layers.Conv1D(256, (4,), padding='same',  activation='relu')(X)\n",
        "    X = tf.keras.layers.MaxPooling1D(2,)(X)\n",
        "\n",
        "    X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
        "    X = tf.keras.layers.Dropout(0.1)(X)\n",
        "    X = tf.keras.layers.Flatten()(X)\n",
        "    X = tf.keras.layers.Dense(512, activation='relu')(X)\n",
        "    X = tf.keras.layers.Dense(64, activation='relu')(X)\n",
        "    X = tf.keras.layers.Dense(8, activation='relu')(X)\n",
        "    y = tf.keras.layers.Dense(1, activation='softmax', name='outputs')(X)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "\n",
        "    bert_model.trainable = False\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    return build_model(SEQUENCE_LENGTH)\n",
        "\n",
        "\n",
        "m1 = create_model()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd4lPVS60r2-"
      },
      "source": [
        "\n",
        "tf.keras.utils.plot_model(m1, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlYEb8mO4Zrv"
      },
      "source": [
        "print(m1.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbANX9ah4yGv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}