{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOFITZHUhqDPR8ig/0yq6Cm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment1/NLP_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-DAX9VLZlw3"
      },
      "source": [
        "import requests\n",
        "def get_train_test_files():\n",
        "    TRAIN_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Test_For_Evaluation.csv'\n",
        "    TRAIN_FILE_LOCAL = 'Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE_LOCAL = 'Assessment1_Toxic_Test.csv'\n",
        "\n",
        "    def download(url, localfile):\n",
        "        with open(localfile, 'wb') as f:\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            f.write(r.content)\n",
        "\n",
        "    download(TRAIN_FILE, TRAIN_FILE_LOCAL)\n",
        "    download(TEST_FILE, TEST_FILE_LOCAL)\n",
        "\n",
        "    return TRAIN_FILE_LOCAL, TEST_FILE_LOCAL\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERvBOEn0wss6"
      },
      "source": [
        "!pip install spacy  nltk spacymoji huggingface -q       >/dev/null 2>&1         \n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "!pip install -q -U tensorflow-text                      >/dev/null 2>&1\n",
        "!pip install -q tf-models-official                      >/dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v6ohiY0s5N1"
      },
      "source": [
        "import pandas as pd\n",
        "def get_train_test_df():\n",
        "    train_csv, test_csv = get_train_test_files()\n",
        "\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "\n",
        "    return train_df, test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVfYulLnuGVJ"
      },
      "source": [
        "import re\n",
        "def remove_roles(line:str)->str:\n",
        "    # Remove texts like @USER, @MODERATOR etc\n",
        "    pat = re.compile(u'\\@[A-Za-z]+')\n",
        "    return re.sub(pat, '', line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nU-BO8guUcM"
      },
      "source": [
        "import re\n",
        "def remove_emojis(line:str)->str:\n",
        "    pat = re.compile(\"[\"\n",
        "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                u\"\\U00002702-\\U000027B0\"\n",
        "                u\"\\U000024C2-\\U0001F251\"\n",
        "                u\"\\U0001f926-\\U0001f937\"\n",
        "                u'\\U00010000-\\U0010ffff'\n",
        "                u\"\\u200d\"\n",
        "                u\"\\u2640-\\u2642\"\n",
        "                u\"\\u2600-\\u2B55\"\n",
        "                u\"\\u23cf\"\n",
        "                u\"\\u23e9\"\n",
        "                u\"\\u231a\"\n",
        "                u\"\\u3030\"\n",
        "                u\"\\ufe0f\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return re.sub(pat, '', line)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Hi-K1gSDZW"
      },
      "source": [
        "import re\n",
        "def remove_ellipses(line:str)->str:\n",
        "    pat = re.compile(u'\\.\\.+')\n",
        "    return re.sub(pat, ' ', line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6JFm16r7M3E"
      },
      "source": [
        "def to_lower(line:str)->str:\n",
        "    return line.lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSDv8R1lbfFP"
      },
      "source": [
        "def replace_number_with_tag(line:str)->str:\n",
        "    line = re.sub(\"\\s\\d*((\\.|\\,)\\d+)?\\s\", \" nummer \", line)\n",
        "    line = re.sub('\\s\\d+$', '', line)\n",
        "    line = re.sub('^\\d+\\s', '', line)\n",
        "    return line"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgSXlzZ4fAkF"
      },
      "source": [
        "def remove_urls(line:str)->str:\n",
        "    return re.sub('https?:\\/\\/\\S+', ' hyperlink ', line)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_TxMNrC7doY"
      },
      "source": [
        "def basic_clean(s:pd.Series)->pd.Series:\n",
        "    return s.map(to_lower)                                                  \\\n",
        "            .map(remove_emojis)                                             \\\n",
        "            .map(remove_roles)                                              \\\n",
        "            .map(remove_ellipses)                                           \\\n",
        "            .map(replace_number_with_tag)                                   \\\n",
        "            .map(remove_urls)\n",
        "\n",
        "def get_clean_train_test_df()->tuple:\n",
        "    train_df, test_df = get_train_test_df()\n",
        "    train_df['comment_text'] = basic_clean(train_df['comment_text'])\n",
        "    test_df['comment_text'] = basic_clean(test_df['comment_text'])\n",
        "    return train_df, test_df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "s8YSZZ4vxTk5",
        "outputId": "13743c6b-ab73-49d1-a439-7e9d029e6f27"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string\n",
        "import spacy\n",
        "from spacymoji import Emoji\n",
        "\n",
        "\n",
        "def is_punct_only(token:str)->bool:\n",
        "    for c in list(token):\n",
        "        if c not in string.punctuation:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_same(l1:list, l2:list)->bool:\n",
        "    if (len(l1) != len(l2)):\n",
        "        return False\n",
        "    for x, y in zip(l1, l2):\n",
        "        if x != y:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def do_basic_nlp_cleaning(line:str)->str:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(line)\n",
        "\n",
        "    # Some tokens start with a punctuation, remove the first one\n",
        "    def remove_first_punctuation(tok:str)->str:\n",
        "        return                                                              \\\n",
        "            tok[1:]                                                         \\\n",
        "            if tok[0] in set(string.punctuation) and len(tok) != 0          \\\n",
        "            else tok\n",
        "\n",
        "    tokens = [remove_first_punctuation(w) for w in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"german\"))\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [w for w in tokens if not is_punct_only(w)]\n",
        "\n",
        "    # Stem words\n",
        "    stem = SnowballStemmer('german')\n",
        "    tokens = [stem.stem(w) for w in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "import spacy\n",
        "def get_cleaning_function():\n",
        "    #nlp = spacy.load(\"de_dep_news_trf\")\n",
        "    nlp = spacy.load(\"de_core_news_sm\")\n",
        "    emoji = Emoji(nlp)\n",
        "    nlp.add_pipe(emoji, first=True)\n",
        "    stopwords = spacy.lang.de.stop_words.STOP_WORDS\n",
        "\n",
        "    def do_basic_nlp_cleaning(line:str)->str:\n",
        "        def is_interesting_token(token):\n",
        "            if token.pos_ in set(['NUM', 'SYM']):\n",
        "                return False\n",
        "            if token.text in stopwords:\n",
        "                return False\n",
        "            if (token.is_punct):\n",
        "                return False\n",
        "            #if token._.is_emoji:\n",
        "            #    return False\n",
        "            return True\n",
        "\n",
        "        def remove_terminal_punctuations(word):\n",
        "            word = word.strip()\n",
        "            while word != \"\" and word[0] in list(string.punctuation):\n",
        "                word = word[1:]\n",
        "            while word != \"\" and word[-1] in list(string.punctuation):\n",
        "                word = word[:-1]\n",
        "            return word\n",
        "\n",
        "        doc = nlp(line)\n",
        "        words = [tok.lemma_.lower() for tok in doc if is_interesting_token(tok)]\n",
        "        words = [remove_terminal_punctuations(word) for word in words]\n",
        "        words = [word for word in words if word != \"\"]\n",
        "        return  \" \".join(words)\n",
        "\n",
        "    return do_basic_nlp_cleaning\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def is_empty_string(s:str)->bool:\n",
        "    if s == '' or s == None:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "cleaning_fn = get_cleaning_function()\n",
        "train_df, test_df = get_clean_train_test_df()\n",
        "train_df['comment_text'] = train_df['comment_text'].map(cleaning_fn)\n",
        "empty_rows = train_df['comment_text'].map(is_empty_string)\n",
        "train_df = train_df[~ empty_rows]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ee4491cbeb75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m \u001b[0mcleaning_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_cleaning_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_clean_train_test_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'comment_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaning_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-ee4491cbeb75>\u001b[0m in \u001b[0;36mget_cleaning_function\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"de_core_news_sm\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0memoji\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEmoji\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memoji\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mstopwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSTOP_WORDS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0mbad_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE966\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbad_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 769\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    770\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfactory_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E966] `nlp.add_pipe` now takes the string name of the registered component factory, not a callable component. Expected string, but got <spacymoji.Emoji object at 0x7fb1cce4f690> (name: 'None').\n\n- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and call `nlp.add_pipe('name')` instead.\n\n- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string name instead, e.g. `nlp.add_pipe('textcat')`.\n\n- If you're using a custom component: Add the decorator `@Language.component` (for function components) or `@Language.factory` (for class components / factories) to your custom component and assign it a name, e.g. `@Language.component('your_name')`. You can then run `nlp.add_pipe('your_name')` to add it to the pipeline."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZotXwWTZm7CB"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "def get_initial_feel(model, vectorizer, y_column:str):\n",
        "    print('*' * 80)\n",
        "    print(f\"Classifying for : {y_column}\")\n",
        "    \n",
        "    trainX, testX, trainY, testY = train_test_split(\\\n",
        "                        train_df['comment_text'], train_df[y_column])\n",
        "\n",
        "    cv =  vectorizer.fit(trainX)\n",
        "    trainX = cv.transform(trainX)\n",
        "    testX = cv.transform(testX)\n",
        "\n",
        "    model.fit(trainX, trainY)\n",
        "\n",
        "    predY = model.predict(testX)\n",
        "\n",
        "    print(confusion_matrix(testY, predY))\n",
        "    print(f\"Accuracy: {accuracy_score(testY, predY)}\")\n",
        "    print(f\"f1-score: {f1_score(testY, predY)}\")\n",
        "\n",
        "def tryout(model, vectorizer):\n",
        "    print(repr(model), repr(vectorizer))\n",
        "    print()\n",
        "    get_initial_feel(model, vectorizer, 'Sub1_Toxic')\n",
        "    get_initial_feel(model, vectorizer, 'Sub2_Engaging')\n",
        "    get_initial_feel(model, vectorizer, 'Sub3_FactClaiming')\n",
        "    print()\n",
        "\n",
        "\n",
        "tryout(MultinomialNB(), CountVectorizer(ngram_range=(1, 3)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbcLIE7oC2ML"
      },
      "source": [
        "## BERT MODEL\n",
        "\n",
        "Use code from https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9TbWSQ0c7C_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77e9250-a17f-4a7a-d399-8ae1c4ec9ef3"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
        "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
        "\n",
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "text_test = [\"\"\"this is such an amazing movie! evaluationScriptGermeval2018.pl\n",
        "\n",
        "represents the evaluation tool. It can be used to evaluated the predictions for both tasks of the shared task.\n",
        "\n",
        "In order to obtain further information on the usage of that tool please type:\n",
        "As discussed in Section 2, our pretrained language models will learn pre-existing biases from the\n",
        "training datasets. The main portion (89%) of our training data, namely the OSCAR dataset, uses texts\n",
        "scraped from the internet, which is in some respects problematic. First off, this dataset contains a lot of\n",
        "explicit and indecent material. While we filtered out many of these documents through keyword match-\n",
        "ing, we cannot guarantee that this method was successful in every case. Furthermore, many websites\n",
        "contain unverified information and any dataset containing this kind of text can lead to a skewed model\n",
        "that reflects commonly found lies and misconceptions. This includes gender, racial and religious biases\n",
        "which are found in textual data of all registers and so we advise that anyone using our model to recognise\n",
        "that it will not always build true and accurate representation of real world concepts. We implore users\n",
        "of the model to seriously consider these issues before deploying it in a production setting, especially\n",
        "in situations where impartiality matter, such as journalism, and institutional decision making like job\n",
        "\n",
        "The underground cities were well designed for protection against attacks. The few entrances were hidden by foliage and not easily spotted from outside. Inside, they took the form of a labyrinth of passageways which were unnavigable for outsiders, and could be sealed with large rock doors, around a metre high and shaped like mill-stones. These doors were built such that they could be rolled into a closed position relatively easily, but could not be moved from the outside. They had a hole in the centre which was probably used as a kind of peephole. In some cities there were holes in ceiling above, through which the enemy could be attacked with spears.[6] The cities descended up to twelve stories – over 100 metres – under the ground and had everything necessary for a long siege. The upper stories were largely used as stables and storerooms, with a constant temperature of around 10 °C. In the walls of the caverns there were receptacles for various kinds of food, as well as hollows for vessels in which liquids could be stored. Further down, were the living and working spaces, where furniture, including seats, tables, and beds were carved out of the rock. Working spaces include a wine press at Derinkuyu, a copper foundry in Kaymakli, as well as cisterns and wells which ensured a supply of drinking water during a long siege.[9] There were also prisons and toilets.\n",
        "applications or insurance assessments\n",
        "\"\"\"]\n",
        "text_preprocessed = bert_preprocess_model(text_test)\n",
        "\n",
        "#print(f'Keys       : {list(text_preprocessed.keys())}')\n",
        "#print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
        "#print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
        "#print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
        "#print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')\n",
        "\n",
        "\n",
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)\n",
        "bert_results = bert_model(text_preprocessed)\n",
        "\n",
        "#print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
        "print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
        "print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
        "print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
        "print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pooled Outputs Shape:(1, 512)\n",
            "Pooled Outputs Values:[ 0.9107527   0.4435962  -0.0445334   0.07857981  0.13065211  0.95631254\n",
            "  0.9006179  -0.79465264 -0.56049114 -0.94223607 -0.3001277  -0.973857  ]\n",
            "Sequence Outputs Shape:(1, 128, 512)\n",
            "Sequence Outputs Values:[[ 0.01486441  0.7203154   0.14484876 ... -0.8022307   0.06363815\n",
            "   0.95406747]\n",
            " [-0.65695393  0.08590551  0.15920955 ...  0.64430267 -0.19999222\n",
            "   0.7745597 ]\n",
            " [-0.81394494  0.74874014 -0.43530738 ...  0.07674243 -0.4267268\n",
            "   0.6159239 ]\n",
            " ...\n",
            " [-0.65857285  1.4859799   0.7598989  ... -0.02287899  0.23170051\n",
            "   0.85067797]\n",
            " [-0.77028626  1.3962332   0.19560781 ... -0.1806682   0.76875293\n",
            "   1.0642036 ]\n",
            " [ 0.44916624  0.7581736   0.42707348 ... -0.8116752   1.2785095\n",
            "   1.1305085 ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-qI8qAcpDYo3",
        "outputId": "f7065bdb-e8e5-486a-968f-ef826a1a2550"
      },
      "source": [
        "print(bert_results.keys())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['pooled_output', 'default', 'encoder_outputs', 'sequence_output'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt-7fa8Ip0TM",
        "outputId": "db24aad6-9d47-4072-cc69-55f4523da4b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(text_preprocessed['input_mask'])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]], shape=(1, 128), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WV64NvetqfkS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}