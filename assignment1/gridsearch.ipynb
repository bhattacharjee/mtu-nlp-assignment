{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOBEtQUyszCCy7fn/c10ugh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment1/gridsearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERvBOEn0wss6"
      },
      "source": [
        "!pip install spacy  nltk spacymoji huggingface -q       >/dev/null 2>&1         \n",
        "!pip install -q -U tensorflow-text                      >/dev/null 2>&1\n",
        "!pip install -q tf-models-official                      >/dev/null 2>&1\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "!pip install transformers                               >/dev/null 2>&1\n",
        "\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "\n",
        "!pip install mlxtend                                    >/dev/null 2>&1\n",
        "!pip install imblearn                                   >/dev/null 2>&1\n",
        "\n",
        "# handling emojis\n",
        "!pip install demoji                                     >/dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-DAX9VLZlw3"
      },
      "source": [
        "import requests\n",
        "def get_train_test_files():\n",
        "    TRAIN_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Test_For_Evaluation.csv'\n",
        "    TRAIN_FILE_LOCAL = 'Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE_LOCAL = 'Assessment1_Toxic_Test.csv'\n",
        "\n",
        "    def download(url, localfile):\n",
        "        with open(localfile, 'wb') as f:\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            f.write(r.content)\n",
        "\n",
        "    download(TRAIN_FILE, TRAIN_FILE_LOCAL)\n",
        "    download(TEST_FILE, TEST_FILE_LOCAL)\n",
        "\n",
        "    return TRAIN_FILE_LOCAL, TEST_FILE_LOCAL\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrlJ1H1maDxM"
      },
      "source": [
        "# Functions to read the CSV and do basic cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVfYulLnuGVJ"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import demoji\n",
        "\n",
        "def remove_roles(line:str)->str:\n",
        "    # Remove texts like @USER, @MODERATOR etc\n",
        "    pat = re.compile(u'\\@[A-Za-z]+')\n",
        "    return re.sub(pat, '', line)\n",
        "\n",
        "def get_train_test_df():\n",
        "    train_csv, test_csv = get_train_test_files()\n",
        "\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "def remove_emojis(line:str)->str:\n",
        "    pat = re.compile(\"[\"\n",
        "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                u\"\\U00002702-\\U000027B0\"\n",
        "                u\"\\U000024C2-\\U0001F251\"\n",
        "                u\"\\U0001f926-\\U0001f937\"\n",
        "                u'\\U00010000-\\U0010ffff'\n",
        "                u\"\\u200d\"\n",
        "                u\"\\u2640-\\u2642\"\n",
        "                u\"\\u2600-\\u2B55\"\n",
        "                u\"\\u23cf\"\n",
        "                u\"\\u23e9\"\n",
        "                u\"\\u231a\"\n",
        "                u\"\\u3030\"\n",
        "                u\"\\ufe0f\"\n",
        "        \"]+\", flags=re.UNICODE)\n",
        "    return re.sub(pat, '', line)\n",
        "\n",
        "def remove_emojis(line:str)->str:\n",
        "    # Replace emojis with their description, eg __thumbs_down__\n",
        "    demoji_str = demoji.replace_with_desc(line, sep=\" ::: \")\n",
        "    if (demoji_str == line):\n",
        "        return line\n",
        "    \n",
        "    inEmoji = False\n",
        "    currentEmojiWords = []\n",
        "    allWords = []\n",
        "\n",
        "    def accumulate(word:str)->None:\n",
        "        nonlocal inEmoji\n",
        "        nonlocal currentEmojiWords\n",
        "        nonlocal allWords\n",
        "        if not inEmoji and word != \":::\":\n",
        "            allWords.append(word)\n",
        "        elif inEmoji:\n",
        "            if word == ':::':\n",
        "                currentEmoji = \"_\".join(currentEmojiWords)\n",
        "                currentEmoji = \"__\" + currentEmoji + \"__\"\n",
        "                allWords.append(currentEmoji)\n",
        "                currentEmojiWords = []\n",
        "            else:\n",
        "                currentEmojiWords.append(word)\n",
        "        else: # Not in emoji but ::: is true\n",
        "            inEmoji = True\n",
        "\n",
        "    [accumulate(word) for word in demoji_str.split()]\n",
        "\n",
        "    sentence = \" \".join(allWords)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def remove_ellipses(line:str)->str:\n",
        "    pat = re.compile(u'\\.\\.+')\n",
        "    return re.sub(pat, ' ', line)\n",
        "\n",
        "def to_lower(line:str)->str:\n",
        "    return line.lower()\n",
        "\n",
        "def replace_number_with_tag(line:str)->str:\n",
        "    line = re.sub(\"\\s\\d*((\\.|\\,)\\d+)?\\s\", \" nummer \", line)\n",
        "    line = re.sub('\\s\\d+$', '', line)\n",
        "    line = re.sub('^\\d+\\s', '', line)\n",
        "    return line\n",
        "\n",
        "def remove_urls(line:str)->str:\n",
        "    return re.sub('https?:\\/\\/\\S+', ' hyperlink ', line)\n",
        "\n",
        "def basic_clean(s:pd.Series)->pd.Series:\n",
        "    return s.map(to_lower)                                                  \\\n",
        "            .map(remove_emojis)                                             \\\n",
        "            .map(remove_roles)                                              \\\n",
        "            .map(remove_ellipses)                                           \\\n",
        "            .map(replace_number_with_tag)                                   \\\n",
        "            .map(remove_urls)\n",
        "\n",
        "def get_clean_train_test_df()->tuple:\n",
        "    train_df, test_df = get_train_test_df()\n",
        "    train_df['comment_text'] = basic_clean(train_df['comment_text'])\n",
        "    test_df['comment_text'] = basic_clean(test_df['comment_text'])\n",
        "    return train_df, test_df\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6_dto2JaR2f"
      },
      "source": [
        "# Clean using Spacy and Enrich"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8YSZZ4vxTk5"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string\n",
        "import spacy\n",
        "from spacymoji import Emoji\n",
        "import  de_core_news_sm\n",
        "\n",
        "def is_punct_only(token:str)->bool:\n",
        "    for c in list(token):\n",
        "        if c not in string.punctuation:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_same(l1:list, l2:list)->bool:\n",
        "    if (len(l1) != len(l2)):\n",
        "        return False\n",
        "    for x, y in zip(l1, l2):\n",
        "        if x != y:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def get_num_of_allcap_words(s:str)->int:\n",
        "    def is_allcaps(s:str)->bool:\n",
        "        if (len(s) < 3):\n",
        "            return False\n",
        "        for c in list(s):\n",
        "            if not (\\\n",
        "                    (ord(c) <=ord('Z') and ord(c) >= ord('A')) or           \\\n",
        "                    (ord(c) >= ord('0') and ord(c) <= ord('9'))             \\\n",
        "                    ):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    if len(s) < 3:\n",
        "        return 0\n",
        "    tokens = [w.strip() for w in s.split()]\n",
        "    return sum([1 for t in tokens if is_allcaps(t)])\n",
        "\n",
        "def get_percentage_of_excalamations(s:str)->float:\n",
        "    if len(s) == 0:\n",
        "        return 0.0\n",
        "    exclamation_count = sum([1 for c in list(s) if c == '!'])\n",
        "    return exclamation_count / len(s)\n",
        "\n",
        "\n",
        "def is_empty_string(s:str)->bool:\n",
        "    if s == '' or s == None:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def do_basic_nlp_cleaning(line:str)->str:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(line)\n",
        "\n",
        "    # Some tokens start with a punctuation, remove the first one\n",
        "    def remove_first_punctuation(tok:str)->str:\n",
        "        return                                                              \\\n",
        "            tok[1:]                                                         \\\n",
        "            if tok[0] in set(string.punctuation) and len(tok) != 0          \\\n",
        "            else tok\n",
        "\n",
        "    tokens = [remove_first_punctuation(w) for w in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"german\"))\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [w for w in tokens if not is_punct_only(w)]\n",
        "\n",
        "    # Stem words\n",
        "    stem = SnowballStemmer('german')\n",
        "    tokens = [stem.stem(w) for w in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def get_cleaning_function():\n",
        "    #nlp = spacy.load(\"de_dep_news_trf\")\n",
        "    #nlp = spacy.load(\"de_core_news_sm\")\n",
        "    nlp = de_core_news_sm.load()\n",
        "    emoji = Emoji(nlp)\n",
        "    nlp.add_pipe(emoji, first=True)\n",
        "    stopwords = spacy.lang.de.stop_words.STOP_WORDS\n",
        "\n",
        "    def do_basic_nlp_cleaning(line:str)->str:\n",
        "        def is_interesting_token(token):\n",
        "            if token.pos_ in set(['NUM', 'SYM']):\n",
        "                return False\n",
        "            if token.text in stopwords:\n",
        "                return False\n",
        "            if (token.is_punct):\n",
        "                return False\n",
        "            #if token._.is_emoji:\n",
        "            #    return False\n",
        "            return True\n",
        "\n",
        "        def remove_terminal_punctuations(word):\n",
        "            word = word.strip()\n",
        "            while word != \"\" and word[0] in list(string.punctuation):\n",
        "                word = word[1:]\n",
        "            while word != \"\" and word[-1] in list(string.punctuation):\n",
        "                word = word[:-1]\n",
        "            return word\n",
        "\n",
        "        doc = nlp(line)\n",
        "        words = [tok.lemma_.lower() for tok in doc if is_interesting_token(tok)]\n",
        "        words = [remove_terminal_punctuations(word) for word in words]\n",
        "        words = [word for word in words if word != \"\"]\n",
        "        return  \" \".join(words)\n",
        "\n",
        "    return do_basic_nlp_cleaning\n",
        "\n",
        "def get_enriched_dataset(df):\n",
        "    df['cleaned_comment_text'] = df['comment_text'].map(cleaning_fn)\n",
        "    df['n_all_caps'] = df['comment_text'].map(get_num_of_allcap_words)\n",
        "    df['perc_exclamations'] = df['comment_text'].map(get_percentage_of_excalamations)\n",
        "    df['num_exclamations'] = df['comment_text'].map(lambda s: sum([1 for x in list(s) if x == '!']))\n",
        "    return df\n",
        "\n",
        "cleaning_fn = get_cleaning_function()\n",
        "train_df, test_df = get_clean_train_test_df()\n",
        "train_df = get_enriched_dataset(train_df)\n",
        "test_df = get_enriched_dataset(test_df)\n",
        "empty_rows = train_df['cleaned_comment_text'].map(is_empty_string)\n",
        "train_df = train_df[~ empty_rows]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWTtxHw0aa4N"
      },
      "source": [
        "# Print Enriched Training DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "n3q-TKbwZRBJ",
        "outputId": "4b979eb4-de57-4b28-ccc2-f45960a5161a"
      },
      "source": [
        "train_df"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>Sub1_Toxic</th>\n",
              "      <th>Sub2_Engaging</th>\n",
              "      <th>Sub3_FactClaiming</th>\n",
              "      <th>cleaned_comment_text</th>\n",
              "      <th>n_all_caps</th>\n",
              "      <th>perc_exclamations</th>\n",
              "      <th>num_exclamations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gestern bei illner, montag bei nummer  ist das...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>gestern illner montag nummer langsam ör-partei...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>mein gott der war erst gestern bei illner. die...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>gott gestern illner redaktionen versagen</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>die cdu lässt das so wie so nicht zu . sagen ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>cdu lässt sagen reich bekommen nummer milliard...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bei meiner beschissenen rente als 2x geschiede...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>beschissen rente geschieden mann steuern krank...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>wer nummer jahre zum mindestlohn arbeiten muß,...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>nummer mindestlohn arbeiten erhalten € rente n...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.005025</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3189</th>\n",
              "      <td>hier mal eine info. flüchtlinge werden nummer ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>mal info flüchtlinge nummer km küste schlauchb...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3190</th>\n",
              "      <td>.aha .mal abwarten kommt bei uns auch .firmen ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>aha mal abwarten firmen entlassen mitarbeiter ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3191</th>\n",
              "      <td>.so ist es</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>so</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3192</th>\n",
              "      <td>.die warten da</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>die warten</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3193</th>\n",
              "      <td>.das bekommen die gesagt wie sich verhalten s...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>das bekommen verhalten kameras richten tut ers...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3149 rows × 8 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           comment_text  ...  num_exclamations\n",
              "0     gestern bei illner, montag bei nummer  ist das...  ...                 0\n",
              "1     mein gott der war erst gestern bei illner. die...  ...                 0\n",
              "2      die cdu lässt das so wie so nicht zu . sagen ...  ...                 0\n",
              "3     bei meiner beschissenen rente als 2x geschiede...  ...                 0\n",
              "4     wer nummer jahre zum mindestlohn arbeiten muß,...  ...                 3\n",
              "...                                                 ...  ...               ...\n",
              "3189  hier mal eine info. flüchtlinge werden nummer ...  ...                 0\n",
              "3190  .aha .mal abwarten kommt bei uns auch .firmen ...  ...                 0\n",
              "3191                                         .so ist es  ...                 0\n",
              "3192                                     .die warten da  ...                 0\n",
              "3193   .das bekommen die gesagt wie sich verhalten s...  ...                 0\n",
              "\n",
              "[3149 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68DFX_fjYgQz"
      },
      "source": [
        "# Multinomial NB (original)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL5Boe5qYfF4",
        "outputId": "eef50750-1d5f-43cf-e2c6-50c731405eab"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "#from sklearn.pipeline import Pipeline\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn.over_sampling import SMOTE \n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.preprocessing import DenseTransformer\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "def get_feature_column_names(df):\n",
        "    return [cname for cname in df.columns if not cname.startswith('Sub')]\n",
        "\n",
        "def get_target_column_names(df):\n",
        "    return [cname for cname in df.columns if cname.startswith('Sub')]\n",
        "\n",
        "def is_text_column(colname:str)->bool:\n",
        "    if 'text' in colname:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_text_columns(df)->list:\n",
        "    return [cn for cn in df.columns if is_text_column(cn)]\n",
        "\n",
        "def get_nontext_columns(df)->list:\n",
        "    return [cn for cn in df.columns if not is_text_column(cn)]\n",
        "\n",
        "def run_classification(                                                     \\\n",
        "                       dataset:pd.DataFrame,                                \\\n",
        "                       target_column:str,                                   \\\n",
        "                       clf_gen_fn,                                          \\\n",
        "                       use_smote=False)->tuple:\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'n_all_caps']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'num_exclamations']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'perc_exclamations']]\n",
        "    X = dataset[get_feature_column_names(dataset)]\n",
        "    y = dataset[target_column]\n",
        "    trainX, testX, trainY, testY = train_test_split(X, y, random_state=0)\n",
        "\n",
        "    def get_text_pipeline():\n",
        "        return Pipeline(                                                    \\\n",
        "                        [                                                   \\\n",
        "                            ('cv', CountVectorizer(),),                     \\\n",
        "                            ('tfid', TfidfVectorizer()),                    \\\n",
        "                        ])\n",
        "\n",
        "                        \n",
        "    column_trans = make_column_transformer(                                 \\\n",
        "                            (CountVectorizer(ngram_range=(1,1)), 'cleaned_comment_text'),   \\\n",
        "                            (TfidfVectorizer(use_idf=True), 'cleaned_comment_text'),    \\\n",
        "                            (TfidfVectorizer(use_idf=False), 'cleaned_comment_text'),    \\\n",
        "                            remainder=MinMaxScaler(),                       \\\n",
        "                        )\n",
        "\n",
        "    if use_smote:\n",
        "        classif_pipeline = Pipeline(                                        \\\n",
        "                                [                                           \\\n",
        "                                    ('column_transformer', column_trans),   \\\n",
        "                                    ('dense', DenseTransformer()),          \\\n",
        "                                    ('smote', SMOTE(n_jobs=-1)),            \\\n",
        "                                    ('clf', clf_gen_fn()),                  \\\n",
        "                                ])\n",
        "    else:\n",
        "        classif_pipeline = Pipeline(                                        \\\n",
        "                                [                                           \\\n",
        "                                    ('column_transformer', column_trans),   \\\n",
        "                                    ('dense', DenseTransformer()),          \\\n",
        "                                    ('clf', clf_gen_fn()),                  \\\n",
        "                                ])\n",
        "    \n",
        "    \n",
        "    classif_pipeline.fit(trainX, trainY)\n",
        "    y_pred = classif_pipeline.predict(testX)\n",
        "\n",
        "    return accuracy_score(testY, y_pred), f1_score(testY, y_pred)\n",
        "\n",
        "def run_classifiers():\n",
        "\n",
        "    classifiers = {\n",
        "        \"LinearSVC_nosmote\": (False, lambda: LinearSVC(),),\n",
        "        \"LinearSVC\": (True, lambda: LinearSVC(),),\n",
        "        \"AdaBoost\": (True, lambda: AdaBoostClassifier(),),\n",
        "        \"AdaBoost_nosmote\": (False, lambda: AdaBoostClassifier(),),\n",
        "        \"MultinomialNB_nosmote\": (False, lambda: MultinomialNB(),),\n",
        "        \"RandomForestClassifier\": (True, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "        \"RandomForestClassifier_nosmote\": (False, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "        \"BernoulliNB_nosmote\": (False, lambda: BernoulliNB(),),\n",
        "    }\n",
        "\n",
        "    result_df = pd.DataFrame(                                                           \\\n",
        "                    {                                                       \\\n",
        "                        'classifier': pd.Series('str'),                     \\\n",
        "                        'task_name': pd.Series('str'),                      \\\n",
        "                        'metric': pd.Series('str'),                         \\\n",
        "                        'smote': pd.Series(int),                            \\\n",
        "                        'value': pd.Series(float),                          \\\n",
        "                    })\n",
        "\n",
        "    for clfname, value in classifiers.items():\n",
        "        use_smote, clfgen = value\n",
        "        for colname in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "            accuracy, f1 = run_classification(train_df, colname, clfgen, use_smote)\n",
        "            print(f\"{clfname:20.20s} {colname:20.20s} accuracy={accuracy:1.3f}              f1={f1:1.3f}   smote={use_smote}\")\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'accuracy',\n",
        "                'value': accuracy,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'f1_score',\n",
        "                'value': f1,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "\n",
        "        \n",
        "    return result_df\n",
        "\n",
        "result_df = run_classifiers()\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC_nosmote    Sub1_Toxic           accuracy=0.648              f1=0.438   smote=False\n",
            "LinearSVC_nosmote    Sub2_Engaging        accuracy=0.780              f1=0.541   smote=False\n",
            "LinearSVC_nosmote    Sub3_FactClaiming    accuracy=0.716              f1=0.535   smote=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC            Sub1_Toxic           accuracy=0.624              f1=0.475   smote=True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC            Sub2_Engaging        accuracy=0.775              f1=0.538   smote=True\n",
            "LinearSVC            Sub3_FactClaiming    accuracy=0.708              f1=0.533   smote=True\n",
            "AdaBoost             Sub1_Toxic           accuracy=0.652              f1=0.363   smote=True\n",
            "AdaBoost             Sub2_Engaging        accuracy=0.756              f1=0.451   smote=True\n",
            "AdaBoost             Sub3_FactClaiming    accuracy=0.702              f1=0.472   smote=True\n",
            "AdaBoost_nosmote     Sub1_Toxic           accuracy=0.657              f1=0.372   smote=False\n",
            "AdaBoost_nosmote     Sub2_Engaging        accuracy=0.749              f1=0.393   smote=False\n",
            "AdaBoost_nosmote     Sub3_FactClaiming    accuracy=0.709              f1=0.476   smote=False\n",
            "MultinomialNB_nosmot Sub1_Toxic           accuracy=0.662              f1=0.429   smote=False\n",
            "MultinomialNB_nosmot Sub2_Engaging        accuracy=0.737              f1=0.486   smote=False\n",
            "MultinomialNB_nosmot Sub3_FactClaiming    accuracy=0.714              f1=0.593   smote=False\n",
            "RandomForestClassifi Sub1_Toxic           accuracy=0.647              f1=0.491   smote=True\n",
            "RandomForestClassifi Sub2_Engaging        accuracy=0.782              f1=0.423   smote=True\n",
            "RandomForestClassifi Sub3_FactClaiming    accuracy=0.732              f1=0.489   smote=True\n",
            "RandomForestClassifi Sub1_Toxic           accuracy=0.650              f1=0.455   smote=False\n",
            "RandomForestClassifi Sub2_Engaging        accuracy=0.773              f1=0.335   smote=False\n",
            "RandomForestClassifi Sub3_FactClaiming    accuracy=0.730              f1=0.455   smote=False\n",
            "BernoulliNB_nosmote  Sub1_Toxic           accuracy=0.657              f1=0.215   smote=False\n",
            "BernoulliNB_nosmote  Sub2_Engaging        accuracy=0.779              f1=0.379   smote=False\n",
            "BernoulliNB_nosmote  Sub3_FactClaiming    accuracy=0.695              f1=0.302   smote=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a1buL1EozuH",
        "outputId": "fd27021f-d9a7-4533-9191-44848cff7605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(result_df)\n",
        "result_df"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        classifier  ...            value\n",
            "0                              str  ...  <class 'float'>\n",
            "1                LinearSVC_nosmote  ...         0.648477\n",
            "2                LinearSVC_nosmote  ...         0.438134\n",
            "3                LinearSVC_nosmote  ...         0.780457\n",
            "4                LinearSVC_nosmote  ...         0.541114\n",
            "5                LinearSVC_nosmote  ...         0.715736\n",
            "6                LinearSVC_nosmote  ...          0.53527\n",
            "7                        LinearSVC  ...         0.624365\n",
            "8                        LinearSVC  ...         0.475177\n",
            "9                        LinearSVC  ...         0.775381\n",
            "10                       LinearSVC  ...         0.537859\n",
            "11                       LinearSVC  ...         0.708122\n",
            "12                       LinearSVC  ...          0.53252\n",
            "13                        AdaBoost  ...         0.652284\n",
            "14                        AdaBoost  ...         0.362791\n",
            "15                        AdaBoost  ...         0.756345\n",
            "16                        AdaBoost  ...         0.451429\n",
            "17                        AdaBoost  ...         0.701777\n",
            "18                        AdaBoost  ...          0.47191\n",
            "19                AdaBoost_nosmote  ...          0.65736\n",
            "20                AdaBoost_nosmote  ...         0.372093\n",
            "21                AdaBoost_nosmote  ...         0.748731\n",
            "22                AdaBoost_nosmote  ...         0.392638\n",
            "23                AdaBoost_nosmote  ...         0.709391\n",
            "24                AdaBoost_nosmote  ...         0.475973\n",
            "25           MultinomialNB_nosmote  ...         0.662437\n",
            "26           MultinomialNB_nosmote  ...         0.429185\n",
            "27           MultinomialNB_nosmote  ...          0.73731\n",
            "28           MultinomialNB_nosmote  ...         0.486352\n",
            "29           MultinomialNB_nosmote  ...         0.714467\n",
            "30           MultinomialNB_nosmote  ...         0.593128\n",
            "31          RandomForestClassifier  ...         0.647208\n",
            "32          RandomForestClassifier  ...         0.490842\n",
            "33          RandomForestClassifier  ...         0.781726\n",
            "34          RandomForestClassifier  ...         0.422819\n",
            "35          RandomForestClassifier  ...         0.732234\n",
            "36          RandomForestClassifier  ...         0.489104\n",
            "37  RandomForestClassifier_nosmote  ...         0.649746\n",
            "38  RandomForestClassifier_nosmote  ...         0.454545\n",
            "39  RandomForestClassifier_nosmote  ...         0.772843\n",
            "40  RandomForestClassifier_nosmote  ...         0.334572\n",
            "41  RandomForestClassifier_nosmote  ...         0.729695\n",
            "42  RandomForestClassifier_nosmote  ...         0.455243\n",
            "43             BernoulliNB_nosmote  ...          0.65736\n",
            "44             BernoulliNB_nosmote  ...         0.215116\n",
            "45             BernoulliNB_nosmote  ...         0.779188\n",
            "46             BernoulliNB_nosmote  ...         0.378571\n",
            "47             BernoulliNB_nosmote  ...         0.695431\n",
            "48             BernoulliNB_nosmote  ...         0.302326\n",
            "\n",
            "[49 rows x 5 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classifier</th>\n",
              "      <th>task_name</th>\n",
              "      <th>metric</th>\n",
              "      <th>smote</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>str</td>\n",
              "      <td>str</td>\n",
              "      <td>str</td>\n",
              "      <td>&lt;class 'int'&gt;</td>\n",
              "      <td>&lt;class 'float'&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LinearSVC_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0</td>\n",
              "      <td>0.648477</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LinearSVC_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.438134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>LinearSVC_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0</td>\n",
              "      <td>0.780457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LinearSVC_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.541114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>LinearSVC_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0</td>\n",
              "      <td>0.715736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LinearSVC_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.53527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>1</td>\n",
              "      <td>0.624365</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.475177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>1</td>\n",
              "      <td>0.775381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.537859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>1</td>\n",
              "      <td>0.708122</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>LinearSVC</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.53252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>1</td>\n",
              "      <td>0.652284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.362791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>1</td>\n",
              "      <td>0.756345</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.451429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>1</td>\n",
              "      <td>0.701777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.47191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>AdaBoost_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0</td>\n",
              "      <td>0.65736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>AdaBoost_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.372093</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>AdaBoost_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0</td>\n",
              "      <td>0.748731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>AdaBoost_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.392638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>AdaBoost_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0</td>\n",
              "      <td>0.709391</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AdaBoost_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.475973</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>MultinomialNB_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0</td>\n",
              "      <td>0.662437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>MultinomialNB_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.429185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>MultinomialNB_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0</td>\n",
              "      <td>0.73731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>MultinomialNB_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.486352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>MultinomialNB_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0</td>\n",
              "      <td>0.714467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>MultinomialNB_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.593128</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>1</td>\n",
              "      <td>0.647208</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.490842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>1</td>\n",
              "      <td>0.781726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.422819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>1</td>\n",
              "      <td>0.732234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.489104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>RandomForestClassifier_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0</td>\n",
              "      <td>0.649746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>RandomForestClassifier_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.454545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>RandomForestClassifier_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0</td>\n",
              "      <td>0.772843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>RandomForestClassifier_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.334572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>RandomForestClassifier_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0</td>\n",
              "      <td>0.729695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>RandomForestClassifier_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.455243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>BernoulliNB_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0</td>\n",
              "      <td>0.65736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>BernoulliNB_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.215116</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>BernoulliNB_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0</td>\n",
              "      <td>0.779188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>BernoulliNB_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.378571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>BernoulliNB_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>accuracy</td>\n",
              "      <td>0</td>\n",
              "      <td>0.695431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>BernoulliNB_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.302326</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                        classifier  ...            value\n",
              "0                              str  ...  <class 'float'>\n",
              "1                LinearSVC_nosmote  ...         0.648477\n",
              "2                LinearSVC_nosmote  ...         0.438134\n",
              "3                LinearSVC_nosmote  ...         0.780457\n",
              "4                LinearSVC_nosmote  ...         0.541114\n",
              "5                LinearSVC_nosmote  ...         0.715736\n",
              "6                LinearSVC_nosmote  ...          0.53527\n",
              "7                        LinearSVC  ...         0.624365\n",
              "8                        LinearSVC  ...         0.475177\n",
              "9                        LinearSVC  ...         0.775381\n",
              "10                       LinearSVC  ...         0.537859\n",
              "11                       LinearSVC  ...         0.708122\n",
              "12                       LinearSVC  ...          0.53252\n",
              "13                        AdaBoost  ...         0.652284\n",
              "14                        AdaBoost  ...         0.362791\n",
              "15                        AdaBoost  ...         0.756345\n",
              "16                        AdaBoost  ...         0.451429\n",
              "17                        AdaBoost  ...         0.701777\n",
              "18                        AdaBoost  ...          0.47191\n",
              "19                AdaBoost_nosmote  ...          0.65736\n",
              "20                AdaBoost_nosmote  ...         0.372093\n",
              "21                AdaBoost_nosmote  ...         0.748731\n",
              "22                AdaBoost_nosmote  ...         0.392638\n",
              "23                AdaBoost_nosmote  ...         0.709391\n",
              "24                AdaBoost_nosmote  ...         0.475973\n",
              "25           MultinomialNB_nosmote  ...         0.662437\n",
              "26           MultinomialNB_nosmote  ...         0.429185\n",
              "27           MultinomialNB_nosmote  ...          0.73731\n",
              "28           MultinomialNB_nosmote  ...         0.486352\n",
              "29           MultinomialNB_nosmote  ...         0.714467\n",
              "30           MultinomialNB_nosmote  ...         0.593128\n",
              "31          RandomForestClassifier  ...         0.647208\n",
              "32          RandomForestClassifier  ...         0.490842\n",
              "33          RandomForestClassifier  ...         0.781726\n",
              "34          RandomForestClassifier  ...         0.422819\n",
              "35          RandomForestClassifier  ...         0.732234\n",
              "36          RandomForestClassifier  ...         0.489104\n",
              "37  RandomForestClassifier_nosmote  ...         0.649746\n",
              "38  RandomForestClassifier_nosmote  ...         0.454545\n",
              "39  RandomForestClassifier_nosmote  ...         0.772843\n",
              "40  RandomForestClassifier_nosmote  ...         0.334572\n",
              "41  RandomForestClassifier_nosmote  ...         0.729695\n",
              "42  RandomForestClassifier_nosmote  ...         0.455243\n",
              "43             BernoulliNB_nosmote  ...          0.65736\n",
              "44             BernoulliNB_nosmote  ...         0.215116\n",
              "45             BernoulliNB_nosmote  ...         0.779188\n",
              "46             BernoulliNB_nosmote  ...         0.378571\n",
              "47             BernoulliNB_nosmote  ...         0.695431\n",
              "48             BernoulliNB_nosmote  ...         0.302326\n",
              "\n",
              "[49 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hzgub_1VakyE"
      },
      "source": [
        "# Multinomial NB pipeline (modified)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5pktZUUkIxo",
        "outputId": "52d88641-318e-4c99-bf9c-49a6dba1c0d3"
      },
      "source": [
        "#!pip install --upgrade scikit-learn\n",
        "#!pip install imblearn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.7/dist-packages (from imblearn) (0.4.3)\n",
            "Requirement already satisfied: numpy>=1.8.2 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn->imblearn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->imbalanced-learn->imblearn) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20->imbalanced-learn->imblearn) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o6s91zrzoFX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMrooO2zz1mT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "31f9bb4d-1779-4194-f267-9fc791d2271c"
      },
      "source": [
        "#!pip install --upgrade scikit-learn"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (0.22.2.post1)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.0.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (23.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 23.2 MB 295 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-1.0.1 threadpoolctl-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "sklearn"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_eGS0WpoyOo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mV_S-WOU3sQp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "PXqnV32AaoCL",
        "outputId": "442f07b5-b99f-4988-c83f-ab1b0db161e0"
      },
      "source": [
        "import sklearn\n",
        "sklearn.set_config(display=\"diagram\")\n",
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "#from imblearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.preprocessing import DenseTransformer\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import sklearn\n",
        "import itertools\n",
        "\n",
        "MAX_COMBINATION_NUMBER=3\n",
        "\n",
        "def get_feature_column_names(df):\n",
        "    return [cname for cname in df.columns if not cname.startswith('Sub')]\n",
        "\n",
        "def get_target_column_names(df):\n",
        "    return [cname for cname in df.columns if cname.startswith('Sub')]\n",
        "\n",
        "def is_text_column(colname:str)->bool:\n",
        "    if 'text' in colname:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_text_columns(df)->list:\n",
        "    return [cn for cn in df.columns if is_text_column(cn)]\n",
        "\n",
        "def get_nontext_columns(df)->list:\n",
        "    return [cn for cn in df.columns if not is_text_column(cn)]\n",
        "\n",
        "def run_classification(                                                     \\\n",
        "                       dataset:pd.DataFrame,                                \\\n",
        "                       target_column:str,                                   \\\n",
        "                       clf_gen_fn,                                          \\\n",
        "                       use_smote=False)->tuple:\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'n_all_caps']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'num_exclamations']]\n",
        "    #dataset = dataset[[cn for cn in dataset.columns if cn != 'perc_exclamations']]\n",
        "    X = dataset[get_feature_column_names(dataset)]\n",
        "    y = dataset[target_column]\n",
        "    trainX, testX, trainY, testY = train_test_split(X, y, random_state=0)\n",
        "\n",
        "    def get_text_pipeline():\n",
        "        return Pipeline(                                                    \\\n",
        "                        [                                                   \\\n",
        "                            ('cv', CountVectorizer(),),                     \\\n",
        "                            ('tfidt', TfidfVectorizer(),),                  \\\n",
        "                        ])\n",
        "\n",
        "                        \n",
        "    def get_vectorizer(typev):                        \n",
        "        if \"tfidf\" == typev:\n",
        "            return make_column_transformer(                                 \\\n",
        "                                (TfidfVectorizer(ngram_range=(1,1), use_idf=False), 'cleaned_comment_text'),    \\\n",
        "                                remainder='drop',                               \\\n",
        "                            )\n",
        "        elif \"idf\" == typev:\n",
        "            return make_column_transformer(                                 \\\n",
        "                                (TfidfVectorizer(ngram_range=(1,1), use_idf=True), 'cleaned_comment_text'),    \\\n",
        "                                remainder='drop',                               \\\n",
        "                            )\n",
        "        else:\n",
        "            return make_column_transformer(                                 \\\n",
        "                                (CountVectorizer(ngram_range=(1,1)), 'cleaned_comment_text'),   \\\n",
        "                                remainder='drop',                               \\\n",
        "                            )\n",
        "    \n",
        "    \n",
        "    \n",
        "    def get_classif_pipeline(typev):\n",
        "        if use_smote:\n",
        "            classif_pipeline = Pipeline(                                        \\\n",
        "                                    [                                           \\\n",
        "                                        ('column_transformer', get_vectorizer(typev)),   \\\n",
        "                                        ('dense', DenseTransformer()),          \\\n",
        "                                        ('clf', clf_gen_fn()),                  \\\n",
        "                                    ])\n",
        "        else:\n",
        "            classif_pipeline = Pipeline(                                        \\\n",
        "                                    [                                           \\\n",
        "                                        ('column_transformer', get_vectorizer(typev)),   \\\n",
        "                                        ('dense', DenseTransformer()),          \\\n",
        "                                        ('clf', clf_gen_fn()),                  \\\n",
        "                                    ])\n",
        "        return classif_pipeline\n",
        "    \n",
        "    def get_dummy_classifier(cols):\n",
        "        col_selector = ColumnTransformer([('selector', 'passthrough', cols)], remainder='drop')\n",
        "        classif2_pipeline = Pipeline(                                           \\\n",
        "                                [                                               \\\n",
        "                                    ('colselector', col_selector),              \\\n",
        "                                    ('dummy', LinearSVC(max_iter=100_000) if len(cols) < 2 else DecisionTreeClassifier()),                 \\\n",
        "                                ])\n",
        "        return classif2_pipeline\n",
        "\n",
        "    import itertools\n",
        "    col_arr = []\n",
        "    numericcols = get_nontext_columns(trainX)\n",
        "    for i in range(1, min(MAX_COMBINATION_NUMBER + 1, len(numericcols) + 1)):\n",
        "        for arr in itertools.combinations(numericcols, i):\n",
        "            col_arr.append(list(arr))\n",
        "\n",
        "    classifier_array = []\n",
        "    for i in ['tf', 'tfidf', 'count']:\n",
        "        classifier_array.append((f'classifier_{i}', get_classif_pipeline(i)))\n",
        "    for n, arr in enumerate(col_arr):\n",
        "        classifier_array.append((f'classifier_{n}', get_dummy_classifier(arr)))\n",
        "\n",
        "    classifier = StackingClassifier(classifier_array, final_estimator=RandomForestClassifier())\n",
        "       \n",
        "    classifier.fit(trainX, trainY)\n",
        "    y_pred = classifier.predict(testX)\n",
        "\n",
        "\n",
        "    return accuracy_score(testY, y_pred), f1_score(testY, y_pred), Pipeline([('pipeline', classifier)])\n",
        "\n",
        "\n",
        "def run_classifiers():\n",
        "\n",
        "    classifiers = {\n",
        "        \"LinearSVC_nosmote\": (False, lambda: LinearSVC(),),\n",
        "        #\"LinearSVC\": (True, lambda: LinearSVC(),),\n",
        "        #\"AdaBoost\": (True, lambda: AdaBoostClassifier(),),\n",
        "        #\"AdaBoost_nosmote\": (False, lambda: AdaBoostClassifier(),),\n",
        "        \"MultinomialNB_nosmote\": (False, lambda: MultinomialNB(),),\n",
        "        \"RandomForestClassifier\": (True, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "        #\"RandomForestClassifier_nosmote\": (False, lambda: RandomForestClassifier(n_jobs=-1),),\n",
        "        #\"BernoulliNB_nosmote\": (False, lambda: BernoulliNB(),),\n",
        "    }\n",
        "\n",
        "    result_df = pd.DataFrame(                                                           \\\n",
        "                    {                                                       \\\n",
        "                        'classifier': pd.Series('str'),                     \\\n",
        "                        'task_name': pd.Series('str'),                      \\\n",
        "                        'metric': pd.Series('str'),                         \\\n",
        "                        'smote': pd.Series(int),                            \\\n",
        "                        'value': pd.Series(float),                          \\\n",
        "                    })\n",
        "\n",
        "    model = None\n",
        "    for clfname, value in classifiers.items():\n",
        "        use_smote, clfgen = value\n",
        "        for colname in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "            accuracy, f1, model = run_classification(train_df, colname, clfgen, use_smote)\n",
        "            print(f\"{clfname:20.20s} {colname:20.20s} accuracy={accuracy:1.3f}              f1={f1:1.3f}   smote={use_smote}\")\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'accuracy',\n",
        "                'value': accuracy,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "            result_dict = {\n",
        "                'classifier': clfname,\n",
        "                'task_name': colname,\n",
        "                'metric': 'f1_score',\n",
        "                'value': f1,\n",
        "                'smote': 1 if use_smote else 0\n",
        "            }\n",
        "            result_df = result_df.append(result_dict, ignore_index=True)\n",
        "\n",
        "        \n",
        "    return result_df, model\n",
        "\n",
        "result_df, model = run_classifiers()\n",
        "\n",
        "model\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearSVC_nosmote    Sub1_Toxic           accuracy=0.654              f1=0.367   smote=False\n",
            "LinearSVC_nosmote    Sub2_Engaging        accuracy=0.763              f1=0.490   smote=False\n",
            "LinearSVC_nosmote    Sub3_FactClaiming    accuracy=0.716              f1=0.513   smote=False\n",
            "MultinomialNB_nosmot Sub1_Toxic           accuracy=0.664              f1=0.433   smote=False\n",
            "MultinomialNB_nosmot Sub2_Engaging        accuracy=0.763              f1=0.438   smote=False\n",
            "MultinomialNB_nosmot Sub3_FactClaiming    accuracy=0.714              f1=0.514   smote=False\n",
            "RandomForestClassifi Sub1_Toxic           accuracy=0.640              f1=0.357   smote=True\n",
            "RandomForestClassifi Sub2_Engaging        accuracy=0.794              f1=0.524   smote=True\n",
            "RandomForestClassifi Sub3_FactClaiming    accuracy=0.712              f1=0.526   smote=True\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style>#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 {color: black;background-color: white;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 pre{padding: 0;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-toggleable {background-color: white;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-estimator:hover {background-color: #d4ebff;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-item {z-index: 1;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-parallel-item:only-child::after {width: 0;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-884c5b8c-a4b8-4ffd-8858-50f4bf9d5b56\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e5c19263-39c0-4f39-a08a-59f610c0bcc3\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e5c19263-39c0-4f39-a08a-59f610c0bcc3\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('pipeline',\n",
              "                 StackingClassifier(estimators=[('classifier_tf',\n",
              "                                                 Pipeline(steps=[('column_transformer',\n",
              "                                                                  ColumnTransformer(transformers=[('countvectorizer',\n",
              "                                                                                                   CountVectorizer(),\n",
              "                                                                                                   'cleaned_comment_text')])),\n",
              "                                                                 ('dense',\n",
              "                                                                  DenseTransformer()),\n",
              "                                                                 ('clf',\n",
              "                                                                  RandomForestClassifier(n_jobs=-1))])),\n",
              "                                                ('classifier_tfidf',\n",
              "                                                 Pipeline(steps=[('column_transformer',\n",
              "                                                                  ColumnTr...\n",
              "                                                                  ColumnTransformer(transformers=[('selector',\n",
              "                                                                                                   'passthrough',\n",
              "                                                                                                   ['perc_exclamations',\n",
              "                                                                                                    'num_exclamations'])])),\n",
              "                                                                 ('dummy',\n",
              "                                                                  DecisionTreeClassifier())])),\n",
              "                                                ('classifier_6',\n",
              "                                                 Pipeline(steps=[('colselector',\n",
              "                                                                  ColumnTransformer(transformers=[('selector',\n",
              "                                                                                                   'passthrough',\n",
              "                                                                                                   ['n_all_caps',\n",
              "                                                                                                    'perc_exclamations',\n",
              "                                                                                                    'num_exclamations'])])),\n",
              "                                                                 ('dummy',\n",
              "                                                                  DecisionTreeClassifier())]))],\n",
              "                                    final_estimator=RandomForestClassifier()))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"1eef4d5a-fa11-4800-9982-3e6f7b8733eb\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"1eef4d5a-fa11-4800-9982-3e6f7b8733eb\">pipeline: StackingClassifier</label><div class=\"sk-toggleable__content\"><pre>StackingClassifier(estimators=[('classifier_tf',\n",
              "                                Pipeline(steps=[('column_transformer',\n",
              "                                                 ColumnTransformer(transformers=[('countvectorizer',\n",
              "                                                                                  CountVectorizer(),\n",
              "                                                                                  'cleaned_comment_text')])),\n",
              "                                                ('dense', DenseTransformer()),\n",
              "                                                ('clf',\n",
              "                                                 RandomForestClassifier(n_jobs=-1))])),\n",
              "                               ('classifier_tfidf',\n",
              "                                Pipeline(steps=[('column_transformer',\n",
              "                                                 ColumnTransformer(transformers=[('tf...\n",
              "                                                 ColumnTransformer(transformers=[('selector',\n",
              "                                                                                  'passthrough',\n",
              "                                                                                  ['perc_exclamations',\n",
              "                                                                                   'num_exclamations'])])),\n",
              "                                                ('dummy',\n",
              "                                                 DecisionTreeClassifier())])),\n",
              "                               ('classifier_6',\n",
              "                                Pipeline(steps=[('colselector',\n",
              "                                                 ColumnTransformer(transformers=[('selector',\n",
              "                                                                                  'passthrough',\n",
              "                                                                                  ['n_all_caps',\n",
              "                                                                                   'perc_exclamations',\n",
              "                                                                                   'num_exclamations'])])),\n",
              "                                                ('dummy',\n",
              "                                                 DecisionTreeClassifier())]))],\n",
              "                   final_estimator=RandomForestClassifier())</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_tf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"4fcd2d56-ecf6-4270-929b-c2292989b4d4\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"4fcd2d56-ecf6-4270-929b-c2292989b4d4\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('countvectorizer', CountVectorizer(),\n",
              "                                 'cleaned_comment_text')])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"156a1f52-2834-4ef9-97df-e7f328ca9681\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"156a1f52-2834-4ef9-97df-e7f328ca9681\">countvectorizer</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5edb57ab-0448-435a-8b12-521ff75b303d\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"5edb57ab-0448-435a-8b12-521ff75b303d\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"a985196f-db5a-492b-83a4-0a7da4a85dc4\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"a985196f-db5a-492b-83a4-0a7da4a85dc4\">DenseTransformer</label><div class=\"sk-toggleable__content\"><pre>DenseTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e083fdb0-a1df-4d99-8173-521c92aa2642\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e083fdb0-a1df-4d99-8173-521c92aa2642\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=-1)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_tfidf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"34a1ce7c-f455-4788-a85e-2f8a59385e01\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"34a1ce7c-f455-4788-a85e-2f8a59385e01\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('tfidfvectorizer',\n",
              "                                 TfidfVectorizer(use_idf=False),\n",
              "                                 'cleaned_comment_text')])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"aa0059de-6ec9-4277-8fff-d21ee027d624\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"aa0059de-6ec9-4277-8fff-d21ee027d624\">tfidfvectorizer</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"762d909f-e917-4a0a-8e00-f3d95068169b\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"762d909f-e917-4a0a-8e00-f3d95068169b\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(use_idf=False)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"a3539a06-78d0-4fb4-8ec2-f6a4d4fdfdfd\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"a3539a06-78d0-4fb4-8ec2-f6a4d4fdfdfd\">DenseTransformer</label><div class=\"sk-toggleable__content\"><pre>DenseTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"0c78cf4f-1750-40fc-ad84-6698d1c79830\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"0c78cf4f-1750-40fc-ad84-6698d1c79830\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=-1)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_count</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"7b1ce851-ff59-473f-a66b-478c4e5ef6d5\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"7b1ce851-ff59-473f-a66b-478c4e5ef6d5\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('countvectorizer', CountVectorizer(),\n",
              "                                 'cleaned_comment_text')])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"369948df-ec5c-4167-9017-11e53f6348f3\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"369948df-ec5c-4167-9017-11e53f6348f3\">countvectorizer</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"0f276d59-e4fa-4e0a-9c2e-479a17ac4bc4\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"0f276d59-e4fa-4e0a-9c2e-479a17ac4bc4\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"18629cb1-2834-4c98-8135-9dbdd9ca1291\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"18629cb1-2834-4c98-8135-9dbdd9ca1291\">DenseTransformer</label><div class=\"sk-toggleable__content\"><pre>DenseTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f08e9ddb-8092-4f0e-8ccb-2daa04ee4ec9\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"f08e9ddb-8092-4f0e-8ccb-2daa04ee4ec9\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_jobs=-1)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_0</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"d1ba8bd4-6a2a-4b17-b494-19d75aaabd38\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"d1ba8bd4-6a2a-4b17-b494-19d75aaabd38\">colselector: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('selector', 'passthrough', ['n_all_caps'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"7a94722d-3efd-4e96-8ac9-64bb13bdf2bb\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"7a94722d-3efd-4e96-8ac9-64bb13bdf2bb\">selector</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"d480510b-19e3-44e4-b6d0-04d450b41b56\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"d480510b-19e3-44e4-b6d0-04d450b41b56\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e4922fc5-5cf0-4077-b88c-a2c75a3538ec\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e4922fc5-5cf0-4077-b88c-a2c75a3538ec\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(max_iter=100000)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_1</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f9bd7a70-9c39-460b-8dd6-3e2e44684c67\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"f9bd7a70-9c39-460b-8dd6-3e2e44684c67\">colselector: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('selector', 'passthrough',\n",
              "                                 ['perc_exclamations'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"ec31854f-c7cf-4108-b8f4-4619063612c5\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"ec31854f-c7cf-4108-b8f4-4619063612c5\">selector</label><div class=\"sk-toggleable__content\"><pre>['perc_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"314be51d-027a-48f4-a3ce-14336d5f1452\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"314be51d-027a-48f4-a3ce-14336d5f1452\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"9f1d40bb-d8da-4a81-9b8b-63a6bad11301\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"9f1d40bb-d8da-4a81-9b8b-63a6bad11301\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(max_iter=100000)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_2</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"fef551fc-319c-44b5-a822-666f7799a5b0\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"fef551fc-319c-44b5-a822-666f7799a5b0\">colselector: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('selector', 'passthrough',\n",
              "                                 ['num_exclamations'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"97993d7b-ca5a-45ee-b663-d4469d682920\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"97993d7b-ca5a-45ee-b663-d4469d682920\">selector</label><div class=\"sk-toggleable__content\"><pre>['num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"db6b8638-b6d6-4d6c-8fe2-c90791e3b928\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"db6b8638-b6d6-4d6c-8fe2-c90791e3b928\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f0921e6f-ebb4-4157-b50f-3d86a63aab3f\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"f0921e6f-ebb4-4157-b50f-3d86a63aab3f\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(max_iter=100000)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_3</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"658e3bdd-0405-447a-bc48-2b8e440c8fb1\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"658e3bdd-0405-447a-bc48-2b8e440c8fb1\">colselector: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('selector', 'passthrough',\n",
              "                                 ['n_all_caps', 'perc_exclamations'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"ad1f13e9-fe9a-4730-a23a-3963f581794b\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"ad1f13e9-fe9a-4730-a23a-3963f581794b\">selector</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps', 'perc_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"90a715ea-1942-4aeb-a79b-e11821bdf58d\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"90a715ea-1942-4aeb-a79b-e11821bdf58d\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"34da6677-c98c-4623-ae60-e079e8dcc01b\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"34da6677-c98c-4623-ae60-e079e8dcc01b\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_4</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"55f80ef6-73a7-4a30-ae40-e97d06da7d97\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"55f80ef6-73a7-4a30-ae40-e97d06da7d97\">colselector: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('selector', 'passthrough',\n",
              "                                 ['n_all_caps', 'num_exclamations'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c315de9c-2e92-4a93-a7a6-ae741842d9e4\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"c315de9c-2e92-4a93-a7a6-ae741842d9e4\">selector</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps', 'num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"7eabb481-7f0d-414c-b069-33a05b391ffa\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"7eabb481-7f0d-414c-b069-33a05b391ffa\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"8834ec3f-fc68-42e5-9c8c-88b8e0e8852d\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"8834ec3f-fc68-42e5-9c8c-88b8e0e8852d\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_5</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"ca8994d5-b10d-42c9-9391-a9cc979f5066\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"ca8994d5-b10d-42c9-9391-a9cc979f5066\">colselector: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('selector', 'passthrough',\n",
              "                                 ['perc_exclamations', 'num_exclamations'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5b65c482-25db-4d87-8763-8a2612cb81ba\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"5b65c482-25db-4d87-8763-8a2612cb81ba\">selector</label><div class=\"sk-toggleable__content\"><pre>['perc_exclamations', 'num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2e9677ec-88f6-42c2-8d19-268d2c68baff\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"2e9677ec-88f6-42c2-8d19-268d2c68baff\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"8918f43f-c000-4edb-a201-2e6d46203dd3\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"8918f43f-c000-4edb-a201-2e6d46203dd3\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>classifier_6</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5edcfec3-10d7-4669-bbb1-7ef53d635770\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"5edcfec3-10d7-4669-bbb1-7ef53d635770\">colselector: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[('selector', 'passthrough',\n",
              "                                 ['n_all_caps', 'perc_exclamations',\n",
              "                                  'num_exclamations'])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e551df47-d14b-4141-ba70-f660ac43fc76\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e551df47-d14b-4141-ba70-f660ac43fc76\">selector</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps', 'perc_exclamations', 'num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"9d575c4b-c90b-478a-a92c-a8da7195a500\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"9d575c4b-c90b-478a-a92c-a8da7195a500\">passthrough</label><div class=\"sk-toggleable__content\"><pre>passthrough</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"be37c13e-ceec-4ed1-b9e0-e88a0f2c8346\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"be37c13e-ceec-4ed1-b9e0-e88a0f2c8346\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"324c59a6-94a3-42aa-a05e-6e8b5d94c6f1\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"324c59a6-94a3-42aa-a05e-6e8b5d94c6f1\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('pipeline',\n",
              "                 StackingClassifier(estimators=[('classifier_tf',\n",
              "                                                 Pipeline(steps=[('column_transformer',\n",
              "                                                                  ColumnTransformer(transformers=[('countvectorizer',\n",
              "                                                                                                   CountVectorizer(),\n",
              "                                                                                                   'cleaned_comment_text')])),\n",
              "                                                                 ('dense',\n",
              "                                                                  DenseTransformer()),\n",
              "                                                                 ('clf',\n",
              "                                                                  RandomForestClassifier(n_jobs=-1))])),\n",
              "                                                ('classifier_tfidf',\n",
              "                                                 Pipeline(steps=[('column_transformer',\n",
              "                                                                  ColumnTr...\n",
              "                                                                  ColumnTransformer(transformers=[('selector',\n",
              "                                                                                                   'passthrough',\n",
              "                                                                                                   ['perc_exclamations',\n",
              "                                                                                                    'num_exclamations'])])),\n",
              "                                                                 ('dummy',\n",
              "                                                                  DecisionTreeClassifier())])),\n",
              "                                                ('classifier_6',\n",
              "                                                 Pipeline(steps=[('colselector',\n",
              "                                                                  ColumnTransformer(transformers=[('selector',\n",
              "                                                                                                   'passthrough',\n",
              "                                                                                                   ['n_all_caps',\n",
              "                                                                                                    'perc_exclamations',\n",
              "                                                                                                    'num_exclamations'])])),\n",
              "                                                                 ('dummy',\n",
              "                                                                  DecisionTreeClassifier())]))],\n",
              "                                    final_estimator=RandomForestClassifier()))])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EiRR4fd9sb7",
        "outputId": "ff55c50d-999c-46bb-9423-e6f9fa2124f4"
      },
      "source": [
        "def print_df(df, metric, task):\n",
        "    df = df[(df['metric'] == metric) & (df['task_name'] == task)]\n",
        "    df = df.sort_values(by=['value'], ascending=False)\n",
        "    print(df.head(3))\n",
        "    return df\n",
        "\n",
        "for task_name in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "    print('=' * 80)\n",
        "    print(task_name)\n",
        "    print('-' * len(task_name))\n",
        "    print()\n",
        "    for metric in ['accuracy', 'f1_score']:\n",
        "        print_df(result_df, metric, task_name)\n",
        "        print()\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Sub1_Toxic\n",
            "----------\n",
            "\n",
            "                classifier   task_name    metric smote     value\n",
            "7    MultinomialNB_nosmote  Sub1_Toxic  accuracy     0  0.663706\n",
            "1        LinearSVC_nosmote  Sub1_Toxic  accuracy     0  0.653553\n",
            "13  RandomForestClassifier  Sub1_Toxic  accuracy     1  0.639594\n",
            "\n",
            "                classifier   task_name    metric smote     value\n",
            "8    MultinomialNB_nosmote  Sub1_Toxic  f1_score     0  0.432548\n",
            "2        LinearSVC_nosmote  Sub1_Toxic  f1_score     0  0.366589\n",
            "14  RandomForestClassifier  Sub1_Toxic  f1_score     1  0.357466\n",
            "\n",
            "================================================================================\n",
            "Sub2_Engaging\n",
            "-------------\n",
            "\n",
            "                classifier      task_name    metric smote     value\n",
            "15  RandomForestClassifier  Sub2_Engaging  accuracy     1  0.794416\n",
            "3        LinearSVC_nosmote  Sub2_Engaging  accuracy     0   0.76269\n",
            "9    MultinomialNB_nosmote  Sub2_Engaging  accuracy     0   0.76269\n",
            "\n",
            "                classifier      task_name    metric smote     value\n",
            "16  RandomForestClassifier  Sub2_Engaging  f1_score     1  0.523529\n",
            "4        LinearSVC_nosmote  Sub2_Engaging  f1_score     0  0.490463\n",
            "10   MultinomialNB_nosmote  Sub2_Engaging  f1_score     0  0.438438\n",
            "\n",
            "================================================================================\n",
            "Sub3_FactClaiming\n",
            "-----------------\n",
            "\n",
            "                classifier          task_name    metric smote     value\n",
            "5        LinearSVC_nosmote  Sub3_FactClaiming  accuracy     0  0.715736\n",
            "11   MultinomialNB_nosmote  Sub3_FactClaiming  accuracy     0  0.714467\n",
            "17  RandomForestClassifier  Sub3_FactClaiming  accuracy     1  0.711929\n",
            "\n",
            "                classifier          task_name    metric smote     value\n",
            "18  RandomForestClassifier  Sub3_FactClaiming  f1_score     1  0.526096\n",
            "12   MultinomialNB_nosmote  Sub3_FactClaiming  f1_score     0  0.514039\n",
            "6        LinearSVC_nosmote  Sub3_FactClaiming  f1_score     0  0.513043\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "5mvNlGAA-eN9",
        "outputId": "47a01cd7-204d-4be1-80b9-873f9643701f"
      },
      "source": [
        "#result_df\n",
        "result_df[result_df['metric'] == 'f1_score'].sort_values(by='value', ascending=False)\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>classifier</th>\n",
              "      <th>task_name</th>\n",
              "      <th>metric</th>\n",
              "      <th>smote</th>\n",
              "      <th>value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.526096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.523529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>MultinomialNB_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.514039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LinearSVC_nosmote</td>\n",
              "      <td>Sub3_FactClaiming</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.513043</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>LinearSVC_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.490463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>MultinomialNB_nosmote</td>\n",
              "      <td>Sub2_Engaging</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.438438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>MultinomialNB_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.432548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>LinearSVC_nosmote</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>0</td>\n",
              "      <td>0.366589</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>RandomForestClassifier</td>\n",
              "      <td>Sub1_Toxic</td>\n",
              "      <td>f1_score</td>\n",
              "      <td>1</td>\n",
              "      <td>0.357466</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                classifier          task_name    metric smote     value\n",
              "18  RandomForestClassifier  Sub3_FactClaiming  f1_score     1  0.526096\n",
              "16  RandomForestClassifier      Sub2_Engaging  f1_score     1  0.523529\n",
              "12   MultinomialNB_nosmote  Sub3_FactClaiming  f1_score     0  0.514039\n",
              "6        LinearSVC_nosmote  Sub3_FactClaiming  f1_score     0  0.513043\n",
              "4        LinearSVC_nosmote      Sub2_Engaging  f1_score     0  0.490463\n",
              "10   MultinomialNB_nosmote      Sub2_Engaging  f1_score     0  0.438438\n",
              "8    MultinomialNB_nosmote         Sub1_Toxic  f1_score     0  0.432548\n",
              "2        LinearSVC_nosmote         Sub1_Toxic  f1_score     0  0.366589\n",
              "14  RandomForestClassifier         Sub1_Toxic  f1_score     1  0.357466"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbcLIE7oC2ML"
      },
      "source": [
        "## BERT MODEL\n",
        "\n",
        "Use code from https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9TbWSQ0c7C_"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "from official.nlp import optimization\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "tfhub_handle_encoder = \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1\"\n",
        "tfhub_handle_preprocess = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n",
        "\n",
        "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)\n",
        "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEzTyC_sru6w"
      },
      "source": [
        "## HUGGING FACE TRANSFORMERS : GERMAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zul78K-dKK3h",
        "outputId": "f4967889-3c28-4d08-fa98-708cd2f854c4"
      },
      "source": [
        "from transformers import AutoTokenizer,TFAutoModel\n",
        "from transformers import AutoTokenizer, AutoModelForMaskedLM, TFAutoModelForMaskedLM\n",
        "import tensorflow as tf\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import IPython\n",
        "\n",
        "SEQUENCE_LENGTH = 512\n",
        "\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
        "\n",
        "def tokenize_input_with_bert(bert_tokenizer, input:str):\n",
        "    tokens = bert_tokenizer.encode_plus(\n",
        "                    input,\n",
        "                    max_length=SEQUENCE_LENGTH,\n",
        "                    truncation=True,\n",
        "                    padding='max_length',\n",
        "                    add_special_tokens=True,\n",
        "                    return_attention_mask=True,\n",
        "                    return_token_type_ids=False,\n",
        "                    return_tensors='np'\n",
        "                )\n",
        "    return tokens['input_ids'], tokens['attention_mask']\n",
        "\n",
        "def tokenize_input_with_bert(bert_tokenizer, input:str, seq_len:int):\n",
        "    def pad(t):\n",
        "        try:\n",
        "            t = t.reshape((t.shape[1],))\n",
        "        except:\n",
        "            pass\n",
        "        t = t[:seq_len]\n",
        "        pad_len = max(0, seq_len - t.shape[0])\n",
        "        t = np.pad(t, (0, pad_len))\n",
        "        t = t.reshape((1, t.shape[0]))\n",
        "        return t\n",
        "\n",
        "    tokens = bert_tokenizer(\n",
        "                    input,\n",
        "                    truncation=True,\n",
        "                    add_special_tokens=True,\n",
        "                    return_attention_mask=True,\n",
        "                    return_token_type_ids=False,\n",
        "                    return_tensors='np')\n",
        "\n",
        "    return pad(tokens['input_ids']), pad(tokens['attention_mask'])\n",
        "\n",
        "def tokenize_dataframe_of_text(df, seq_len:int):\n",
        "    x_ids = np.zeros((0, seq_len))\n",
        "    x_mask = np.zeros((0, seq_len))\n",
        "\n",
        "    for i, text in enumerate(df):\n",
        "        id, mask = tokenize_input_with_bert(bert_tokenizer, text, seq_len)\n",
        "        x_ids = np.append(x_ids, id, axis=0)\n",
        "        x_mask = np.append(x_mask, mask, axis=0)\n",
        "\n",
        "    return x_ids, x_mask\n",
        "\n",
        "def build_model(seq_len):\n",
        "    \n",
        "    input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
        "    mask = tf.keras.layers.Input(shape=(seq_len,), name='attn_mask', dtype='int32')\n",
        "\n",
        "    bert_model = TFAutoModelForMaskedLM.from_pretrained(\"bert-base-german-cased\")\n",
        "\n",
        "    X = bert_model(input_ids, attention_mask=mask)\n",
        "    X = X[0]\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "    X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
        "    X = tf.keras.layers.Dropout(0.1)(X)\n",
        "    X = tf.keras.layers.Flatten()(X)\n",
        "    y = tf.keras.layers.Dense(1, activation='softmax', name='outputs')(X)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "\n",
        "    bert_model.trainable = False\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "def fit_model(model, X, y):\n",
        "    X_ids, X_masks = tokenize_dataframe_of_text(X, SEQUENCE_LENGTH)\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X_ids, X_masks, y))\n",
        "\n",
        "    def map_fn(input_ids, masks, labels):\n",
        "        return {'input_ids': input_ids, 'attn_mask': masks}, labels\n",
        "    ds = ds.map(map_fn)\n",
        "\n",
        "    ds = ds.shuffle(10000).batch(32)\n",
        "\n",
        "    train_ds = ds.take((len(ds) * 8) // 10)\n",
        "    val_ds = ds.skip((len(ds) * 8) // 10)\n",
        "\n",
        "    model = build_model(SEQUENCE_LENGTH)\n",
        "\n",
        "    history = model.fit(train_ds, validation_data=val_ds, epochs=20)\n",
        "\n",
        "    return history\n",
        "\n",
        "def create_model():\n",
        "    return build_model(SEQUENCE_LENGTH)\n",
        "\n",
        "model = create_model()\n",
        "train_df, test_df = get_clean_train_test_df()\n",
        "history = fit_model(model, train_df['comment_text'], train_df['Sub3_FactClaiming'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-german-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
            "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
            "\n",
            "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at bert-base-german-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "80/80 [==============================] - 918s 11s/step - loss: 11.7735 - accuracy: 0.3371 - val_loss: 10.7914 - val_accuracy: 0.3549\n",
            "Epoch 2/20\n",
            "80/80 [==============================] - 916s 11s/step - loss: 16.1275 - accuracy: 0.3484 - val_loss: 9.1900 - val_accuracy: 0.3644\n",
            "Epoch 3/20\n",
            "80/80 [==============================] - 905s 11s/step - loss: 9.7318 - accuracy: 0.3391 - val_loss: 12.2273 - val_accuracy: 0.3628\n",
            "Epoch 4/20\n",
            "80/80 [==============================] - 902s 11s/step - loss: 7.8757 - accuracy: 0.3406 - val_loss: 5.4716 - val_accuracy: 0.3549\n",
            "Epoch 5/20\n",
            "80/80 [==============================] - 902s 11s/step - loss: 3.0658 - accuracy: 0.3410 - val_loss: 1.6754 - val_accuracy: 0.3580\n",
            "Epoch 6/20\n",
            "80/80 [==============================] - 901s 11s/step - loss: 2.2503 - accuracy: 0.3457 - val_loss: 0.6449 - val_accuracy: 0.3502\n",
            "Epoch 7/20\n",
            "80/80 [==============================] - 894s 11s/step - loss: 1.0822 - accuracy: 0.3379 - val_loss: 0.5585 - val_accuracy: 0.3281\n",
            "Epoch 8/20\n",
            "80/80 [==============================] - 904s 11s/step - loss: 0.9531 - accuracy: 0.3383 - val_loss: 0.6451 - val_accuracy: 0.3438\n",
            "Epoch 9/20\n",
            "80/80 [==============================] - 897s 11s/step - loss: 0.7708 - accuracy: 0.3375 - val_loss: 0.2565 - val_accuracy: 0.3265\n",
            "Epoch 10/20\n",
            "80/80 [==============================] - 901s 11s/step - loss: 0.7691 - accuracy: 0.3352 - val_loss: 0.2079 - val_accuracy: 0.3565\n",
            "Epoch 11/20\n",
            "80/80 [==============================] - 902s 11s/step - loss: 0.7401 - accuracy: 0.3434 - val_loss: 0.2741 - val_accuracy: 0.3407\n",
            "Epoch 12/20\n",
            "80/80 [==============================] - 899s 11s/step - loss: 0.6107 - accuracy: 0.3449 - val_loss: 0.2575 - val_accuracy: 0.3580\n",
            "Epoch 13/20\n",
            "80/80 [==============================] - 895s 11s/step - loss: 0.5235 - accuracy: 0.3355 - val_loss: 0.2401 - val_accuracy: 0.3281\n",
            "Epoch 14/20\n",
            "80/80 [==============================] - 890s 11s/step - loss: 0.7208 - accuracy: 0.3371 - val_loss: 0.2019 - val_accuracy: 0.3344\n",
            "Epoch 15/20\n",
            "80/80 [==============================] - 898s 11s/step - loss: 0.5936 - accuracy: 0.3406 - val_loss: 0.2444 - val_accuracy: 0.3596\n",
            "Epoch 16/20\n",
            "80/80 [==============================] - 891s 11s/step - loss: 0.7424 - accuracy: 0.3398 - val_loss: 0.3053 - val_accuracy: 0.3628\n",
            "Epoch 17/20\n",
            "80/80 [==============================] - 892s 11s/step - loss: 0.8034 - accuracy: 0.3387 - val_loss: 0.2235 - val_accuracy: 0.3423\n",
            "Epoch 18/20\n",
            "80/80 [==============================] - 907s 11s/step - loss: 0.6434 - accuracy: 0.3387 - val_loss: 0.2895 - val_accuracy: 0.3628\n",
            "Epoch 19/20\n",
            "12/80 [===>..........................] - ETA: 10:34 - loss: 0.6248 - accuracy: 0.3646"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QS8NgaRs-k8O"
      },
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(history.history)\n",
        "plt.plot(history.history['loss'], label='loss', color='b')\n",
        "plt.plot(history.history['val_loss'], label='val_loss', color='r')\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='acc', color='g')\n",
        "plt.plot(history.history['val_accuracy'], label='val_acc', color='cyan')\n",
        "\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_OUi7BXtbzo"
      },
      "source": [
        "import IPython\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjV2E3i_0f3i"
      },
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jGhWSvlj2Ywc"
      },
      "source": [
        "def build_model(seq_len):\n",
        "    \n",
        "    input_ids = tf.keras.layers.Input(shape=(seq_len,), name='input_ids', dtype='int32')\n",
        "    mask = tf.keras.layers.Input(shape=(seq_len,), name='attn_mask', dtype='int32')\n",
        "\n",
        "    bert_model = TFAutoModelForMaskedLM.from_pretrained(\"bert-base-german-cased\")\n",
        "\n",
        "    X = bert_model(input_ids, attention_mask=mask)\n",
        "    X = X[0]\n",
        "    X = tf.keras.layers.BatchNormalization()(X)\n",
        "\n",
        "\n",
        "    X = tf.keras.layers.Conv1D(64, (4,), padding='same',  activation='relu')(X)\n",
        "    X = tf.keras.layers.MaxPooling1D(2,)(X)\n",
        "    \n",
        "    X = tf.keras.layers.Conv1D(128, (4,), padding='same',  activation='relu')(X)\n",
        "    X = tf.keras.layers.MaxPooling1D(2,)(X)\n",
        "\n",
        "    X = tf.keras.layers.Conv1D(256, (4,), padding='same',  activation='relu')(X)\n",
        "    X = tf.keras.layers.MaxPooling1D(2,)(X)\n",
        "\n",
        "    X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
        "    X = tf.keras.layers.Dropout(0.1)(X)\n",
        "    X = tf.keras.layers.Flatten()(X)\n",
        "    X = tf.keras.layers.Dense(512, activation='relu')(X)\n",
        "    X = tf.keras.layers.Dense(64, activation='relu')(X)\n",
        "    X = tf.keras.layers.Dense(8, activation='relu')(X)\n",
        "    y = tf.keras.layers.Dense(1, activation='softmax', name='outputs')(X)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_ids, mask], outputs=y)\n",
        "\n",
        "    bert_model.trainable = False\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    loss = tf.keras.losses.BinaryCrossentropy()\n",
        "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def create_model():\n",
        "    return build_model(SEQUENCE_LENGTH)\n",
        "\n",
        "\n",
        "m1 = create_model()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd4lPVS60r2-"
      },
      "source": [
        "\n",
        "tf.keras.utils.plot_model(m1, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DlYEb8mO4Zrv"
      },
      "source": [
        "print(m1.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbANX9ah4yGv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}