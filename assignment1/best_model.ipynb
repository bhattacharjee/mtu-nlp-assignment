{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "best_model.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNu+vsFlIF/3Xt48hvBTBsc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment1/best_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERvBOEn0wss6"
      },
      "source": [
        "!pip install spacy  nltk spacymoji huggingface -q       >/dev/null 2>&1         \n",
        "!pip install -q -U tensorflow-text                      >/dev/null 2>&1\n",
        "!pip install -q tf-models-official                      >/dev/null 2>&1\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "!pip install transformers                               >/dev/null 2>&1\n",
        "\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "\n",
        "!pip install mlxtend                                    >/dev/null 2>&1\n",
        "!pip install imblearn                                   >/dev/null 2>&1\n",
        "\n",
        "# handling emojis\n",
        "!pip install demoji                                     >/dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-DAX9VLZlw3"
      },
      "source": [
        "import requests\n",
        "from functools import lru_cache\n",
        "import sklearn\n",
        "\n",
        "@lru_cache(maxsize=10)\n",
        "def get_train_test_files():\n",
        "    TRAIN_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Test_For_Evaluation.csv'\n",
        "    EXTRA_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/germeval2018_a.txt'\n",
        "    TRAIN_FILE_LOCAL = 'Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE_LOCAL = 'Assessment1_Toxic_Test.csv'\n",
        "    EXTRA_FILE_LOCAL = 'germeval2018.csv'\n",
        "\n",
        "    def download(url, localfile):\n",
        "        with open(localfile, 'wb') as f:\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            f.write(r.content)\n",
        "\n",
        "    download(TRAIN_FILE, TRAIN_FILE_LOCAL)\n",
        "    download(TEST_FILE, TEST_FILE_LOCAL)\n",
        "    download(EXTRA_FILE, EXTRA_FILE_LOCAL)\n",
        "\n",
        "    return TRAIN_FILE_LOCAL, TEST_FILE_LOCAL, EXTRA_FILE_LOCAL\n",
        "\n",
        "def seed_random():\n",
        "    import numpy as np\n",
        "    import random\n",
        "    np.random.seed(0)\n",
        "    random.seed(0)\n",
        "\n",
        "sklearn.set_config(display=\"diagram\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrlJ1H1maDxM"
      },
      "source": [
        "# Functions to read the CSV and do basic cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVfYulLnuGVJ"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import demoji\n",
        "\n",
        "def remove_roles(line:str)->str:\n",
        "    # Remove texts like @USER, @MODERATOR etc\n",
        "    pat = re.compile(u'\\@[A-Za-z]+')\n",
        "    return re.sub(pat, '', line)\n",
        "\n",
        "def get_train_test_df():\n",
        "    train_csv, test_csv, extra_csv = get_train_test_files()\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "    extra_df = pd.read_csv(extra_csv)\n",
        "    return train_df, test_df, extra_df\n",
        "\n",
        "def remove_emojis(line:str)->str:\n",
        "    # Replace emojis with their description, eg __thumbs_down__\n",
        "    demoji_str = demoji.replace_with_desc(line, sep=\" ::: \")\n",
        "    if (demoji_str == line):\n",
        "        return line\n",
        "    \n",
        "    inEmoji = False\n",
        "    currentEmojiWords = []\n",
        "    allWords = []\n",
        "\n",
        "    def accumulate(word:str)->None:\n",
        "        nonlocal inEmoji\n",
        "        nonlocal currentEmojiWords\n",
        "        nonlocal allWords\n",
        "        if not inEmoji and word != \":::\":\n",
        "            allWords.append(word)\n",
        "        elif inEmoji:\n",
        "            if word == ':::':\n",
        "                currentEmoji = \"_\".join(currentEmojiWords)\n",
        "                currentEmoji = \"__\" + currentEmoji + \"__\"\n",
        "                allWords.append(currentEmoji)\n",
        "                currentEmojiWords = []\n",
        "            else:\n",
        "                currentEmojiWords.append(word)\n",
        "        else: # Not in emoji but ::: is true\n",
        "            inEmoji = True\n",
        "\n",
        "    [accumulate(word) for word in demoji_str.split()]\n",
        "\n",
        "    sentence = \" \".join(allWords)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def remove_ellipses(line:str)->str:\n",
        "    pat = re.compile(u'\\.\\.+')\n",
        "    return re.sub(pat, ' ', line)\n",
        "\n",
        "def to_lower(line:str)->str:\n",
        "    return line.lower()\n",
        "\n",
        "def replace_number_with_tag(line:str)->str:\n",
        "    line = re.sub(\"\\s\\d*((\\.|\\,)\\d+)?\\s\", \" nummer \", line)\n",
        "    line = re.sub('\\s\\d+$', '', line)\n",
        "    line = re.sub('^\\d+\\s', '', line)\n",
        "    return line\n",
        "\n",
        "def remove_urls(line:str)->str:\n",
        "    return re.sub('https?:\\/\\/\\S+', ' hyperlink ', line)\n",
        "\n",
        "def basic_clean(s:pd.Series)->pd.Series:\n",
        "    return s.map(to_lower)                                                  \\\n",
        "            .map(remove_emojis)                                             \\\n",
        "            .map(remove_roles)                                              \\\n",
        "            .map(remove_ellipses)                                           \\\n",
        "            .map(replace_number_with_tag)                                   \\\n",
        "            .map(remove_urls)\n",
        "\n",
        "def get_clean_train_test_df()->tuple:\n",
        "    train_df, test_df, extra_df = get_train_test_df()\n",
        "    train_df['comment_text'] = basic_clean(train_df['comment_text'])\n",
        "    test_df['comment_text'] = basic_clean(test_df['comment_text'])\n",
        "    extra_df['comment_text'] = basic_clean(extra_df['comment_text'])\n",
        "    return train_df, test_df, extra_df\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6_dto2JaR2f"
      },
      "source": [
        "# Clean using Spacy and Enrich"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8YSZZ4vxTk5"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string\n",
        "import spacy\n",
        "from spacymoji import Emoji\n",
        "import  de_core_news_sm\n",
        "\n",
        "def is_punct_only(token:str)->bool:\n",
        "    for c in list(token):\n",
        "        if c not in string.punctuation:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_same(l1:list, l2:list)->bool:\n",
        "    if (len(l1) != len(l2)):\n",
        "        return False\n",
        "    for x, y in zip(l1, l2):\n",
        "        if x != y:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def get_num_of_allcap_words(s:str)->int:\n",
        "    def is_allcaps(s:str)->bool:\n",
        "        if (len(s) < 3):\n",
        "            return False\n",
        "        for c in list(s):\n",
        "            if not (\\\n",
        "                    (ord(c) <=ord('Z') and ord(c) >= ord('A')) or           \\\n",
        "                    (ord(c) >= ord('0') and ord(c) <= ord('9'))             \\\n",
        "                    ):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    if len(s) < 3:\n",
        "        return 0\n",
        "    tokens = [w.strip() for w in s.split()]\n",
        "    return sum([1 for t in tokens if is_allcaps(t)])\n",
        "\n",
        "def get_percentage_of_excalamations(s:str)->float:\n",
        "    if len(s) == 0:\n",
        "        return 0.0\n",
        "    exclamation_count = sum([1 for c in list(s) if c == '!'])\n",
        "    return exclamation_count / len(s)\n",
        "\n",
        "\n",
        "def is_empty_string(s:str)->bool:\n",
        "    if s == '' or s == None:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def do_basic_nlp_cleaning(line:str)->str:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(line)\n",
        "\n",
        "    # Some tokens start with a punctuation, remove the first one\n",
        "    def remove_first_punctuation(tok:str)->str:\n",
        "        return                                                              \\\n",
        "            tok[1:]                                                         \\\n",
        "            if tok[0] in set(string.punctuation) and len(tok) != 0          \\\n",
        "            else tok\n",
        "\n",
        "    tokens = [remove_first_punctuation(w) for w in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"german\"))\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [w for w in tokens if not is_punct_only(w)]\n",
        "\n",
        "    # Stem words\n",
        "    stem = SnowballStemmer('german')\n",
        "    tokens = [stem.stem(w) for w in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def get_cleaning_function(remove_named_ents:bool=True, pos_tagging:bool=False):\n",
        "    nlp = de_core_news_sm.load()\n",
        "    emoji = Emoji(nlp)\n",
        "    nlp.add_pipe(emoji, first=True)\n",
        "    stopwords = spacy.lang.de.stop_words.STOP_WORDS\n",
        "\n",
        "    def do_basic_nlp_cleaning(line:str)->str:\n",
        "        def is_interesting_token(token, doc):\n",
        "            if token.pos_ in set(['NUM', 'SYM']):\n",
        "                return False\n",
        "            if remove_named_ents:\n",
        "                for e in doc.ents:\n",
        "                    for t in e:\n",
        "                        if token.text == t.text:\n",
        "                            return False\n",
        "            if token.text in stopwords:\n",
        "                return False\n",
        "            if (token.is_punct):\n",
        "                return False\n",
        "            #if token._.is_emoji:\n",
        "            #    return False\n",
        "            return True\n",
        "\n",
        "        def remove_terminal_punctuations(word):\n",
        "            word = word.strip()\n",
        "            while word != \"\" and word[0] in list(string.punctuation):\n",
        "                word = word[1:]\n",
        "            while word != \"\" and word[-1] in list(string.punctuation):\n",
        "                word = word[:-1]\n",
        "            return word\n",
        "\n",
        "        def get_final_string(tok, doc):\n",
        "            lemma = tok.lemma_.lower()\n",
        "            if pos_tagging:\n",
        "                lemma = lemma + \":\" + tok.pos_\n",
        "                lemma = lemma + \":\" + tok.tag_\n",
        "            return lemma\n",
        "\n",
        "        doc = nlp(line)\n",
        "        words = [get_final_string(tok, doc) for tok in doc if is_interesting_token(tok, doc)]\n",
        "        words = [remove_terminal_punctuations(word) for word in words]\n",
        "        words = [word for word in words if word != \"\"]\n",
        "        return  \" \".join(words)\n",
        "\n",
        "    return do_basic_nlp_cleaning\n",
        "\n",
        "def get_enriched_dataset(df):\n",
        "    cleaning_fn = get_cleaning_function(remove_named_ents=True, pos_tagging=True)\n",
        "    df['cleaned_comment_text'] = df['comment_text'].map(cleaning_fn)\n",
        "    df['n_all_caps'] = df['comment_text'].map(get_num_of_allcap_words)\n",
        "    df['perc_exclamations'] = df['comment_text'].map(get_percentage_of_excalamations)\n",
        "    df['num_exclamations'] = df['comment_text'].map(lambda s: sum([1 for x in list(s) if x == '!']))\n",
        "    return df\n",
        "\n",
        "import functools\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache(maxsize=128)\n",
        "def get_all_enriched_dfs_cached():\n",
        "    train_df, test_df, extra_df = get_clean_train_test_df()\n",
        "    train_df = get_enriched_dataset(train_df)\n",
        "    test_df = get_enriched_dataset(test_df)\n",
        "    extra_df = get_enriched_dataset(extra_df)\n",
        "    return train_df, test_df, extra_df\n",
        "    \n",
        "def get_all_enriched_dataframes():\n",
        "    tr, te, ex = get_all_enriched_dfs_cached()\n",
        "    return tr.copy(), te.copy(), ex.copy()\n",
        "\n",
        "train_df, test_df, extra_df = get_all_enriched_dfs_cached()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68DFX_fjYgQz"
      },
      "source": [
        "# Multinomial NB (original)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zL5Boe5qYfF4",
        "outputId": "12e2dd3d-e626-4f52-8fd4-9986041a5565"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "#from imblearn.pipeline import Pipeline\n",
        "#from imblearn.over_sampling import SMOTE \n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.preprocessing import DenseTransformer\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def get_feature_column_names(df):\n",
        "    return [cname for cname in df.columns if not cname.startswith('Sub')]\n",
        "\n",
        "def get_target_column_names(df):\n",
        "    return [cname for cname in df.columns if cname.startswith('Sub')]\n",
        "\n",
        "def is_text_column(colname:str)->bool:\n",
        "    if 'text' in colname:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_text_columns(df)->list:\n",
        "    return [cn for cn in df.columns if is_text_column(cn)]\n",
        "\n",
        "def get_nontext_columns(df)->list:\n",
        "    return [cn for cn in df.columns if not is_text_column(cn)]\n",
        "\n",
        "def run_classification(                                                     \\\n",
        "                       dataset:pd.DataFrame,                                \\\n",
        "                       extra_dataset:pd.DataFrame,                          \\\n",
        "                       test_dataset:pd.DataFrame,                           \\\n",
        "                       target_column:str,                                   \\\n",
        "                       clf_gen_fn,                                          \\\n",
        "                       grid_search_dict=None,                               \\\n",
        "                       use_extra_dataset=False,                             \\\n",
        "                       )->tuple:\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "\n",
        "    seed_random()\n",
        "\n",
        "    X = dataset[get_feature_column_names(dataset)]\n",
        "    y = dataset[target_column]\n",
        "    trainX, testX, trainY, testY = train_test_split(X, y, random_state=0)\n",
        "\n",
        "    if use_extra_dataset:\n",
        "        extraX = extra_dataset[get_feature_column_names(dataset)]\n",
        "        extray = extra_dataset[target_column]\n",
        "        trainX = pd.concat([trainX, extraX])\n",
        "        trainY = pd.concat([trainY, extray])\n",
        "        print(\"Added additional data from GermEval 2018\")\n",
        "\n",
        "    if None != grid_search_dict and isinstance(grid_search_dict, dict):\n",
        "        # TODO: Update grid for the vectorizers\n",
        "        # Right now, we're hitting RAM constraints if we turn these on\n",
        "        gridupd = {\n",
        "        }\n",
        "        grid_search_dict = grid_search_dict.copy()\n",
        "        grid_search_dict.update(gridupd)\n",
        "\n",
        "                        \n",
        "    column_trans = make_column_transformer(                                 \\\n",
        "                            (CountVectorizer(ngram_range=(1,1)), 'cleaned_comment_text'),   \\\n",
        "                            (TfidfVectorizer(use_idf=True), 'cleaned_comment_text'),    \\\n",
        "                            (TfidfVectorizer(use_idf=False), 'cleaned_comment_text'),    \\\n",
        "                            remainder=MinMaxScaler(),                       \\\n",
        "                        )\n",
        "    \n",
        "    classif_pipeline = Pipeline(                                        \\\n",
        "                            [                                           \\\n",
        "                                ('column_transformer', column_trans),   \\\n",
        "                                ('dense', DenseTransformer()),          \\\n",
        "                                ('clf', clf_gen_fn()),                  \\\n",
        "                            ])\n",
        "\n",
        "    if None != grid_search_dict and isinstance(grid_search_dict, dict):\n",
        "        search = GridSearchCV(classif_pipeline, grid_search_dict, cv=3, n_jobs=-1, scoring='f1')\n",
        "        search.fit(trainX, trainY)\n",
        "        classif_pipeline = search.best_estimator_\n",
        "        print(search.best_params_)\n",
        "        print(search.best_score_)\n",
        "    else:\n",
        "        classif_pipeline.fit(trainX, trainY)\n",
        "\n",
        "    y_pred = classif_pipeline.predict(testX)\n",
        "\n",
        "    return accuracy_score(testY, y_pred), f1_score(testY, y_pred), classif_pipeline\n",
        "\n",
        "def predict_on_test_set(\\\n",
        "                        dataset,\\\n",
        "                        colname,\\\n",
        "                        classif_pipeline):\n",
        "    seed_random()\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "    y_pred = classif_pipeline.predict(dataset)\n",
        "    return y_pred.tolist()\n",
        "\n",
        "def run_classifiers():\n",
        "\n",
        "    linearsvc_gen = lambda: LinearSVC()\n",
        "    linearsvc_paramgrid = {'clf__class_weight': [None, 'balanced'], 'clf__max_iter': [1000, 10000]}\n",
        "    classifiers = {\n",
        "        \"LinearSVC\": [\n",
        "                      \"Sub1_Toxic\",\n",
        "                      linearsvc_gen,\n",
        "                      linearsvc_paramgrid,\n",
        "                      True]\n",
        "    }\n",
        "\n",
        "    best_classifiers = list() # list of tuples: (columnname, clfname, pipeline)\n",
        "\n",
        "    for clfname, val in classifiers.items():\n",
        "        colname = val[0]\n",
        "        generator = val[1]\n",
        "        gridsearch = val[2]\n",
        "        use_extra_data = val[3]\n",
        "\n",
        "        train_df, test_df, extra_df = get_all_enriched_dfs_cached()\n",
        "        acc, f1, classif_pipeline = run_classification(\\\n",
        "            dataset=train_df,\\\n",
        "            extra_dataset=extra_df if use_extra_data else None,\\\n",
        "            test_dataset=test_df,\\\n",
        "            target_column=colname,\\\n",
        "            clf_gen_fn=generator,\\\n",
        "            grid_search_dict=gridsearch,\\\n",
        "            use_extra_dataset=use_extra_data)\n",
        "        \n",
        "        best_classifiers.append((colname, clfname, classif_pipeline))\n",
        "\n",
        "        print(colname, clfname, acc, f1,)\n",
        "\n",
        "\n",
        "    test_pred_df_dict = {\n",
        "        'comment_text': test_df['comment_text'].to_numpy().tolist()\n",
        "    }\n",
        "    for colname, clfname, clf_pipeline in best_classifiers:\n",
        "        train_df, test_df, extra_df = get_all_enriched_dfs_cached()\n",
        "        y_pred = predict_on_test_set(test_df, clfname, clf_pipeline)\n",
        "        temp_dict = {colname: y_pred}\n",
        "        test_pred_df_dict.update(temp_dict)\n",
        "        print(y_pred)\n",
        "        \n",
        "\n",
        "    result_df = pd.DataFrame(test_pred_df_dict)\n",
        "    return result_df, classif_pipeline\n",
        "\n",
        "seed_random()\n",
        "result_df, model = run_classifiers()\n",
        "\n",
        "model\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added additional data from GermEval 2018\n",
            "{'clf__class_weight': 'balanced', 'clf__max_iter': 10000}\n",
            "0.5167117866763667\n",
            "Sub1_Toxic LinearSVC 0.655819774718398 0.4701348747591522\n",
            "[0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0]\n",
            "                                         comment_text  Sub1_Toxic\n",
            "0   ziemlich traurig diese kommentare zu lesen. ih...           0\n",
            "1   sag ich doch, wir befeuern den klimawandel. ra...           0\n",
            "2   dummerweise haben wir in der eu und in der usa...           0\n",
            "3   \"so lange gewinnmaximierung vorrang hat, wird ...           0\n",
            "4   sollte es dann doch einen klimawandel geben, d...           0\n",
            "5    ja o.k aber nicht mit so schwachsinnigen dars...           1\n",
            "6    seltsamerweise steigen die temperaturen mit d...           1\n",
            "7    aus sicht der grã¼nen fraktion kurz ã¼berschl...           0\n",
            "8   wenn die industrie angeblich so extrem viel tu...           0\n",
            "9                  die luftqualitã¤t ist nicht besser           0\n",
            "10  schublade auf, schublade zu. zu mehr denkleist...           1\n",
            "11                  schon mal was von physik gehã¶rt?           0\n",
            "12   geh wieder pennen dein gesã¼lze interessiert ...           1\n",
            "13  wenn ich einen konjunktiv verwende hat das ein...           0\n",
            "14                          nach mir die sintflut â˜º           0\n",
            "15                                staatsfernsehen ! !           1\n",
            "16   oh weh das mã¶chten wir uns garnicht vorstellen.           0\n",
            "17  aber eure generation waren die schlimmsten ver...           1\n",
            "18  eine ernsthafte diskussion ist doch vã¶llig ã¼...           0\n",
            "19  fã¼r deutsche/einzahler gibt es wieder nix ! a...           0\n",
            "20  \" nummer milliarden!\" das ist dein denkfehler....           1\n",
            "21   geld ist genug da. oder mit volker pispers wo...           0\n",
            "22   wir bezahlen auch nur mit unserem vertrauen. ...           0\n",
            "23  war schon thema bei illner. kã¶nnt ihr euch ni...           0\n",
            "24  soll er keine rente erhalten? ãœbrigens bekomm...           0\n",
            "25  wie doof muãÿ man eigentlich sein,um diesen lã...           1\n",
            "26  passt zum thema friedmann gestern wo die cdu p...           0\n",
            "27  diese ankã¼ndigungen werden immer vor wahlen g...           0\n",
            "28                                 ohne kommentar!!!!           1\n",
            "29  wer zahlt den die steuern, nummer das ist das ...           1\n",
            "30  die \"rentenreform\" der spd ist ein schlechter ...           0\n",
            "31  die \"rentenreform\" der spd ist ein schlechter ...           0\n",
            "32  wie wã¤re es dann mit einem (bedingungslosen) ...           0\n",
            "33  und jede form des einkommens wird ab einer bes...           0\n",
            "34   nein, bei der finanzierung liegst du komplett...           0\n",
            "35   artikel nummer gg ist sicherlich bekannt. wob...           0\n",
            "36  3) unser geldsystem (euro, eurokrise, alternat...           0\n",
            "37  \"das geld dafã¼r kann man doch einfach drucken...           0\n",
            "38  zu dem verweise ich gerne auf meine facebookse...           0\n",
            "39  die idee einer grundrente kommt von der linksp...           1\n",
            "40  sehr gut analysiert - bin ich vollkommen konfo...           0\n",
            "41   noch mehr bã¼rokratie? gleiches geld fã¼r all...           0\n",
            "42         alle ausreisepflichtige flã¼chtlinge raus.           0\n",
            "43   doch wurde und wird was verteilt , die diã¤te...           0\n",
            "44                                       wieso schule           0\n",
            "45  es ist fatal die einen hilfesuchenden menschen...           0\n",
            "46  ich bin ganz klar fã¼r die besteuerung. nur mã...           0\n",
            "47  also eine gleiche rente fã¼r alle? wie wã¤re e...           0\n",
            "48  , warum wã¤hlt man eine partei, die den tod vo...           0\n",
            "49  wie wã¤re es mit der steuerpolitik der partei ...           0\n",
            "50  gestern bei illner, montag bei nummer  ist das...           0\n",
            "51  mein gott der war erst gestern bei illner. die...           1\n",
            "52   die cdu lã¤sst das so wie so nicht zu . sagen...           1\n",
            "53  bei meiner beschissenen rente als 2x geschiede...           1\n",
            "54  wer nummer jahre zum mindestlohn arbeiten muãÿ...           0\n",
            "55  wenn ich immer lese \"das muss die jã¼ngere gen...           0\n",
            "56  das machen wir doch. und deswegen zerstã¶ren w...           0\n",
            "57  die jã¼ngere generation bekommt so viel kinder...           0\n",
            "58                       keine sorge, sie wussten es.           0\n",
            "59  gute kritik, doch wir brauchen nicht nur numme...           0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style>#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c {color: black;background-color: white;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c pre{padding: 0;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-toggleable {background-color: white;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-estimator:hover {background-color: #d4ebff;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-item {z-index: 1;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-parallel-item:only-child::after {width: 0;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-e2299a48-5c11-48bd-88cb-9227c4f0218c div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-e2299a48-5c11-48bd-88cb-9227c4f0218c\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"34344667-8cbf-4731-9cbe-1ca49643eeac\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"34344667-8cbf-4731-9cbe-1ca49643eeac\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('column_transformer',\n",
              "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                                   transformers=[('countvectorizer',\n",
              "                                                  CountVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-1',\n",
              "                                                  TfidfVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-2',\n",
              "                                                  TfidfVectorizer(use_idf=False),\n",
              "                                                  'cleaned_comment_text')])),\n",
              "                ('dense', DenseTransformer()),\n",
              "                ('clf', LinearSVC(class_weight='balanced', max_iter=10000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"9b23a4d4-98ed-4ef8-8924-5ec0f4659831\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"9b23a4d4-98ed-4ef8-8924-5ec0f4659831\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                  transformers=[('countvectorizer', CountVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-1', TfidfVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-2',\n",
              "                                 TfidfVectorizer(use_idf=False),\n",
              "                                 'cleaned_comment_text')])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"25172176-2ba6-4952-8eb9-0ebc957e5669\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"25172176-2ba6-4952-8eb9-0ebc957e5669\">countvectorizer</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2164a8d3-3e80-49a5-b904-c4aa3fb24395\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"2164a8d3-3e80-49a5-b904-c4aa3fb24395\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b23beb11-b70f-428f-8c94-daff14663c4a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"b23beb11-b70f-428f-8c94-daff14663c4a\">tfidfvectorizer-1</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"26153233-af5d-488f-9ac2-d1ca1575de71\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"26153233-af5d-488f-9ac2-d1ca1575de71\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"1da85050-072d-4d67-beff-a94fe98dfd3a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"1da85050-072d-4d67-beff-a94fe98dfd3a\">tfidfvectorizer-2</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f5937131-99b4-4658-86d4-821ea46d8297\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"f5937131-99b4-4658-86d4-821ea46d8297\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(use_idf=False)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"58ffbf15-e50a-4a72-a0be-17aaaf3a6fab\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"58ffbf15-e50a-4a72-a0be-17aaaf3a6fab\">remainder</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps', 'perc_exclamations', 'num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"ba31fee6-d3d9-4143-95aa-9281d9f3d69d\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"ba31fee6-d3d9-4143-95aa-9281d9f3d69d\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"272cd64b-c4d1-4a88-9aa9-93a687ed4f59\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"272cd64b-c4d1-4a88-9aa9-93a687ed4f59\">DenseTransformer</label><div class=\"sk-toggleable__content\"><pre>DenseTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"d27ee509-5c24-4a73-993b-9b9c35768529\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"d27ee509-5c24-4a73-993b-9b9c35768529\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(class_weight='balanced', max_iter=10000)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('column_transformer',\n",
              "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                                   transformers=[('countvectorizer',\n",
              "                                                  CountVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-1',\n",
              "                                                  TfidfVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-2',\n",
              "                                                  TfidfVectorizer(use_idf=False),\n",
              "                                                  'cleaned_comment_text')])),\n",
              "                ('dense', DenseTransformer()),\n",
              "                ('clf', LinearSVC(class_weight='balanced', max_iter=10000))])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xtp9t15rluQH"
      },
      "source": [
        "train_df, test_df, extra_df = get_all_enriched_dfs_cached()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqmr-94Goyr1",
        "outputId": "13e86f31-17f0-4482-d4b3-5bc0fe331d35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "'comment_text'].to_numpy().tolist()test_df["
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ziemlich traurig diese kommentare zu lesen. ihr kã¶nnt euch zwar belã¼gen, dass es den vom menschen gemachten klimawandel nicht gibt, nur kann man die natur nicht belã¼gen. wie viele menschen mã¼ssen denn noch auf grund des klimawandels ihre lebensgrundlage verlieren oder gar sterben, bis ihr den ernst der lage erkannt habt?',\n",
              " 'sag ich doch, wir befeuern den klimawandel. raucher kã¶nnen ihr lebensende meiner meinung nach auch gerne befeuern, nur hab ich daran kein interesse.',\n",
              " 'dummerweise haben wir in der eu und in der usa einen viel hã¶heren co2 fuãÿabdruck als z.b. die afrikaner oder inder.',\n",
              " '\"so lange gewinnmaximierung vorrang hat, wird sich das nur schleppend ã¤ndern\" da gebe ich dir recht.',\n",
              " 'sollte es dann doch einen klimawandel geben, der unabhã¤ngig vom menschen stattfindet? lernt er nichts von periodischen klimaverã¤nderungen?',\n",
              " ' ja o.k aber nicht mit so schwachsinnigen darstellungen. alles weg atomenergie autoindustrie landwirtschaft lebensmittelindustrie usw wovon sollen sie menschen leben ? wens nach den grã¼nen geht wieder zurã¼ck in die steinzeit. rom wurde auch nicht an einen tag erbaut',\n",
              " ' seltsamerweise steigen die temperaturen mit der sauberen luft. seit ã¼ber nummer jahren verbessert sich die luftqualitã¤t, seither steigen gleichzeitig die durchschnittliche mitteltemperatur.',\n",
              " ' aus sicht der grã¼nen fraktion kurz ã¼berschlagen die fossilen energiestoffe, der verbrennungsmotor, die industrielle landwirtschaft, die grenzen, schiffs- und flugverkehr, fleischverzehr, usw. reicht das erstmal?',\n",
              " 'wenn die industrie angeblich so extrem viel tut, frage ich mich, warum immer alles schlimmer wird.',\n",
              " 'die luftqualitã¤t ist nicht besser',\n",
              " 'schublade auf, schublade zu. zu mehr denkleistung reicht es wohl bei dir nicht.',\n",
              " ' schon mal was von physik gehã¶rt?',\n",
              " ' geh wieder pennen dein gesã¼lze interessiert mich nicht',\n",
              " 'wenn ich einen konjunktiv verwende hat das einen grund liebe hysterischen klimafachleute,die ihr hier ja alle wohl seid.lauter srudierte experten,die es ganz genau wissen. weil sie es gelesen habenðÿ˜\\x81',\n",
              " 'nach mir die sintflut â˜º',\n",
              " ' staatsfernsehen ! !',\n",
              " ' oh weh das mã¶chten wir uns garnicht vorstellen.',\n",
              " 'aber eure generation waren die schlimmsten verbrecher was umweltschutz angeht. vielleicht sollte dafã¼r jetzt auch die konsequenzen gezogen werden indem man eine ã–ko-steuer auf die renten setzt. 5-10% der brutto-rente wã¤re da schon angebracht. oder sollen dafã¼r ihre enkel und urenkel aufkommen?',\n",
              " 'eine ernsthafte diskussion ist doch vã¶llig ã¼berflã¼ssig. entweder wird alles abgeschaltet oder abgeschafft, was der grã¼nen fraktion nicht beliebt, oder es wird nicht ausreichend gehandelt. das ist die mentalitã¤t der neuen trendlemminge aus dem sektor klimaschutz.',\n",
              " 'fã¼r deutsche/einzahler gibt es wieder nix ! arbeit rechnet sich fã¼r viele normale arbeiter nicht mehr nummer so isses korrekt ! wer zum mindestlohn ackern muãÿ , hat bei stk nummer netto grad 1147â‚¬ ! alg1 dann ca 769â‚¬ und kriegt nach nummer jahren 514â‚¬ rente . das ist fakt und krank ðÿ¤¬ das geld ist da , wird aber hier gebunkert ðÿ¤¬  hyperlink   hyperlink ',\n",
              " '\" nummer milliarden!\" das ist dein denkfehler. auf was fã¼r einnamen kann denn der staat deutschland zurã¼ck greifen?',\n",
              " ' geld ist genug da. oder mit volker pispers worten mal zu sagen:\" geld ist eine fantasie\"',\n",
              " ' wir bezahlen auch nur mit unserem vertrauen. mehr nicht. nummer wurde der goldstandart beendet. einen direkten gegenwert hat geld nicht.',\n",
              " 'war schon thema bei illner. kã¶nnt ihr euch nicht abstimmen? am besten noch die selben leute einladen.',\n",
              " 'soll er keine rente erhalten? ãœbrigens bekommt auch jeder beamter geld von uns steuerzahlern',\n",
              " 'wie doof muãÿ man eigentlich sein,um diesen lã¼gnern dieses sozialpaket ab zunehmen,wer hat denn die agenta nummer auf den weh gebracht.und nun merken die verbrecher auf einmal,au, uns laufen ja die wã¤hler weg, besonders im osten. man leute , wacht doch endlich auf,wie oft lasst ihr euch noch verarschen. die haben einfach angst.',\n",
              " 'passt zum thema friedmann gestern wo die cdu politikerin sagte das gering verdiener sich immobilen kaufen sollen damit sie spã¤ter nicht bedã¼rftig werden ðÿ‘žðÿ‘žðÿ‘žðÿ˜¡ðÿ˜¡ðÿ˜¡',\n",
              " 'diese ankã¼ndigungen werden immer vor wahlen gemacht und gehalten wird nichts. wann begreift der bã¼rger das entlich mal .',\n",
              " ' ohne kommentar!!!!',\n",
              " 'wer zahlt den die steuern, nummer das ist das geld der allgemeinheit, oder sehe ich das falsch,,,??',\n",
              " 'die \"rentenreform\" der spd ist ein schlechter scherz. warum sollten nur renter die lã¤nger als nummer jahre gearbeitet haben eine grundrente von nummer euro bekommen, was noch unterhalb der armutsgrenze liegt? warum sollen rentner, die weniger gearbeitet haben in noch grã¶ãÿerer armut leben mã¼ssen?',\n",
              " 'die \"rentenreform\" der spd ist ein schlechter scherz und damit wollen sie sich wã¤hler kaufen. sie werden 1. ihre wahlversprechen nicht einhalten und 2. warum sollten nur rentner, die nummer jahre lang gearbeitet haben, eine grundrente von nummer euro beziehen, was noch unterhalb der armutsgrenze liegt und rentner, die weniger gearbeitet haben, noch weniger rente beziehen und mit noch weniger geld vor sich hin wegitieren? jeder mensch hat einen anspruch auf ein meschenwã¼rdiges leben und teilhabe an der gesellschaft. fã¼r mich ist das absolute minimumg eine sanktionsfreie mindestsicherung von 1050â‚¬ nach dem vorschlag der partei die linke. besser finde ich aber das grundeinkommenszonzept der bag grundeinkommen die linke.',\n",
              " 'wie wã¤re es dann mit einem (bedingungslosen) grundeinkommen? es wã¼rde jeder erwachsene bekommen und dann kann jeder entscheiden, wie er sein leben gestalten mã¶chte.',\n",
              " 'und jede form des einkommens wird ab einer bestimmten hã¶he besteuert. details gibt es z.b. bei der bag grundeinkommen.',\n",
              " ' nein, bei der finanzierung liegst du komplett falsch. ich verweise mal mal auf die ausarbeitung der bag grundeinkommen und auf meine zusammenstellung ã¼ber das grundlegende wissen ã¼ber unser geld- und wirtschaftssystem. da vereise ich auf heiner flassbeck mit seinem makroskop und heinz josef bontrup, die selber aber keine verfechter des grundeinkommen sind. weitere gute quellen ã¼ber unser geld- und wirtschaftssystem sind bernd senf. mr dax dirk mã¼ller und sahra wagenknecht ergã¤nzend empfhele ich dazu harald lesch, speziell zum thema kapitalozã¤n, christian felber zum thema geneinwohlã¶konomie und ergã¤nzend richard david precht. und natã¼rlich gibt es noch einige weitere spannende quellen. \"wã¼rden die menschen unser geldsystem verstehen, dann gã¤be es eine revolution noch vor morgen frã¼h\" henry ford',\n",
              " ' artikel nummer gg ist sicherlich bekannt. wobei artikel nummer der bayrischen verfassung viel witziger ist. mal ein paar beispiele. die reichsten 10! menschen in deutschland verfã¼gen ã¼ber ca. nummer milliarden euro. die reichsten nummer menschen verfã¼gen ã¼ber ein vermã¶gen von nummer milliarden euro. das reichste 1% der menschen in deutschland verfã¼gt ã¼ber 33% des gesamtvermã¶gens. die reichsten 10% der menschen in deutschland verfã¼gen ã¼ber 66% des gesamten vermã¶gens. das problem bei diesen zahlen sind, dass es schã¤tzungen sind. nummer wurde die vermã¶genssteuer ausgesetzt, weil die vermã¶gen nicht ausreichend erfasst werden. bis heute hat sich daran nichts geã¤ndert. dem einen sein vermã¶gen sind des anderen seine schulden. 16,8% der menschen in deutschland leben unterhalb der armutsgrenze. und je grã¶ãÿer das vermã¶gen, desto grã¶ãÿer die zinssummen, die wir arbeitende bevã¶lkerung erwirtschaften mã¼ssen. die reichen haben die macht ihre steuern zu â€žoptimierenâ€œ sei es durch gekaufte gesetze oder durch steueroasen. panamapapers, paradisepapers und wie die anderen steuerschlupflã¶cher alle heiãÿen.',\n",
              " '3) unser geldsystem (euro, eurokrise, alternative geldsysteme, bge, umverteilung, gemeinwohlã¶konomie) ****************************************************************** - volker pispers ã¼ber unser geldsystem  hyperlink  - wie geld funktioniert - einfach erklã¤rt  hyperlink  - mr. dax dirk mã¼ller ã¼ber unser geldsystem  hyperlink  - dirk mã¼ller ã¼ber unser geldsystem  hyperlink  - 23.01.2013 bernd senf: zinsesszins, geldschã¶pfung und spekutaltin  hyperlink  - planet wissen: der chiemgauer  hyperlink  - 06.01.2019 weltwirtschaftskrise nummer - silvio gesell & \"das wunder von wã¶rgl\" - experiment regionalgeld  hyperlink  - heiner flassbeck: ist der kapitalismus am ende?  hyperlink  - 04.02.2017 was ist los in europa? podium mit flassbeck (makroskop) und lã¼cke (ifw) in kiel  hyperlink  - andreas popp - wir sind nicht verschuldet.  hyperlink  alternative geldsysteme ******************** - die kommende demokratie: sozialismus nummer  hyperlink  - bruno kreisky forum: sahra wagenknecht reichtum ohne gier (13.6.2016)  hyperlink  - andreas popp â€“ plan b  hyperlink  - gã¼ltige stimme - andreas popp im verhã¶r bei roland dã¼ringer  hyperlink  - dirk mã¼ller auf dem jahrestreffen des diplomatic council - fã¼r ein faires finanzsystem  hyperlink  - gemeinwohl-ã–konomie - christian felber: wirtschaft neu denken  hyperlink  ãœber den euro und die eurokrise --------------------------------------------------------------------------- - heiner flassbeck: warum die rettung europas nicht gelingen kann (der euro und warum er nicht funktioniert)  hyperlink  - vortrag heiner flassbeck (24.11.2017, haus der eu wien)  hyperlink  - heinz-josef bontrup heinz-josef bontrup: die mainstream-ã–konomie und ihr versagen  hyperlink  - kenfm im gesprã¤ch mit: heinz-josef bontrup  hyperlink  - die griechenlandlã¼ge  hyperlink  - macht ohne kontrolle â€“ die troika  hyperlink  - macht ohne kontrolle: die troika - jung & naiv: folge nummer  hyperlink  - eu poly - ein europã¤ischer alptraum  hyperlink  - ard doku der groãÿe euro schwindel wie der ganze irrsinn begann  hyperlink  - die exportlãœge nummer - wie deutschland sich in den abgrund fã„hrt! - hans-werner sinn, gã¼nter verheugen  hyperlink  - die anstalt â€“ die exportwippe  hyperlink  - 27.08.2018 kontraste: die opfer des exportweltmeisters - wie deutschland arbeitslosigkeit in europa produziert  hyperlink  - sendung ard monitor bankenrettung  hyperlink  - staatsgeheimnis bankenrettung arte doku  hyperlink  bedingungsloses) grundeinkommen (bge) *********************************** - 08.07.2015 bielefelder parteitag der partei â€ždie linke\": diskussion zum bedingungslosen grundeinkommen  hyperlink  - 02.09.2012 argumentationen gegen ein bedingungsloses grundeinkommen  hyperlink  - 11.12. nummer prof. dr. sascha liebermann - bedingungsloses grundeinkommen  hyperlink  - 18.05.2017 digitalisierung und grundeinkommen â€“ mit richard david precht  hyperlink  - 11.12.2017 prof. dr. sascha liebermann - bedingungsloses grundeinkommen  hyperlink  - netzwerk grundeinkommen  hyperlink  - detailierte informationen ã¼ber den vorschlag die linke ã¼ber ein grundeinkommen  hyperlink  - bag grundeinkommen die linke  hyperlink  sonstiges: ******** - die anstalt: thema umverteilung  hyperlink  - kenfm-positionen 5: geld regiert die welt - welchen anteil am elend hat der zins?  hyperlink  - richard david precht | palais dresden | wie werden wir leben? gemeinwohl-ã–konomie, bedingungsloses grundeinkommen  hyperlink  - \"wirtschaft ohne wachstum\" prof. m. kennedy, dirk mã¼ller, prof. h. peukert, prof. h. spehl (1/2) \"  hyperlink  - wirtschaft ohne wachstum\" prof. m. kennedy, dirk mã¼ller, prof. h. peukert, prof. h. spehl (2/2)  hyperlink  - ulrike guerot ã¼ber die utopie der \"europã¤ischen republik\" - jung & naiv: folge nummer  hyperlink  ?v=virdzjbbm5g&index=9&list=plhuamix_u9z1oanh2uwlctugrh_csuje- - kenfm im gesprã¤ch mit: heinz-josef bontrup  hyperlink  - 03.09.2011 finanzgeschã¤fte nummer a erklã¤rt  hyperlink  - 02.05.2018 1. mai in witten: rede von prof. heinz-j. bontrup  hyperlink  - 23.10. nummer ernst wolff die krise am horizont, zehn jahre nach der finanzkrise-keines der probleme gelã¶st.  hyperlink  - 06.08.2017 kenfm-positionen 11: der globale marshallplan - konzepte fã¼r eine welt von morgen  hyperlink  - 18.01.2017 bernd senf â€“ der nebel um das geld  hyperlink  - 27.01.2018 prof. bontrup - wirtschaftliche macht  hyperlink ',\n",
              " '\"das geld dafã¼r kann man doch einfach drucken. weggedieren sie weiterâ€¼ï¸\\x8f\" ich verweise auf die grundegelde funktionsweise unseres geldsystems und die ausarbeitung der bag grundeinkomen die linke.',\n",
              " 'zu dem verweise ich gerne auf meine facebookseite \"neu denken\"',\n",
              " 'die idee einer grundrente kommt von der linkspartei. die spd hat jahre lang gegen die grundrente gewettert. die spd versucht so ihren arsch zu retten. ich finde die grundrente sollte steuerfrei sein und zum leben reichen. also nicht unter nummer euro.',\n",
              " 'sehr gut analysiert - bin ich vollkommen konform ! aber nummer bitte das nã¤chste mal mit einer orthographischen, einigermaãÿen korrekten, schreibweise danke!',\n",
              " ' noch mehr bã¼rokratie? gleiches geld fã¼r alle und fertig!',\n",
              " ' alle ausreisepflichtige flã¼chtlinge raus.',\n",
              " ' doch wurde und wird was verteilt , die diã¤ten der abgeordneten . da sollte man 40% streichen dann hã¤tten die immer noch zuviel',\n",
              " ' wieso schule',\n",
              " 'es ist fatal die einen hilfesuchenden menschen, gegen andere hilfesuchende menschen auszuspielen. anstatt nach unten zu treten mã¼ssen wir endlich nach oben die die augen unserer ausbeuter schauen.',\n",
              " 'ich bin ganz klar fã¼r die besteuerung. nur mã¼ssen einkommen bis nummer euro entlastet werden und darã¼ber hinaus wesentlich stã¤rker belastet. speziell bei renten benã¶tigen wir nicht nur eine untergrenze, sondern auch eine obergrenze.',\n",
              " 'also eine gleiche rente fã¼r alle? wie wã¤re es mit einem bge von nummer euro und wer mehr haben mã¶chte, der muss arbeiten gehen und oder einen teil des geldes, was er durch arbeit verdient hat zur seite legen.',\n",
              " ', warum wã¤hlt man eine partei, die den tod von menschen in deutschland willentlich in kauf nimmt?',\n",
              " 'wie wã¤re es mit der steuerpolitik der partei die linke, die einkommen bis 7.100â‚¬ entlasten und die einkommen darã¼ber mehr belasten mã¶chte.',\n",
              " 'gestern bei illner, montag bei nummer  ist das nicht langsam ã–r-parteihilfe wie bei den grã¼nlingen ? ðÿ¤¦â€\\x8dâ __trade_mark__',\n",
              " 'mein gott der war erst gestern bei illner. die redaktionen versagen.',\n",
              " ' die cdu lã¤sst das so wie so nicht zu . sagen doch nur wenn sie reichen was bekommen sollen ( nummer milliarden fã¼r die hotellobby)',\n",
              " 'bei meiner beschissenen rente als 2x geschiedener mann muss ich auch noch steuern und krankenkasse bezahlen. auf meine privaten geldanlagen gab und gibt es auch keine zinsen . ihr solltet euch schã¤men und in hã¶lle braten,,,,,,ðÿ __trade_mark__ __šðÿ__ __trade_mark__ __ˆðÿ__ __trade_mark__',\n",
              " 'wer nummer jahre zum mindestlohn arbeiten muãÿ, erhã¤lt 514â‚¬ rente dafã¼r nummer das ist doch das grundproblem ðÿ¤¦â€\\x8dâ __trade_mark__ __€ï¸\\x8f_wir_steuern_somit_sehenden_auges_in_die_massenaltersarmut_!_habe_selbst_jetzt_jahrelang_zum_mindestlohn_gearbeitet_,_netto_1147â‚¬_bei_stkl.1_._ergebnis_ist_jetzt_ein_alg1_von_769â‚¬_._500.000_anderen_beschã¤ftigten_im_callcenter_geht_es_da_auch_so_ und_da_hilft_kein_gesabbel_!_lã¶sungen_mã¼ssen_her_!_ðÿ¤¦â€\\x8dâ__ __trade_mark__',\n",
              " 'wenn ich immer lese \"das muss die jã¼ngere generaltion\" bezahlen, dann fall ich immer lachend vom stuhl. ja, es kã¶nnen schulden enstehen. aber was stehen diesen schulden denn gegenã¼ber? richtig! es ensteht die exakt selbe menge an guthaben. und dass kann man steuern. und deswegen nennt man die steuern auch steuern um die geldmenge zu stuern.',\n",
              " 'das machen wir doch. und deswegen zerstã¶ren wir ja die eurozone und viele lã¤nder berschweren sich ã¼ber unsere auãÿenhandelsbilanzã¼berschã¼sse. ins detail geht da z.b. heiner flassbeck mit seinem makroskop, und heinz joesf bontrup. ergã¤nzende informationen gibt es von bernd senf und mr. dax dirk mã¼ller. wer dann noch langeweile hat, der sollte sich mit dem \"reichtum ohne gier\" von sahra wagenknecht und der gemeinwohlã¶konomie von christian felber auseinander setzen. natã¼rlich sollte man sich voher gundlegend ã¼ber unser geldsystem infomieren. und keine sorge, ich habe eine ã¼berwiegend leicht zu verstehende infomrationssammlung zu diesem thema zusammengestellt. gerne kannst du auf meiner facebookseite \"neu denken\" vorbei schauen.',\n",
              " 'die jã¼ngere generation bekommt so viel kindergeld wie keine generation. dã¼rfen auch was zurã¼ck geben',\n",
              " 'keine sorge, sie wussten es.',\n",
              " 'gute kritik, doch wir brauchen nicht nur nummer euro mindestlohn (12,68â‚¬ pro stunde benã¶tigt man laut bundesregierung um nicht auf hartz4 angewiesen sein zu mã¼ssen) sondern wir benã¶tigen auch eine sanktionsfreie mindestsicherung, oder ein grundeinkommen, fã¼r all diejenigen, die kaum oder gar nicht gearbeitet haben bzw. konnten. jeder mensch hat das recht auf ein menschenwã¼rdiges leben mit teilhabe an der gesellschaft.']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YfOUOuIozv5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}