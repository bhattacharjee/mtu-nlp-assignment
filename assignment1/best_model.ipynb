{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "best_model.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNE7ejVNGsAXtEtx0rUyENI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment1/best_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERvBOEn0wss6"
      },
      "source": [
        "!pip install spacy  nltk spacymoji huggingface -q       >/dev/null 2>&1         \n",
        "!pip install -q -U tensorflow-text                      >/dev/null 2>&1\n",
        "!pip install -q tf-models-official                      >/dev/null 2>&1\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "!pip install transformers                               >/dev/null 2>&1\n",
        "\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "\n",
        "!pip install mlxtend                                    >/dev/null 2>&1\n",
        "!pip install imblearn                                   >/dev/null 2>&1\n",
        "\n",
        "# handling emojis\n",
        "!pip install demoji                                     >/dev/null 2>&1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-DAX9VLZlw3"
      },
      "source": [
        "import requests\n",
        "from functools import lru_cache\n",
        "import sklearn\n",
        "\n",
        "@lru_cache(maxsize=10)\n",
        "def get_train_test_files():\n",
        "    TRAIN_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Test_For_Evaluation.csv'\n",
        "    EXTRA_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/germeval2018_a.txt'\n",
        "    TRAIN_FILE_LOCAL = 'Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE_LOCAL = 'Assessment1_Toxic_Test.csv'\n",
        "    EXTRA_FILE_LOCAL = 'germeval2018.csv'\n",
        "\n",
        "    def download(url, localfile):\n",
        "        with open(localfile, 'wb') as f:\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            f.write(r.content)\n",
        "\n",
        "    download(TRAIN_FILE, TRAIN_FILE_LOCAL)\n",
        "    download(TEST_FILE, TEST_FILE_LOCAL)\n",
        "    download(EXTRA_FILE, EXTRA_FILE_LOCAL)\n",
        "\n",
        "    return TRAIN_FILE_LOCAL, TEST_FILE_LOCAL, EXTRA_FILE_LOCAL\n",
        "\n",
        "def seed_random():\n",
        "    import numpy as np\n",
        "    import random\n",
        "    np.random.seed(0)\n",
        "    random.seed(0)\n",
        "\n",
        "sklearn.set_config(display=\"diagram\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrlJ1H1maDxM"
      },
      "source": [
        "# Functions to read the CSV and do basic cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVfYulLnuGVJ"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import demoji\n",
        "\n",
        "def remove_roles(line:str)->str:\n",
        "    # Remove texts like @USER, @MODERATOR etc\n",
        "    pat = re.compile(u'\\@[A-Za-z]+')\n",
        "    return re.sub(pat, '', line)\n",
        "\n",
        "def get_train_test_df():\n",
        "    train_csv, test_csv, extra_csv = get_train_test_files()\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "    extra_df = pd.read_csv(extra_csv)\n",
        "    return train_df, test_df, extra_df\n",
        "\n",
        "def remove_emojis(line:str)->str:\n",
        "    # Replace emojis with their description, eg __thumbs_down__\n",
        "    demoji_str = demoji.replace_with_desc(line, sep=\" ::: \")\n",
        "    if (demoji_str == line):\n",
        "        return line\n",
        "    \n",
        "    inEmoji = False\n",
        "    currentEmojiWords = []\n",
        "    allWords = []\n",
        "\n",
        "    def accumulate(word:str)->None:\n",
        "        nonlocal inEmoji\n",
        "        nonlocal currentEmojiWords\n",
        "        nonlocal allWords\n",
        "        if not inEmoji and word != \":::\":\n",
        "            allWords.append(word)\n",
        "        elif inEmoji:\n",
        "            if word == ':::':\n",
        "                currentEmoji = \"_\".join(currentEmojiWords)\n",
        "                currentEmoji = \"__\" + currentEmoji + \"__\"\n",
        "                allWords.append(currentEmoji)\n",
        "                currentEmojiWords = []\n",
        "            else:\n",
        "                currentEmojiWords.append(word)\n",
        "        else: # Not in emoji but ::: is true\n",
        "            inEmoji = True\n",
        "\n",
        "    [accumulate(word) for word in demoji_str.split()]\n",
        "\n",
        "    sentence = \" \".join(allWords)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def remove_ellipses(line:str)->str:\n",
        "    pat = re.compile(u'\\.\\.+')\n",
        "    return re.sub(pat, ' ', line)\n",
        "\n",
        "def to_lower(line:str)->str:\n",
        "    return line.lower()\n",
        "\n",
        "def replace_number_with_tag(line:str)->str:\n",
        "    line = re.sub(\"\\s\\d*((\\.|\\,)\\d+)?\\s\", \" nummer \", line)\n",
        "    line = re.sub('\\s\\d+$', '', line)\n",
        "    line = re.sub('^\\d+\\s', '', line)\n",
        "    return line\n",
        "\n",
        "def remove_urls(line:str)->str:\n",
        "    return re.sub('https?:\\/\\/\\S+', ' hyperlink ', line)\n",
        "\n",
        "def basic_clean(s:pd.Series)->pd.Series:\n",
        "    return s.map(to_lower)                                                  \\\n",
        "            .map(remove_emojis)                                             \\\n",
        "            .map(remove_roles)                                              \\\n",
        "            .map(remove_ellipses)                                           \\\n",
        "            .map(replace_number_with_tag)                                   \\\n",
        "            .map(remove_urls)\n",
        "\n",
        "def get_clean_train_test_df()->tuple:\n",
        "    train_df, test_df, extra_df = get_train_test_df()\n",
        "    train_df['comment_text'] = basic_clean(train_df['comment_text'])\n",
        "    test_df['comment_text'] = basic_clean(test_df['comment_text'])\n",
        "    extra_df['comment_text'] = basic_clean(extra_df['comment_text'])\n",
        "    return train_df, test_df, extra_df\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6_dto2JaR2f"
      },
      "source": [
        "# Clean using Spacy and Enrich"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8YSZZ4vxTk5"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string\n",
        "import spacy\n",
        "from spacymoji import Emoji\n",
        "import  de_core_news_sm\n",
        "\n",
        "def is_punct_only(token:str)->bool:\n",
        "    for c in list(token):\n",
        "        if c not in string.punctuation:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_same(l1:list, l2:list)->bool:\n",
        "    if (len(l1) != len(l2)):\n",
        "        return False\n",
        "    for x, y in zip(l1, l2):\n",
        "        if x != y:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def get_num_of_allcap_words(s:str)->int:\n",
        "    def is_allcaps(s:str)->bool:\n",
        "        if (len(s) < 3):\n",
        "            return False\n",
        "        for c in list(s):\n",
        "            if not (\\\n",
        "                    (ord(c) <=ord('Z') and ord(c) >= ord('A')) or           \\\n",
        "                    (ord(c) >= ord('0') and ord(c) <= ord('9'))             \\\n",
        "                    ):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    if len(s) < 3:\n",
        "        return 0\n",
        "    tokens = [w.strip() for w in s.split()]\n",
        "    return sum([1 for t in tokens if is_allcaps(t)])\n",
        "\n",
        "def get_percentage_of_excalamations(s:str)->float:\n",
        "    if len(s) == 0:\n",
        "        return 0.0\n",
        "    exclamation_count = sum([1 for c in list(s) if c == '!'])\n",
        "    return exclamation_count / len(s)\n",
        "\n",
        "\n",
        "def is_empty_string(s:str)->bool:\n",
        "    if s == '' or s == None:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def do_basic_nlp_cleaning(line:str)->str:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(line)\n",
        "\n",
        "    # Some tokens start with a punctuation, remove the first one\n",
        "    def remove_first_punctuation(tok:str)->str:\n",
        "        return                                                              \\\n",
        "            tok[1:]                                                         \\\n",
        "            if tok[0] in set(string.punctuation) and len(tok) != 0          \\\n",
        "            else tok\n",
        "\n",
        "    tokens = [remove_first_punctuation(w) for w in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"german\"))\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [w for w in tokens if not is_punct_only(w)]\n",
        "\n",
        "    # Stem words\n",
        "    stem = SnowballStemmer('german')\n",
        "    tokens = [stem.stem(w) for w in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def get_cleaning_function(remove_named_ents:bool=True, pos_tagging:bool=False):\n",
        "    nlp = de_core_news_sm.load()\n",
        "    emoji = Emoji(nlp)\n",
        "    nlp.add_pipe(emoji, first=True)\n",
        "    stopwords = spacy.lang.de.stop_words.STOP_WORDS\n",
        "\n",
        "    def do_basic_nlp_cleaning(line:str)->str:\n",
        "        def is_interesting_token(token, doc):\n",
        "            if token.pos_ in set(['NUM', 'SYM']):\n",
        "                return False\n",
        "            if remove_named_ents:\n",
        "                for e in doc.ents:\n",
        "                    for t in e:\n",
        "                        if token.text == t.text:\n",
        "                            return False\n",
        "            if token.text in stopwords:\n",
        "                return False\n",
        "            if (token.is_punct):\n",
        "                return False\n",
        "            #if token._.is_emoji:\n",
        "            #    return False\n",
        "            return True\n",
        "\n",
        "        def remove_terminal_punctuations(word):\n",
        "            word = word.strip()\n",
        "            while word != \"\" and word[0] in list(string.punctuation):\n",
        "                word = word[1:]\n",
        "            while word != \"\" and word[-1] in list(string.punctuation):\n",
        "                word = word[:-1]\n",
        "            return word\n",
        "\n",
        "        def get_final_string(tok, doc):\n",
        "            lemma = tok.lemma_.lower()\n",
        "            if pos_tagging:\n",
        "                lemma = lemma + \":\" + tok.pos_\n",
        "                lemma = lemma + \":\" + tok.tag_\n",
        "            return lemma\n",
        "\n",
        "        doc = nlp(line)\n",
        "        words = [get_final_string(tok, doc) for tok in doc if is_interesting_token(tok, doc)]\n",
        "        words = [remove_terminal_punctuations(word) for word in words]\n",
        "        words = [word for word in words if word != \"\"]\n",
        "        return  \" \".join(words)\n",
        "\n",
        "    return do_basic_nlp_cleaning\n",
        "\n",
        "def get_enriched_dataset(df):\n",
        "    cleaning_fn = get_cleaning_function(remove_named_ents=True, pos_tagging=True)\n",
        "    df['cleaned_comment_text'] = df['comment_text'].map(cleaning_fn)\n",
        "    df['n_all_caps'] = df['comment_text'].map(get_num_of_allcap_words)\n",
        "    df['perc_exclamations'] = df['comment_text'].map(get_percentage_of_excalamations)\n",
        "    df['num_exclamations'] = df['comment_text'].map(lambda s: sum([1 for x in list(s) if x == '!']))\n",
        "    return df\n",
        "\n",
        "import functools\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache(maxsize=128)\n",
        "def get_all_enriched_dfs_cached():\n",
        "    train_df, test_df, extra_df = get_clean_train_test_df()\n",
        "    train_df = get_enriched_dataset(train_df)\n",
        "    test_df = get_enriched_dataset(test_df)\n",
        "    extra_df = get_enriched_dataset(extra_df)\n",
        "    return train_df, test_df, extra_df\n",
        "    \n",
        "def get_all_enriched_dataframes():\n",
        "    tr, te, ex = get_all_enriched_dfs_cached()\n",
        "    return tr.copy(), te.copy(), ex.copy()\n",
        "\n",
        "train_df, test_df, extra_df = get_all_enriched_dfs_cached()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWTtxHw0aa4N"
      },
      "source": [
        "# Print Enriched Training DF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "QyA208sEDeIn",
        "outputId": "6fe73f52-fa56-4a34-cc33-7e3d94c285e2"
      },
      "source": [
        "extra_df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>Sub1_Toxic</th>\n",
              "      <th>cleaned_comment_text</th>\n",
              "      <th>n_all_caps</th>\n",
              "      <th>perc_exclamations</th>\n",
              "      <th>num_exclamations</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>meine mutter hat mir erzählt, dass mein vater ...</td>\n",
              "      <td>0</td>\n",
              "      <td>erzählen:VERB:VVPP vater:NOUN:NN wahlkreiskand...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>174_ nummer meine reaktion; |lbr| nicht jeder ...</td>\n",
              "      <td>0</td>\n",
              "      <td>nummer:NOUN:NN reaktion:NOUN:NN lbr|:ADV:ADV m...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#merkel rollt dem emir von #katar, der islamis...</td>\n",
              "      <td>0</td>\n",
              "      <td>merkel:PROPN:NE rollen:VERB:VVFIN emir:NOUN:NN...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.007194</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>„merle ist kein junges unschuldiges mädchen“ k...</td>\n",
              "      <td>0</td>\n",
              "      <td>jung:ADJ:ADJA unschuldig:ADJ:ADJA mädchen:NOUN...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>asylantenflut bringt eben nur negatives für d...</td>\n",
              "      <td>1</td>\n",
              "      <td>SPACE:_SP asylantenflut:PROPN:NE bringen:VERB:...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8536</th>\n",
              "      <td>gegens. zul. zu patenamt &amp;amp; gegenseitige an...</td>\n",
              "      <td>0</td>\n",
              "      <td>gegenseitige:ADJ:ADJA anerk:NOUN:NN altkatholi...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8537</th>\n",
              "      <td>zu merkel fällt mir nur ein, ein mal verräter...</td>\n",
              "      <td>1</td>\n",
              "      <td>SPACE:_SP fällen:VERB:VVFIN mal:ADV:ADV verrät...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8538</th>\n",
              "      <td>nummer ein richtiges zeichen unserer nachbarn ...</td>\n",
              "      <td>1</td>\n",
              "      <td>nummer:NOUN:NN richtig:ADJ:ADJA zeichen:NOUN:N...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8539</th>\n",
              "      <td>_geld ,honecker‘merkel macht uns zur ,ddr‘ kla...</td>\n",
              "      <td>1</td>\n",
              "      <td>geld:NOUN:NN honecker:PROPN:NE merkel:PROPN:NE...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8540</th>\n",
              "      <td>warum wurden die g20-chaoten nicht sofort auf ...</td>\n",
              "      <td>0</td>\n",
              "      <td>sofort:ADV:ADV frisch:ADJ:ADJA verhaften:VERB:...</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8541 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           comment_text  ...  num_exclamations\n",
              "0     meine mutter hat mir erzählt, dass mein vater ...  ...                 0\n",
              "1     174_ nummer meine reaktion; |lbr| nicht jeder ...  ...                 0\n",
              "2     #merkel rollt dem emir von #katar, der islamis...  ...                 1\n",
              "3     „merle ist kein junges unschuldiges mädchen“ k...  ...                 0\n",
              "4      asylantenflut bringt eben nur negatives für d...  ...                 0\n",
              "...                                                 ...  ...               ...\n",
              "8536  gegens. zul. zu patenamt &amp; gegenseitige an...  ...                 0\n",
              "8537   zu merkel fällt mir nur ein, ein mal verräter...  ...                 0\n",
              "8538  nummer ein richtiges zeichen unserer nachbarn ...  ...                 0\n",
              "8539  _geld ,honecker‘merkel macht uns zur ,ddr‘ kla...  ...                 1\n",
              "8540  warum wurden die g20-chaoten nicht sofort auf ...  ...                 0\n",
              "\n",
              "[8541 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68DFX_fjYgQz"
      },
      "source": [
        "# Multinomial NB (original)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 664
        },
        "id": "zL5Boe5qYfF4",
        "outputId": "7b35779a-dbef-42e4-80da-18dd3e7ee944"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "#from imblearn.pipeline import Pipeline\n",
        "#from imblearn.over_sampling import SMOTE \n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.preprocessing import DenseTransformer\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def get_feature_column_names(df):\n",
        "    return [cname for cname in df.columns if not cname.startswith('Sub')]\n",
        "\n",
        "def get_target_column_names(df):\n",
        "    return [cname for cname in df.columns if cname.startswith('Sub')]\n",
        "\n",
        "def is_text_column(colname:str)->bool:\n",
        "    if 'text' in colname:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_text_columns(df)->list:\n",
        "    return [cn for cn in df.columns if is_text_column(cn)]\n",
        "\n",
        "def get_nontext_columns(df)->list:\n",
        "    return [cn for cn in df.columns if not is_text_column(cn)]\n",
        "\n",
        "def run_classification(                                                     \\\n",
        "                       dataset:pd.DataFrame,                                \\\n",
        "                       extra_dataset:pd.DataFrame,                          \\\n",
        "                       test_dataset:pd.DataFrame,                           \\\n",
        "                       target_column:str,                                   \\\n",
        "                       clf_gen_fn,                                          \\\n",
        "                       grid_search_dict=None,                               \\\n",
        "                       use_extra_dataset=False,                             \\\n",
        "                       )->tuple:\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "\n",
        "    X = dataset[get_feature_column_names(dataset)]\n",
        "    y = dataset[target_column]\n",
        "    trainX, testX, trainY, testY = train_test_split(X, y, random_state=0)\n",
        "\n",
        "    print(len(trainX), len(trainY))\n",
        "    if use_extra_dataset:\n",
        "        extraX = extra_dataset[get_feature_column_names(dataset)]\n",
        "        extray = extra_dataset[target_column]\n",
        "        trainX = pd.concat([trainX, extraX])\n",
        "        trainY = pd.concat([trainY, extray])\n",
        "        print(\"Added additional data from GermEval 2018\")\n",
        "        print(len(trainX), len(trainY))\n",
        "\n",
        "    if None != grid_search_dict and isinstance(grid_search_dict, dict):\n",
        "        gridupd = {\n",
        "        }\n",
        "        grid_search_dict = grid_search_dict.copy()\n",
        "        grid_search_dict.update(gridupd)\n",
        "\n",
        "                        \n",
        "    column_trans = make_column_transformer(                                 \\\n",
        "                            (CountVectorizer(ngram_range=(1,1)), 'cleaned_comment_text'),   \\\n",
        "                            (TfidfVectorizer(use_idf=True), 'cleaned_comment_text'),    \\\n",
        "                            (TfidfVectorizer(use_idf=False), 'cleaned_comment_text'),    \\\n",
        "                            remainder=MinMaxScaler(),                       \\\n",
        "                        )\n",
        "    \n",
        "    classif_pipeline = Pipeline(                                        \\\n",
        "                            [                                           \\\n",
        "                                ('column_transformer', column_trans),   \\\n",
        "                                ('dense', DenseTransformer()),          \\\n",
        "                                ('clf', clf_gen_fn()),                  \\\n",
        "                            ])\n",
        "\n",
        "    print(classif_pipeline)\n",
        "\n",
        "    \n",
        "    print(\"here\", grid_search_dict, type(grid_search_dict))\n",
        "\n",
        "    if None != grid_search_dict and isinstance(grid_search_dict, dict):\n",
        "        search = GridSearchCV(classif_pipeline, grid_search_dict, cv=4, n_jobs=-1)\n",
        "        search.fit(trainX, trainY)\n",
        "        classif_pipeline = search.best_estimator_\n",
        "        print(search.best_params_)\n",
        "    else:\n",
        "        classif_pipeline.fit(trainX, trainY)\n",
        "\n",
        "    y_pred = classif_pipeline.predict(testX)\n",
        "\n",
        "    return accuracy_score(testY, y_pred), f1_score(testY, y_pred), classif_pipeline\n",
        "\n",
        "def run_classifiers():\n",
        "\n",
        "    linearsvc_gen = lambda: LinearSVC()\n",
        "    linearsvc_paramgrid = {'clf__class_weight': [None, 'balanced'], 'clf__max_iter': [1000, 10000]}\n",
        "    #linearsvc_paramgrid = {}\n",
        "    classifiers = {\n",
        "        \"LinearSVC\": [\n",
        "                      \"Sub1_Toxic\",\n",
        "                      linearsvc_gen,\n",
        "                      linearsvc_paramgrid,\n",
        "                      True]\n",
        "    }\n",
        "\n",
        "    for clfname, val in classifiers.items():\n",
        "        colname = val[0]\n",
        "        generator = val[1]\n",
        "        gridsearch = val[2]\n",
        "        use_extra_data = val[3]\n",
        "        print(1, 2, 3, 4, 5, gridsearch)\n",
        "\n",
        "\n",
        "        train_df, test_df, extra_df = get_all_enriched_dfs_cached()\n",
        "        acc, f1, classif = run_classification(\\\n",
        "            dataset=train_df,\\\n",
        "            extra_dataset=extra_df if use_extra_data else None,\\\n",
        "            test_dataset=test_df,\\\n",
        "            target_column=colname,\\\n",
        "            clf_gen_fn=generator,\\\n",
        "            grid_search_dict=gridsearch,\\\n",
        "            use_extra_dataset=use_extra_data)\n",
        "\n",
        "        print(acc, f1,)\n",
        "\n",
        "        \n",
        "    result_df = None\n",
        "    return result_df, classif \n",
        "\n",
        "seed_random()\n",
        "result_df, model = run_classifiers()\n",
        "\n",
        "model\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 2 3 4 5 {'clf__class_weight': [None, 'balanced'], 'clf__max_iter': [1000, 10000]}\n",
            "2395 2395\n",
            "Added additional data from GermEval 2018\n",
            "10936 10936\n",
            "Pipeline(steps=[('column_transformer',\n",
            "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
            "                                   transformers=[('countvectorizer',\n",
            "                                                  CountVectorizer(),\n",
            "                                                  'cleaned_comment_text'),\n",
            "                                                 ('tfidfvectorizer-1',\n",
            "                                                  TfidfVectorizer(),\n",
            "                                                  'cleaned_comment_text'),\n",
            "                                                 ('tfidfvectorizer-2',\n",
            "                                                  TfidfVectorizer(use_idf=False),\n",
            "                                                  'cleaned_comment_text')])),\n",
            "                ('dense', DenseTransformer()), ('clf', LinearSVC())])\n",
            "here {'clf__class_weight': [None, 'balanced'], 'clf__max_iter': [1000, 10000]} <class 'dict'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/svm/_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  ConvergenceWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'clf__class_weight': None, 'clf__max_iter': 1000}\n",
            "0.6708385481852316 0.45548654244306414\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<style>#sk-4706f192-0045-4b1f-863f-178ae8107042 {color: black;background-color: white;}#sk-4706f192-0045-4b1f-863f-178ae8107042 pre{padding: 0;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-toggleable {background-color: white;}#sk-4706f192-0045-4b1f-863f-178ae8107042 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-4706f192-0045-4b1f-863f-178ae8107042 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-4706f192-0045-4b1f-863f-178ae8107042 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-estimator:hover {background-color: #d4ebff;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-item {z-index: 1;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-parallel-item:only-child::after {width: 0;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-4706f192-0045-4b1f-863f-178ae8107042 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-4706f192-0045-4b1f-863f-178ae8107042\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e1909942-dcc1-4744-87a0-920ad0614978\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e1909942-dcc1-4744-87a0-920ad0614978\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('column_transformer',\n",
              "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                                   transformers=[('countvectorizer',\n",
              "                                                  CountVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-1',\n",
              "                                                  TfidfVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-2',\n",
              "                                                  TfidfVectorizer(use_idf=False),\n",
              "                                                  'cleaned_comment_text')])),\n",
              "                ('dense', DenseTransformer()), ('clf', LinearSVC())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"bab104da-c2d7-45f3-8a3d-8fc12d800957\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"bab104da-c2d7-45f3-8a3d-8fc12d800957\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                  transformers=[('countvectorizer', CountVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-1', TfidfVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-2',\n",
              "                                 TfidfVectorizer(use_idf=False),\n",
              "                                 'cleaned_comment_text')])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"bc9295db-45dd-40e4-97ca-9a3d2d94266b\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"bc9295db-45dd-40e4-97ca-9a3d2d94266b\">countvectorizer</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b6a37955-9b51-4138-9c52-971d96bb6605\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"b6a37955-9b51-4138-9c52-971d96bb6605\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"50247737-886e-4b63-988b-c7988b4e091c\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"50247737-886e-4b63-988b-c7988b4e091c\">tfidfvectorizer-1</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"548d9fd9-273f-47f7-8452-c2fea28f783c\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"548d9fd9-273f-47f7-8452-c2fea28f783c\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"1a57365e-03c1-4cc4-9058-8864c71ac041\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"1a57365e-03c1-4cc4-9058-8864c71ac041\">tfidfvectorizer-2</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"7261c8f0-6d05-4279-abd0-2bb635f034b0\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"7261c8f0-6d05-4279-abd0-2bb635f034b0\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(use_idf=False)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"eb4ffb1a-25c7-4825-bab9-bec38f2930e9\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"eb4ffb1a-25c7-4825-bab9-bec38f2930e9\">remainder</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps', 'perc_exclamations', 'num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"07475f98-fde1-4e68-85cb-57631d8791c1\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"07475f98-fde1-4e68-85cb-57631d8791c1\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5a92be20-e489-4f58-849f-5d7379259878\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"5a92be20-e489-4f58-849f-5d7379259878\">DenseTransformer</label><div class=\"sk-toggleable__content\"><pre>DenseTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"614e9a9e-919d-46c3-8c56-6ad0226cbd2b\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"614e9a9e-919d-46c3-8c56-6ad0226cbd2b\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC()</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('column_transformer',\n",
              "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                                   transformers=[('countvectorizer',\n",
              "                                                  CountVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-1',\n",
              "                                                  TfidfVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-2',\n",
              "                                                  TfidfVectorizer(use_idf=False),\n",
              "                                                  'cleaned_comment_text')])),\n",
              "                ('dense', DenseTransformer()), ('clf', LinearSVC())])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a1buL1EozuH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "d43ee2c4-a3ff-4eb9-a9b3-70f186c77418"
      },
      "source": [
        "def print_df(df, metric, task):\n",
        "    df = df[(df['metric'] == metric) & (df['task_name'] == task)]\n",
        "    df = df.sort_values(by=['value'], ascending=False)\n",
        "    print(df.head(3))\n",
        "    return df\n",
        "\n",
        "for task_name in ['Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming']:\n",
        "    print('=' * 80)\n",
        "    print(task_name)\n",
        "    print('-' * len(task_name))\n",
        "    print()\n",
        "    for metric in ['accuracy', 'f1_score']:\n",
        "        print_df(result_df, metric, task_name)\n",
        "        print()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Sub1_Toxic\n",
            "----------\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-03cacf209ee8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'f1_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mprint_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-03cacf209ee8>\u001b[0m in \u001b[0;36mprint_df\u001b[0;34m(df, metric, task)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprint_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'metric'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'task_name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mby\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'value'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    }
  ]
}