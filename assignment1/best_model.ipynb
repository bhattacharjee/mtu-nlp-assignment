{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "best_model.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPRfQIofekG15KbO7/zXaBI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment1/best_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERvBOEn0wss6"
      },
      "source": [
        "!pip install spacy  nltk spacymoji huggingface -q       >/dev/null 2>&1         \n",
        "!pip install -q -U tensorflow-text                      >/dev/null 2>&1\n",
        "!pip install -q tf-models-official                      >/dev/null 2>&1\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "!pip install transformers                               >/dev/null 2>&1\n",
        "\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "\n",
        "!pip install mlxtend                                    >/dev/null 2>&1\n",
        "!pip install imblearn                                   >/dev/null 2>&1\n",
        "\n",
        "# handling emojis\n",
        "!pip install demoji                                     >/dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-DAX9VLZlw3"
      },
      "source": [
        "import requests\n",
        "from functools import lru_cache\n",
        "import sklearn\n",
        "\n",
        "@lru_cache(maxsize=10)\n",
        "def get_train_test_files():\n",
        "    TRAIN_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Test_For_Evaluation.csv'\n",
        "    EXTRA_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/germeval2018_a.txt'\n",
        "    TRAIN_FILE_LOCAL = 'Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE_LOCAL = 'Assessment1_Toxic_Test.csv'\n",
        "    EXTRA_FILE_LOCAL = 'germeval2018.csv'\n",
        "\n",
        "    def download(url, localfile):\n",
        "        with open(localfile, 'wb') as f:\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            f.write(r.content)\n",
        "\n",
        "    download(TRAIN_FILE, TRAIN_FILE_LOCAL)\n",
        "    download(TEST_FILE, TEST_FILE_LOCAL)\n",
        "    download(EXTRA_FILE, EXTRA_FILE_LOCAL)\n",
        "\n",
        "    return TRAIN_FILE_LOCAL, TEST_FILE_LOCAL, EXTRA_FILE_LOCAL\n",
        "\n",
        "def seed_random():\n",
        "    import numpy as np\n",
        "    import random\n",
        "    np.random.seed(0)\n",
        "    random.seed(0)\n",
        "\n",
        "sklearn.set_config(display=\"diagram\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrlJ1H1maDxM"
      },
      "source": [
        "# Functions to read the CSV and do basic cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVfYulLnuGVJ"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import demoji\n",
        "\n",
        "from functools import lru_cache\n",
        "\n",
        "def remove_roles(line:str)->str:\n",
        "    # Remove texts like @USER, @MODERATOR etc\n",
        "    pat = re.compile(u'\\@[A-Za-z]+')\n",
        "    return re.sub(pat, '', line)\n",
        "\n",
        "@lru_cache(maxsize=3)\n",
        "def get_train_test_df_cached():\n",
        "    train_csv, test_csv, extra_csv = get_train_test_files()\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "    extra_df = pd.read_csv(extra_csv)\n",
        "    return train_df, test_df, extra_df\n",
        "\n",
        "def get_train_test_df():\n",
        "    tr, te, ex = get_train_test_df_cached()\n",
        "    return tr.copy(), te.copy(), ex.copy()\n",
        "\n",
        "def remove_emojis(line:str)->str:\n",
        "    # Replace emojis with their description, eg __thumbs_down__\n",
        "    demoji_str = demoji.replace_with_desc(line, sep=\" ::: \")\n",
        "    if (demoji_str == line):\n",
        "        return line\n",
        "    \n",
        "    inEmoji = False\n",
        "    currentEmojiWords = []\n",
        "    allWords = []\n",
        "\n",
        "    def accumulate(word:str)->None:\n",
        "        nonlocal inEmoji\n",
        "        nonlocal currentEmojiWords\n",
        "        nonlocal allWords\n",
        "        if not inEmoji and word != \":::\":\n",
        "            allWords.append(word)\n",
        "        elif inEmoji:\n",
        "            if word == ':::':\n",
        "                currentEmoji = \"_\".join(currentEmojiWords)\n",
        "                currentEmoji = \"__\" + currentEmoji + \"__\"\n",
        "                allWords.append(currentEmoji)\n",
        "                currentEmojiWords = []\n",
        "            else:\n",
        "                currentEmojiWords.append(word)\n",
        "        else: # Not in emoji but ::: is true\n",
        "            inEmoji = True\n",
        "\n",
        "    [accumulate(word) for word in demoji_str.split()]\n",
        "\n",
        "    sentence = \" \".join(allWords)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def remove_ellipses(line:str)->str:\n",
        "    pat = re.compile(u'\\.\\.+')\n",
        "    return re.sub(pat, ' ', line)\n",
        "\n",
        "def to_lower(line:str)->str:\n",
        "    return line.lower()\n",
        "\n",
        "def replace_number_with_tag(line:str)->str:\n",
        "    line = re.sub(\"\\s\\d*((\\.|\\,)\\d+)?\\s\", \" nummer \", line)\n",
        "    line = re.sub('\\s\\d+$', '', line)\n",
        "    line = re.sub('^\\d+\\s', '', line)\n",
        "    return line\n",
        "\n",
        "def remove_urls(line:str)->str:\n",
        "    return re.sub('https?:\\/\\/\\S+', ' hyperlink ', line)\n",
        "\n",
        "def basic_clean(s:pd.Series)->pd.Series:\n",
        "    return s.map(to_lower)                                                  \\\n",
        "            .map(remove_emojis)                                             \\\n",
        "            .map(remove_roles)                                              \\\n",
        "            .map(remove_ellipses)                                           \\\n",
        "            .map(replace_number_with_tag)                                   \\\n",
        "            .map(remove_urls)\n",
        "\n",
        "def get_clean_train_test_df()->tuple:\n",
        "    train_df, test_df, extra_df = get_train_test_df()\n",
        "    train_df['comment_text'] = basic_clean(train_df['comment_text'])\n",
        "    test_df['comment_text'] = basic_clean(test_df['comment_text'])\n",
        "    extra_df['comment_text'] = basic_clean(extra_df['comment_text'])\n",
        "    return train_df, test_df, extra_df\n"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6_dto2JaR2f"
      },
      "source": [
        "# Clean using Spacy and Enrich"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8YSZZ4vxTk5"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string\n",
        "import spacy\n",
        "from spacymoji import Emoji\n",
        "import  de_core_news_sm\n",
        "\n",
        "def is_punct_only(token:str)->bool:\n",
        "    for c in list(token):\n",
        "        if c not in string.punctuation:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_same(l1:list, l2:list)->bool:\n",
        "    if (len(l1) != len(l2)):\n",
        "        return False\n",
        "    for x, y in zip(l1, l2):\n",
        "        if x != y:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def get_num_of_allcap_words(s:str)->int:\n",
        "    def is_allcaps(s:str)->bool:\n",
        "        if (len(s) < 3):\n",
        "            return False\n",
        "        for c in list(s):\n",
        "            if not (\\\n",
        "                    (ord(c) <=ord('Z') and ord(c) >= ord('A')) or           \\\n",
        "                    (ord(c) >= ord('0') and ord(c) <= ord('9'))             \\\n",
        "                    ):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    if len(s) < 3:\n",
        "        return 0\n",
        "    tokens = [w.strip() for w in s.split()]\n",
        "    return sum([1 for t in tokens if is_allcaps(t)])\n",
        "\n",
        "def get_percentage_of_excalamations(s:str)->float:\n",
        "    if len(s) == 0:\n",
        "        return 0.0\n",
        "    exclamation_count = sum([1 for c in list(s) if c == '!'])\n",
        "    return exclamation_count / len(s)\n",
        "\n",
        "\n",
        "def is_empty_string(s:str)->bool:\n",
        "    if s == '' or s == None:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def do_basic_nlp_cleaning(line:str)->str:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(line)\n",
        "\n",
        "    # Some tokens start with a punctuation, remove the first one\n",
        "    def remove_first_punctuation(tok:str)->str:\n",
        "        return                                                              \\\n",
        "            tok[1:]                                                         \\\n",
        "            if tok[0] in set(string.punctuation) and len(tok) != 0          \\\n",
        "            else tok\n",
        "\n",
        "    tokens = [remove_first_punctuation(w) for w in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"german\"))\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [w for w in tokens if not is_punct_only(w)]\n",
        "\n",
        "    # Stem words\n",
        "    stem = SnowballStemmer('german')\n",
        "    tokens = [stem.stem(w) for w in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def get_cleaning_function(remove_named_ents:bool=True, pos_tagging:bool=False):\n",
        "    nlp = de_core_news_sm.load()\n",
        "    emoji = Emoji(nlp)\n",
        "    nlp.add_pipe(emoji, first=True)\n",
        "    stopwords = spacy.lang.de.stop_words.STOP_WORDS\n",
        "\n",
        "    def do_basic_nlp_cleaning(line:str)->str:\n",
        "        def is_interesting_token(token, doc):\n",
        "            if token.pos_ in set(['NUM', 'SYM']):\n",
        "                return False\n",
        "            if remove_named_ents:\n",
        "                for e in doc.ents:\n",
        "                    for t in e:\n",
        "                        if token.text == t.text:\n",
        "                            return False\n",
        "            if token.text in stopwords:\n",
        "                return False\n",
        "            if (token.is_punct):\n",
        "                return False\n",
        "            #if token._.is_emoji:\n",
        "            #    return False\n",
        "            return True\n",
        "\n",
        "        def remove_terminal_punctuations(word):\n",
        "            word = word.strip()\n",
        "            while word != \"\" and word[0] in list(string.punctuation):\n",
        "                word = word[1:]\n",
        "            while word != \"\" and word[-1] in list(string.punctuation):\n",
        "                word = word[:-1]\n",
        "            return word\n",
        "\n",
        "        def get_final_string(tok, doc):\n",
        "            lemma = tok.lemma_.lower()\n",
        "            if pos_tagging:\n",
        "                lemma = lemma + \":\" + tok.pos_\n",
        "                lemma = lemma + \":\" + tok.tag_\n",
        "            return lemma\n",
        "\n",
        "        doc = nlp(line)\n",
        "        words = [get_final_string(tok, doc) for tok in doc if is_interesting_token(tok, doc)]\n",
        "        words = [remove_terminal_punctuations(word) for word in words]\n",
        "        words = [word for word in words if word != \"\"]\n",
        "        return  \" \".join(words)\n",
        "\n",
        "    return do_basic_nlp_cleaning\n",
        "\n",
        "def get_enriched_dataset(df):\n",
        "    cleaning_fn = get_cleaning_function(remove_named_ents=True, pos_tagging=True)\n",
        "    df['cleaned_comment_text'] = df['comment_text'].map(cleaning_fn)\n",
        "    df['n_all_caps'] = df['comment_text'].map(get_num_of_allcap_words)\n",
        "    df['perc_exclamations'] = df['comment_text'].map(get_percentage_of_excalamations)\n",
        "    df['num_exclamations'] = df['comment_text'].map(lambda s: sum([1 for x in list(s) if x == '!']))\n",
        "    return df\n",
        "\n",
        "import functools\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache(maxsize=128)\n",
        "def get_all_enriched_dfs_cached():\n",
        "    train_df, test_df, extra_df = get_clean_train_test_df()\n",
        "    train_df = get_enriched_dataset(train_df)\n",
        "    test_df = get_enriched_dataset(test_df)\n",
        "    extra_df = get_enriched_dataset(extra_df)\n",
        "    return train_df, test_df, extra_df\n",
        "    \n",
        "def get_all_enriched_dataframes():\n",
        "    tr, te, ex = get_all_enriched_dfs_cached()\n",
        "    return tr.copy(), te.copy(), ex.copy()\n",
        "\n",
        "train_df, test_df, extra_df = get_all_enriched_dfs_cached()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68DFX_fjYgQz"
      },
      "source": [
        "# Multinomial NB (original)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zL5Boe5qYfF4",
        "outputId": "6e525506-74a4-4842-c8b2-ff2dc86c8a2f"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "#from imblearn.pipeline import Pipeline\n",
        "#from imblearn.over_sampling import SMOTE \n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.preprocessing import DenseTransformer\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def get_feature_column_names(df):\n",
        "    return [cname for cname in df.columns if not cname.startswith('Sub')]\n",
        "\n",
        "def get_target_column_names(df):\n",
        "    return [cname for cname in df.columns if cname.startswith('Sub')]\n",
        "\n",
        "def is_text_column(colname:str)->bool:\n",
        "    if 'text' in colname:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_text_columns(df)->list:\n",
        "    return [cn for cn in df.columns if is_text_column(cn)]\n",
        "\n",
        "def get_nontext_columns(df)->list:\n",
        "    return [cn for cn in df.columns if not is_text_column(cn)]\n",
        "\n",
        "def run_classification(                                                     \\\n",
        "                       dataset:pd.DataFrame,                                \\\n",
        "                       extra_dataset:pd.DataFrame,                          \\\n",
        "                       test_dataset:pd.DataFrame,                           \\\n",
        "                       target_column:str,                                   \\\n",
        "                       clf_gen_fn,                                          \\\n",
        "                       grid_search_dict=None,                               \\\n",
        "                       use_extra_dataset=False,                             \\\n",
        "                       )->tuple:\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "\n",
        "    seed_random()\n",
        "\n",
        "    X = dataset[get_feature_column_names(dataset)]\n",
        "    y = dataset[target_column]\n",
        "    trainX, testX, trainY, testY = train_test_split(X, y, random_state=0)\n",
        "\n",
        "    if use_extra_dataset:\n",
        "        extraX = extra_dataset[get_feature_column_names(dataset)]\n",
        "        extray = extra_dataset[target_column]\n",
        "        trainX = pd.concat([trainX, extraX])\n",
        "        trainY = pd.concat([trainY, extray])\n",
        "        print(\"Added additional data from GermEval 2018\")\n",
        "\n",
        "    if None != grid_search_dict and isinstance(grid_search_dict, dict):\n",
        "        # TODO: Update grid for the vectorizers\n",
        "        # Right now, we're hitting RAM constraints if we turn these on\n",
        "        gridupd = {\n",
        "        }\n",
        "        grid_search_dict = grid_search_dict.copy()\n",
        "        grid_search_dict.update(gridupd)\n",
        "\n",
        "                        \n",
        "    column_trans = make_column_transformer(                                 \\\n",
        "                            (CountVectorizer(ngram_range=(1,1)), 'cleaned_comment_text'),   \\\n",
        "                            (TfidfVectorizer(use_idf=True), 'cleaned_comment_text'),    \\\n",
        "                            (TfidfVectorizer(use_idf=False), 'cleaned_comment_text'),    \\\n",
        "                            remainder=MinMaxScaler(),                       \\\n",
        "                        )\n",
        "    \n",
        "    classif_pipeline = Pipeline(                                        \\\n",
        "                            [                                           \\\n",
        "                                ('column_transformer', column_trans),   \\\n",
        "                                ('dense', DenseTransformer()),          \\\n",
        "                                ('clf', clf_gen_fn()),                  \\\n",
        "                            ])\n",
        "\n",
        "    if None != grid_search_dict and isinstance(grid_search_dict, dict):\n",
        "        search = GridSearchCV(classif_pipeline, grid_search_dict, cv=3, n_jobs=-1, scoring='f1')\n",
        "        search.fit(trainX, trainY)\n",
        "        classif_pipeline = search.best_estimator_\n",
        "        print(\"best params: \", search.best_params_)\n",
        "        print(\"best f1 score: \", search.best_score_)\n",
        "    else:\n",
        "        classif_pipeline.fit(trainX, trainY)\n",
        "\n",
        "    y_pred = classif_pipeline.predict(testX)\n",
        "    print('-' * 40)\n",
        "\n",
        "    return accuracy_score(testY, y_pred), f1_score(testY, y_pred), classif_pipeline\n",
        "\n",
        "def predict_on_test_set(\\\n",
        "                        dataset,\\\n",
        "                        colname,\\\n",
        "                        classif_pipeline):\n",
        "    seed_random()\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "    y_pred = classif_pipeline.predict(dataset)\n",
        "    return y_pred.tolist()\n",
        "\n",
        "def run_classifiers():\n",
        "\n",
        "    linearsvc_gen = lambda: LinearSVC()\n",
        "    linearsvc_paramgrid = {'clf__class_weight': [None, 'balanced'], 'clf__max_iter': [1000, 10000]}\n",
        "    classifiers = {\n",
        "        \"LinearSVC\": [\n",
        "                      \"Sub1_Toxic\",\n",
        "                      linearsvc_gen,\n",
        "                      linearsvc_paramgrid,\n",
        "                      True, # Use additional data\n",
        "                      ]\n",
        "    }\n",
        "\n",
        "    rfc_gen = lambda: RandomForestClassifier()\n",
        "    rfc_paramgrid = {'clf__criterion': ['entropy', 'gini'], 'clf__min_samples_split': [2, 10], 'clf__class_weight': ['balanced', 'balanced_subsample']}\n",
        "    tempdict = {\n",
        "        'RandomForest1': [\n",
        "                          \"Sub2_Engaging\",\n",
        "                          rfc_gen,\n",
        "                          rfc_paramgrid,\n",
        "                          False # Don't use additional data\n",
        "        ]\n",
        "    }\n",
        "    classifiers.update(tempdict)\n",
        "    \n",
        "    mnb_gen = lambda: MultinomialNB()\n",
        "    mnb_paramgrid = {'clf__fit_prior': [True, False]}\n",
        "    tempdict = {\n",
        "        'MultinomialNB1': [\n",
        "                           \"Sub3_FactClaiming\",\n",
        "                           mnb_gen,\n",
        "                           mnb_paramgrid,\n",
        "                           False # Don't use additional data\n",
        "        ]\n",
        "    }\n",
        "    classifiers.update(tempdict)\n",
        "\n",
        "    best_classifiers = list() # list of tuples: (columnname, clfname, pipeline)\n",
        "\n",
        "    for clfname, val in classifiers.items():\n",
        "        colname = val[0]\n",
        "        generator = val[1]\n",
        "        gridsearch = val[2]\n",
        "        use_extra_data = val[3]\n",
        "\n",
        "        print(\"Running: \", colname, clfname)\n",
        "\n",
        "        train_df, test_df, extra_df = get_all_enriched_dfs_cached()\n",
        "        acc, f1, classif_pipeline = run_classification(\\\n",
        "            dataset=train_df,\\\n",
        "            extra_dataset=extra_df if use_extra_data else None,\\\n",
        "            test_dataset=test_df,\\\n",
        "            target_column=colname,\\\n",
        "            clf_gen_fn=generator,\\\n",
        "            grid_search_dict=gridsearch,\\\n",
        "            use_extra_dataset=use_extra_data)\n",
        "        \n",
        "        best_classifiers.append((colname, clfname, classif_pipeline))\n",
        "\n",
        "        print(colname, clfname, acc, f1,)\n",
        "\n",
        "\n",
        "    or_train_df, or_test_df, or_extra_df = get_train_test_df()\n",
        "    test_pred_df_dict = {\n",
        "        'comment_text': or_test_df['comment_text'].to_numpy().tolist()\n",
        "    }\n",
        "\n",
        "    retval = list()\n",
        "    for colname, clfname, clf_pipeline in best_classifiers:\n",
        "        train_df, test_df, extra_df = get_all_enriched_dfs_cached()\n",
        "        y_pred = predict_on_test_set(test_df, clfname, clf_pipeline)\n",
        "        temp_dict = {colname: y_pred}\n",
        "        test_pred_df_dict.update(temp_dict)\n",
        "        tempdict = {'clfname': clfname, 'colname': colname, 'clf_pipeline': clf_pipeline}\n",
        "        retval.append(tempdict)\n",
        "\n",
        "    result_df = pd.DataFrame(test_pred_df_dict)\n",
        "    return result_df, retval\n",
        "\n",
        "seed_random()\n",
        "result_df, retval = run_classifiers()\n",
        "\n",
        "\n",
        "for colname, clfname, clf_pipeline in retval:\n",
        "    print(colname, clfname)\n",
        "    clf_pipeline"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running:  Sub1_Toxic LinearSVC\n",
            "Added additional data from GermEval 2018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best params:  {'clf__class_weight': 'balanced', 'clf__max_iter': 10000}\n",
            "best f1 score:  0.5168579314509509\n",
            "----------------------------------------\n",
            "Sub1_Toxic LinearSVC 0.655819774718398 0.4701348747591522\n",
            "Running:  Sub2_Engaging RandomForest1\n",
            "best params:  {'clf__class_weight': 'balanced', 'clf__criterion': 'gini', 'clf__min_samples_split': 10}\n",
            "best f1 score:  0.5965191250907029\n",
            "----------------------------------------\n",
            "Sub2_Engaging RandomForest1 0.8385481852315394 0.6485013623978202\n",
            "Running:  Sub3_FactClaiming MultinomialNB1\n",
            "best params:  {'clf__fit_prior': False}\n",
            "best f1 score:  0.6029812346017276\n",
            "----------------------------------------\n",
            "Sub3_FactClaiming MultinomialNB1 0.704630788485607 0.5902777777777777\n",
            "clfname colname\n",
            "clfname colname\n",
            "clfname colname\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kWFMqaExVFm",
        "outputId": "fa306112-3612-4d45-c566-e054fbbac652"
      },
      "source": [
        "print(retval)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'clfname': 'LinearSVC', 'colname': 'Sub1_Toxic', 'clf_pipeline': Pipeline(steps=[('column_transformer',\n",
            "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
            "                                   transformers=[('countvectorizer',\n",
            "                                                  CountVectorizer(),\n",
            "                                                  'cleaned_comment_text'),\n",
            "                                                 ('tfidfvectorizer-1',\n",
            "                                                  TfidfVectorizer(),\n",
            "                                                  'cleaned_comment_text'),\n",
            "                                                 ('tfidfvectorizer-2',\n",
            "                                                  TfidfVectorizer(use_idf=False),\n",
            "                                                  'cleaned_comment_text')])),\n",
            "                ('dense', DenseTransformer()),\n",
            "                ('clf', LinearSVC(class_weight='balanced', max_iter=10000))])}, {'clfname': 'RandomForest1', 'colname': 'Sub2_Engaging', 'clf_pipeline': Pipeline(steps=[('column_transformer',\n",
            "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
            "                                   transformers=[('countvectorizer',\n",
            "                                                  CountVectorizer(),\n",
            "                                                  'cleaned_comment_text'),\n",
            "                                                 ('tfidfvectorizer-1',\n",
            "                                                  TfidfVectorizer(),\n",
            "                                                  'cleaned_comment_text'),\n",
            "                                                 ('tfidfvectorizer-2',\n",
            "                                                  TfidfVectorizer(use_idf=False),\n",
            "                                                  'cleaned_comment_text')])),\n",
            "                ('dense', DenseTransformer()),\n",
            "                ('clf',\n",
            "                 RandomForestClassifier(class_weight='balanced',\n",
            "                                        min_samples_split=10))])}, {'clfname': 'MultinomialNB1', 'colname': 'Sub3_FactClaiming', 'clf_pipeline': Pipeline(steps=[('column_transformer',\n",
            "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
            "                                   transformers=[('countvectorizer',\n",
            "                                                  CountVectorizer(),\n",
            "                                                  'cleaned_comment_text'),\n",
            "                                                 ('tfidfvectorizer-1',\n",
            "                                                  TfidfVectorizer(),\n",
            "                                                  'cleaned_comment_text'),\n",
            "                                                 ('tfidfvectorizer-2',\n",
            "                                                  TfidfVectorizer(use_idf=False),\n",
            "                                                  'cleaned_comment_text')])),\n",
            "                ('dense', DenseTransformer()),\n",
            "                ('clf', MultinomialNB(fit_prior=False))])}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShTKxYoz2UHN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}