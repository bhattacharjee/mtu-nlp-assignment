{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "best_model.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN03EWqjDNHwLfyVaf0EIxD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment1/best_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERvBOEn0wss6"
      },
      "source": [
        "!pip install spacy  nltk spacymoji huggingface -q       >/dev/null 2>&1         \n",
        "!pip install -q -U tensorflow-text                      >/dev/null 2>&1\n",
        "!pip install -q tf-models-official                      >/dev/null 2>&1\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "!pip install transformers                               >/dev/null 2>&1\n",
        "\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1\n",
        "!python -m spacy download de_dep_news_trf               >/dev/null 2>&1\n",
        "\n",
        "!pip install mlxtend                                    >/dev/null 2>&1\n",
        "!pip install imblearn                                   >/dev/null 2>&1\n",
        "\n",
        "# handling emojis\n",
        "!pip install demoji                                     >/dev/null 2>&1"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-DAX9VLZlw3"
      },
      "source": [
        "import requests\n",
        "from functools import lru_cache\n",
        "import sklearn\n",
        "\n",
        "@lru_cache(maxsize=10)\n",
        "def get_train_test_files():\n",
        "    TRAIN_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Test_For_Evaluation.csv'\n",
        "    EXTRA_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/germeval2018_a.txt'\n",
        "    TRAIN_FILE_LOCAL = 'Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE_LOCAL = 'Assessment1_Toxic_Test.csv'\n",
        "    EXTRA_FILE_LOCAL = 'germeval2018.csv'\n",
        "\n",
        "    def download(url, localfile):\n",
        "        with open(localfile, 'wb') as f:\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            f.write(r.content)\n",
        "\n",
        "    download(TRAIN_FILE, TRAIN_FILE_LOCAL)\n",
        "    download(TEST_FILE, TEST_FILE_LOCAL)\n",
        "    download(EXTRA_FILE, EXTRA_FILE_LOCAL)\n",
        "\n",
        "    return TRAIN_FILE_LOCAL, TEST_FILE_LOCAL, EXTRA_FILE_LOCAL\n",
        "\n",
        "def seed_random():\n",
        "    import numpy as np\n",
        "    import random\n",
        "    np.random.seed(0)\n",
        "    random.seed(0)\n",
        "\n",
        "sklearn.set_config(display=\"diagram\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrlJ1H1maDxM"
      },
      "source": [
        "# Functions to read the CSV and do basic cleaning\n",
        "\n",
        "\n",
        "Cleaning with Python\n",
        "The data was first loaded using pandas. After that, regular expressions were used to perform the following:\n",
        "1.\tConvert to lowercase\n",
        "2.\tEmojis were replaced with their descriptions. Certain emojis can be relevant to the tasks at hand. The descriptions were modified to be a single word separated by underscores, eg. __thumbs_down__. These emojis are not German, but that should not make any difference to the models.\n",
        "3.\tThe roles like @user, @moderator, etc. was removed. This was done because it was assumed that this might introduce bias into the classification, although the description of. The dataset says that chances of a bias are very unlikely.\n",
        "4.\tEllipses are removed\n",
        "5.\tAny numbers are replaced with a tag, like NUM\n",
        "6.\tURLâ€™s and links are removed\n",
        "7.\tRemove any punctuations\n",
        "8.\tPunctuations at the beginning or end of words are removed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVfYulLnuGVJ"
      },
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import demoji\n",
        "\n",
        "from functools import lru_cache\n",
        "\n",
        "def remove_roles(line:str)->str:\n",
        "    # Remove texts like @USER, @MODERATOR etc\n",
        "    pat = re.compile(u'\\@[A-Za-z]+')\n",
        "    return re.sub(pat, '', line)\n",
        "\n",
        "@lru_cache(maxsize=3)\n",
        "def get_train_test_df_cached():\n",
        "    train_csv, test_csv, extra_csv = get_train_test_files()\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "    extra_df = pd.read_csv(extra_csv)\n",
        "    return train_df, test_df, extra_df\n",
        "\n",
        "def get_train_test_df():\n",
        "    tr, te, ex = get_train_test_df_cached()\n",
        "    return tr.copy(), te.copy(), ex.copy()\n",
        "\n",
        "def remove_emojis(line:str)->str:\n",
        "    # Replace emojis with their description, eg __thumbs_down__\n",
        "    demoji_str = demoji.replace_with_desc(line, sep=\" ::: \")\n",
        "    if (demoji_str == line):\n",
        "        return line\n",
        "    \n",
        "    inEmoji = False\n",
        "    currentEmojiWords = []\n",
        "    allWords = []\n",
        "\n",
        "    def accumulate(word:str)->None:\n",
        "        nonlocal inEmoji\n",
        "        nonlocal currentEmojiWords\n",
        "        nonlocal allWords\n",
        "        if not inEmoji and word != \":::\":\n",
        "            allWords.append(word)\n",
        "        elif inEmoji:\n",
        "            if word == ':::':\n",
        "                currentEmoji = \"_\".join(currentEmojiWords)\n",
        "                currentEmoji = \"__\" + currentEmoji + \"__\"\n",
        "                allWords.append(currentEmoji)\n",
        "                currentEmojiWords = []\n",
        "            else:\n",
        "                currentEmojiWords.append(word)\n",
        "        else: # Not in emoji but ::: is true\n",
        "            inEmoji = True\n",
        "\n",
        "    [accumulate(word) for word in demoji_str.split()]\n",
        "\n",
        "    sentence = \" \".join(allWords)\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def remove_ellipses(line:str)->str:\n",
        "    pat = re.compile(u'\\.\\.+')\n",
        "    return re.sub(pat, ' ', line)\n",
        "\n",
        "def to_lower(line:str)->str:\n",
        "    return line.lower()\n",
        "\n",
        "def replace_number_with_tag(line:str)->str:\n",
        "    line = re.sub(\"\\s\\d*((\\.|\\,)\\d+)?\\s\", \" nummer \", line)\n",
        "    line = re.sub('\\s\\d+$', '', line)\n",
        "    line = re.sub('^\\d+\\s', '', line)\n",
        "    return line\n",
        "\n",
        "def remove_urls(line:str)->str:\n",
        "    return re.sub('https?:\\/\\/\\S+', ' hyperlink ', line)\n",
        "\n",
        "def basic_clean(s:pd.Series)->pd.Series:\n",
        "    return s.map(to_lower)                                                  \\\n",
        "            .map(remove_emojis)                                             \\\n",
        "            .map(remove_roles)                                              \\\n",
        "            .map(remove_ellipses)                                           \\\n",
        "            .map(replace_number_with_tag)                                   \\\n",
        "            .map(remove_urls)\n",
        "\n",
        "def get_clean_train_test_df()->tuple:\n",
        "    train_df, test_df, extra_df = get_train_test_df()\n",
        "    train_df['comment_text'] = basic_clean(train_df['comment_text'])\n",
        "    test_df['comment_text'] = basic_clean(test_df['comment_text'])\n",
        "    extra_df['comment_text'] = basic_clean(extra_df['comment_text'])\n",
        "    return train_df, test_df, extra_df\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6_dto2JaR2f"
      },
      "source": [
        "# Clean using Spacy and Enrich\n",
        "\n",
        "\n",
        "\n",
        "Cleaning with Spacy\n",
        "After the above set of steps, further cleaning was performed via a dedicated NLP toolkit. NLTK and spacy were both evaluated, but spacy seemed to be a better library for some tasks, and this was chosen for all asks as a result. The following operations were performed with spacy.\n",
        "1.\tNumbers or symbols are removed, we have already performed this step earlier, but some numbers may still be present.\n",
        "2.\tStopwords are removed\n",
        "3.\tPunctuations are removed, again this was already done via regular expressions but some may still remain.\n",
        "4.\tWords are lemmatized\n",
        "Parts of Speech Tagging and Named Entity Recognition\n",
        "Experiments were tried with both POS tagging and removal of named entities, and without POS tagging and still having named entities. It was found that removal of named entities gave a big boost to model performance. Also, it was found that POS tagging gave a further small gain in model performance.\n",
        "Both POS tagging and named entity removal were performed by use of the spacy library.\n",
        "Additional Features\n",
        "Taking inspiration from the approaches taken by various teams in the GermEval2021 competition, the following features were added:\n",
        "1.\tNumber of words with length greater than 3 that have all letters in capital\n",
        "2.\tNumber of exclamations\n",
        "3.\tRatio of exclamations to number of characters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8YSZZ4vxTk5"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string\n",
        "import spacy\n",
        "from spacymoji import Emoji\n",
        "import  de_core_news_sm\n",
        "\n",
        "def is_punct_only(token:str)->bool:\n",
        "    for c in list(token):\n",
        "        if c not in string.punctuation:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_same(l1:list, l2:list)->bool:\n",
        "    if (len(l1) != len(l2)):\n",
        "        return False\n",
        "    for x, y in zip(l1, l2):\n",
        "        if x != y:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def get_num_of_allcap_words(s:str)->int:\n",
        "    def is_allcaps(s:str)->bool:\n",
        "        if (len(s) < 3):\n",
        "            return False\n",
        "        for c in list(s):\n",
        "            if not (\\\n",
        "                    (ord(c) <=ord('Z') and ord(c) >= ord('A')) or           \\\n",
        "                    (ord(c) >= ord('0') and ord(c) <= ord('9'))             \\\n",
        "                    ):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    if len(s) < 3:\n",
        "        return 0\n",
        "    tokens = [w.strip() for w in s.split()]\n",
        "    return sum([1 for t in tokens if is_allcaps(t)])\n",
        "\n",
        "def get_percentage_of_excalamations(s:str)->float:\n",
        "    if len(s) == 0:\n",
        "        return 0.0\n",
        "    exclamation_count = sum([1 for c in list(s) if c == '!'])\n",
        "    return exclamation_count / len(s)\n",
        "\n",
        "\n",
        "def is_empty_string(s:str)->bool:\n",
        "    if s == '' or s == None:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def do_basic_nlp_cleaning(line:str)->str:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(line)\n",
        "\n",
        "    # Some tokens start with a punctuation, remove the first one\n",
        "    def remove_first_punctuation(tok:str)->str:\n",
        "        return                                                              \\\n",
        "            tok[1:]                                                         \\\n",
        "            if tok[0] in set(string.punctuation) and len(tok) != 0          \\\n",
        "            else tok\n",
        "\n",
        "    tokens = [remove_first_punctuation(w) for w in tokens]\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"german\"))\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [w for w in tokens if not is_punct_only(w)]\n",
        "\n",
        "    # Stem words\n",
        "    stem = SnowballStemmer('german')\n",
        "    tokens = [stem.stem(w) for w in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def get_cleaning_function(remove_named_ents:bool=True, pos_tagging:bool=False):\n",
        "    nlp = de_core_news_sm.load()\n",
        "    emoji = Emoji(nlp)\n",
        "    nlp.add_pipe(emoji, first=True)\n",
        "    stopwords = spacy.lang.de.stop_words.STOP_WORDS\n",
        "\n",
        "    def do_basic_nlp_cleaning(line:str)->str:\n",
        "        def is_interesting_token(token, doc):\n",
        "            if token.pos_ in set(['NUM', 'SYM']):\n",
        "                return False\n",
        "            if remove_named_ents:\n",
        "                for e in doc.ents:\n",
        "                    for t in e:\n",
        "                        if token.text == t.text:\n",
        "                            return False\n",
        "            if token.text in stopwords:\n",
        "                return False\n",
        "            if (token.is_punct):\n",
        "                return False\n",
        "            #if token._.is_emoji:\n",
        "            #    return False\n",
        "            return True\n",
        "\n",
        "        def remove_terminal_punctuations(word):\n",
        "            word = word.strip()\n",
        "            while word != \"\" and word[0] in list(string.punctuation):\n",
        "                word = word[1:]\n",
        "            while word != \"\" and word[-1] in list(string.punctuation):\n",
        "                word = word[:-1]\n",
        "            return word\n",
        "\n",
        "        def get_final_string(tok, doc):\n",
        "            lemma = tok.lemma_.lower()\n",
        "            if pos_tagging:\n",
        "                lemma = lemma + \":\" + tok.pos_\n",
        "                lemma = lemma + \":\" + tok.tag_\n",
        "            return lemma\n",
        "\n",
        "        doc = nlp(line)\n",
        "        words = [get_final_string(tok, doc) for tok in doc if is_interesting_token(tok, doc)]\n",
        "        words = [remove_terminal_punctuations(word) for word in words]\n",
        "        words = [word for word in words if word != \"\"]\n",
        "        return  \" \".join(words)\n",
        "\n",
        "    return do_basic_nlp_cleaning\n",
        "\n",
        "def get_enriched_dataset(df):\n",
        "    cleaning_fn = get_cleaning_function(remove_named_ents=True, pos_tagging=True)\n",
        "    df['cleaned_comment_text'] = df['comment_text'].map(cleaning_fn)\n",
        "    df['n_all_caps'] = df['comment_text'].map(get_num_of_allcap_words)\n",
        "    df['perc_exclamations'] = df['comment_text'].map(get_percentage_of_excalamations)\n",
        "    df['num_exclamations'] = df['comment_text'].map(lambda s: sum([1 for x in list(s) if x == '!']))\n",
        "    return df\n",
        "\n",
        "import functools\n",
        "from functools import lru_cache\n",
        "\n",
        "@lru_cache(maxsize=128)\n",
        "def get_all_enriched_dfs_cached():\n",
        "    train_df, test_df, extra_df = get_clean_train_test_df()\n",
        "    train_df = get_enriched_dataset(train_df)\n",
        "    test_df = get_enriched_dataset(test_df)\n",
        "    extra_df = get_enriched_dataset(extra_df)\n",
        "    return train_df, test_df, extra_df\n",
        "    \n",
        "def get_all_enriched_dataframes():\n",
        "    tr, te, ex = get_all_enriched_dfs_cached()\n",
        "    return tr.copy(), te.copy(), ex.copy()\n",
        "\n",
        "train_df, test_df, extra_df = get_all_enriched_dfs_cached()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68DFX_fjYgQz"
      },
      "source": [
        "# Classify all the three tasks using the best models determined in the earlier notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zL5Boe5qYfF4",
        "outputId": "c8a336ab-6130-4e9d-8624-0589b90cbdb6"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB, CategoricalNB, BernoulliNB\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "#from imblearn.pipeline import Pipeline\n",
        "#from imblearn.over_sampling import SMOTE \n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from mlxtend.preprocessing import DenseTransformer\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import IPython\n",
        "\n",
        "def get_feature_column_names(df):\n",
        "    return [cname for cname in df.columns if not cname.startswith('Sub')]\n",
        "\n",
        "def get_target_column_names(df):\n",
        "    return [cname for cname in df.columns if cname.startswith('Sub')]\n",
        "\n",
        "def is_text_column(colname:str)->bool:\n",
        "    if 'text' in colname:\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "def get_text_columns(df)->list:\n",
        "    return [cn for cn in df.columns if is_text_column(cn)]\n",
        "\n",
        "def get_nontext_columns(df)->list:\n",
        "    return [cn for cn in df.columns if not is_text_column(cn)]\n",
        "\n",
        "def run_classification(                                                     \\\n",
        "                       dataset:pd.DataFrame,                                \\\n",
        "                       extra_dataset:pd.DataFrame,                          \\\n",
        "                       test_dataset:pd.DataFrame,                           \\\n",
        "                       target_column:str,                                   \\\n",
        "                       clf_gen_fn,                                          \\\n",
        "                       grid_search_dict=None,                               \\\n",
        "                       use_extra_dataset=False,                             \\\n",
        "                       )->tuple:\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "\n",
        "    seed_random()\n",
        "\n",
        "    X = dataset[get_feature_column_names(dataset)]\n",
        "    y = dataset[target_column]\n",
        "    trainX, testX, trainY, testY = train_test_split(X, y, random_state=0)\n",
        "\n",
        "    if use_extra_dataset:\n",
        "        extraX = extra_dataset[get_feature_column_names(dataset)]\n",
        "        extray = extra_dataset[target_column]\n",
        "        trainX = pd.concat([trainX, extraX])\n",
        "        trainY = pd.concat([trainY, extray])\n",
        "        print(\"Added additional data from GermEval 2018\")\n",
        "\n",
        "    if None != grid_search_dict and isinstance(grid_search_dict, dict):\n",
        "        # TODO: Update grid for the vectorizers\n",
        "        # Right now, we're hitting RAM constraints if we turn these on\n",
        "        gridupd = {\n",
        "        }\n",
        "        grid_search_dict = grid_search_dict.copy()\n",
        "        grid_search_dict.update(gridupd)\n",
        "\n",
        "                        \n",
        "    column_trans = make_column_transformer(                                 \\\n",
        "                            (CountVectorizer(ngram_range=(1,1)), 'cleaned_comment_text'),   \\\n",
        "                            (TfidfVectorizer(use_idf=True), 'cleaned_comment_text'),    \\\n",
        "                            (TfidfVectorizer(use_idf=False), 'cleaned_comment_text'),    \\\n",
        "                            remainder=MinMaxScaler(),                       \\\n",
        "                        )\n",
        "    \n",
        "    classif_pipeline = Pipeline(                                        \\\n",
        "                            [                                           \\\n",
        "                                ('column_transformer', column_trans),   \\\n",
        "                                ('dense', DenseTransformer()),          \\\n",
        "                                ('clf', clf_gen_fn()),                  \\\n",
        "                            ])\n",
        "\n",
        "    if None != grid_search_dict and isinstance(grid_search_dict, dict):\n",
        "        search = GridSearchCV(classif_pipeline, grid_search_dict, cv=3, n_jobs=-1, scoring='f1')\n",
        "        search.fit(trainX, trainY)\n",
        "        classif_pipeline = search.best_estimator_\n",
        "        print(\"best params: \", search.best_params_)\n",
        "        print(\"best f1 score: \", search.best_score_)\n",
        "    else:\n",
        "        classif_pipeline.fit(trainX, trainY)\n",
        "\n",
        "    y_pred = classif_pipeline.predict(testX)\n",
        "    print('-' * 40)\n",
        "\n",
        "    IPython.display.display(classif_pipeline)\n",
        "    return accuracy_score(testY, y_pred), f1_score(testY, y_pred), classif_pipeline\n",
        "\n",
        "def predict_on_test_set(\\\n",
        "                        dataset,\\\n",
        "                        colname,\\\n",
        "                        classif_pipeline):\n",
        "    seed_random()\n",
        "    dataset = dataset[[cn for cn in dataset.columns if cn != 'comment_text']]\n",
        "    y_pred = classif_pipeline.predict(dataset)\n",
        "    return y_pred.tolist()\n",
        "\n",
        "def run_classifiers():\n",
        "\n",
        "    linearsvc_gen = lambda: LinearSVC()\n",
        "    linearsvc_paramgrid = {'clf__class_weight': [None, 'balanced'], 'clf__max_iter': [1000, 10000]}\n",
        "    classifiers = {\n",
        "        \"LinearSVC\": [\n",
        "                      \"Sub1_Toxic\",\n",
        "                      linearsvc_gen,\n",
        "                      linearsvc_paramgrid,\n",
        "                      True, # Use additional data\n",
        "                      ]\n",
        "    }\n",
        "\n",
        "    rfc_gen = lambda: RandomForestClassifier()\n",
        "    rfc_paramgrid = {'clf__criterion': ['entropy', 'gini'], 'clf__min_samples_split': [2, 10, 100], 'clf__class_weight': ['balanced', 'balanced_subsample']}\n",
        "    tempdict = {\n",
        "        'RandomForest1': [\n",
        "                          \"Sub2_Engaging\",\n",
        "                          rfc_gen,\n",
        "                          rfc_paramgrid,\n",
        "                          False # Don't use additional data\n",
        "        ]\n",
        "    }\n",
        "    classifiers.update(tempdict)\n",
        "    \n",
        "    mnb_gen = lambda: MultinomialNB()\n",
        "    mnb_paramgrid = {'clf__fit_prior': [True, False]}\n",
        "    tempdict = {\n",
        "        'MultinomialNB1': [\n",
        "                           \"Sub3_FactClaiming\",\n",
        "                           mnb_gen,\n",
        "                           mnb_paramgrid,\n",
        "                           False # Don't use additional data\n",
        "        ]\n",
        "    }\n",
        "    classifiers.update(tempdict)\n",
        "\n",
        "    best_classifiers = list() # list of tuples: (columnname, clfname, pipeline)\n",
        "\n",
        "    for clfname, val in classifiers.items():\n",
        "        colname = val[0]\n",
        "        generator = val[1]\n",
        "        gridsearch = val[2]\n",
        "        use_extra_data = val[3]\n",
        "\n",
        "        print()\n",
        "        print()\n",
        "        print()\n",
        "        print()\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Running: \", colname, clfname)\n",
        "\n",
        "        train_df, test_df, extra_df = get_all_enriched_dfs_cached()\n",
        "        acc, f1, classif_pipeline = run_classification(\\\n",
        "            dataset=train_df,\\\n",
        "            extra_dataset=extra_df if use_extra_data else None,\\\n",
        "            test_dataset=test_df,\\\n",
        "            target_column=colname,\\\n",
        "            clf_gen_fn=generator,\\\n",
        "            grid_search_dict=gridsearch,\\\n",
        "            use_extra_dataset=use_extra_data)\n",
        "        \n",
        "        best_classifiers.append((colname, clfname, classif_pipeline))\n",
        "\n",
        "\n",
        "        print()\n",
        "        print()\n",
        "        print()\n",
        "        print(colname, clfname, \"acc = \", acc, \"f1 = \", f1,)\n",
        "\n",
        "\n",
        "\n",
        "    or_train_df, or_test_df, or_extra_df = get_train_test_df()\n",
        "    test_pred_df_dict = {\n",
        "        'comment_text': or_test_df['comment_text'].to_numpy().tolist()\n",
        "    }\n",
        "\n",
        "    retval = list()\n",
        "    for colname, clfname, clf_pipeline in best_classifiers:\n",
        "        train_df, test_df, extra_df = get_all_enriched_dfs_cached()\n",
        "        y_pred = predict_on_test_set(test_df, clfname, clf_pipeline)\n",
        "        temp_dict = {colname: y_pred}\n",
        "        test_pred_df_dict.update(temp_dict)\n",
        "        tempdict = {'clfname': clfname, 'colname': colname, 'clf_pipeline': clf_pipeline}\n",
        "        retval.append(tempdict)\n",
        "\n",
        "    result_df = pd.DataFrame(test_pred_df_dict)\n",
        "    return result_df, retval\n",
        "\n",
        "seed_random()\n",
        "result_df, retval = run_classifiers()\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Running:  Sub1_Toxic LinearSVC\n",
            "Added additional data from GermEval 2018\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best params:  {'clf__class_weight': 'balanced', 'clf__max_iter': 10000}\n",
            "best f1 score:  0.5167117866763667\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 {color: black;background-color: white;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 pre{padding: 0;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-toggleable {background-color: white;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-estimator:hover {background-color: #d4ebff;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-item {z-index: 1;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-parallel-item:only-child::after {width: 0;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-a33d569c-fb96-4f66-8211-b4e6c23eac81 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-a33d569c-fb96-4f66-8211-b4e6c23eac81\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"9ef21c17-1789-406d-8378-17ea44a03761\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"9ef21c17-1789-406d-8378-17ea44a03761\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('column_transformer',\n",
              "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                                   transformers=[('countvectorizer',\n",
              "                                                  CountVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-1',\n",
              "                                                  TfidfVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-2',\n",
              "                                                  TfidfVectorizer(use_idf=False),\n",
              "                                                  'cleaned_comment_text')])),\n",
              "                ('dense', DenseTransformer()),\n",
              "                ('clf', LinearSVC(class_weight='balanced', max_iter=10000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"27cb42eb-f11b-4711-88d3-1cd9e65d6888\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"27cb42eb-f11b-4711-88d3-1cd9e65d6888\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                  transformers=[('countvectorizer', CountVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-1', TfidfVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-2',\n",
              "                                 TfidfVectorizer(use_idf=False),\n",
              "                                 'cleaned_comment_text')])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"f78e312a-5b02-4f42-9d19-d63a3e7bd725\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"f78e312a-5b02-4f42-9d19-d63a3e7bd725\">countvectorizer</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"8e9bda9a-57bf-495e-b78c-c48c00e261d5\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"8e9bda9a-57bf-495e-b78c-c48c00e261d5\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"4594aa8b-b55f-45b3-8cf0-dcf931c3e517\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"4594aa8b-b55f-45b3-8cf0-dcf931c3e517\">tfidfvectorizer-1</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"a5f13062-21cc-4403-8685-1e3fdada5ee0\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"a5f13062-21cc-4403-8685-1e3fdada5ee0\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"1b56c3fe-1179-4cbe-92ac-ab41d97cc990\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"1b56c3fe-1179-4cbe-92ac-ab41d97cc990\">tfidfvectorizer-2</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"05c84931-7e4e-43b4-ab25-2e7777ac4678\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"05c84931-7e4e-43b4-ab25-2e7777ac4678\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(use_idf=False)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"99bfaffb-5f17-406d-a0d7-bb68eacea7b5\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"99bfaffb-5f17-406d-a0d7-bb68eacea7b5\">remainder</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps', 'perc_exclamations', 'num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"916d84ba-3b5f-4985-a734-26f52effc1e9\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"916d84ba-3b5f-4985-a734-26f52effc1e9\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"732a9806-aaec-4b7d-9e60-25e2e87539d8\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"732a9806-aaec-4b7d-9e60-25e2e87539d8\">DenseTransformer</label><div class=\"sk-toggleable__content\"><pre>DenseTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"82cbc026-1550-438c-bbe5-b1a02af8a1e6\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"82cbc026-1550-438c-bbe5-b1a02af8a1e6\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(class_weight='balanced', max_iter=10000)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('column_transformer',\n",
              "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                                   transformers=[('countvectorizer',\n",
              "                                                  CountVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-1',\n",
              "                                                  TfidfVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-2',\n",
              "                                                  TfidfVectorizer(use_idf=False),\n",
              "                                                  'cleaned_comment_text')])),\n",
              "                ('dense', DenseTransformer()),\n",
              "                ('clf', LinearSVC(class_weight='balanced', max_iter=10000))])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Sub1_Toxic LinearSVC acc =  0.655819774718398 f1 =  0.4701348747591522\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Running:  Sub2_Engaging RandomForest1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/joblib/externals/loky/process_executor.py:705: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best params:  {'clf__class_weight': 'balanced', 'clf__criterion': 'gini', 'clf__min_samples_split': 100}\n",
            "best f1 score:  0.6144904297922249\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 {color: black;background-color: white;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 pre{padding: 0;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-toggleable {background-color: white;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-estimator:hover {background-color: #d4ebff;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-item {z-index: 1;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-parallel-item:only-child::after {width: 0;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-d4742ad5-a116-4d4c-8464-3bc89465e050 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-d4742ad5-a116-4d4c-8464-3bc89465e050\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2268bc2b-e9a7-45f1-a893-c5aa6f14eb8b\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"2268bc2b-e9a7-45f1-a893-c5aa6f14eb8b\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('column_transformer',\n",
              "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                                   transformers=[('countvectorizer',\n",
              "                                                  CountVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-1',\n",
              "                                                  TfidfVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-2',\n",
              "                                                  TfidfVectorizer(use_idf=False),\n",
              "                                                  'cleaned_comment_text')])),\n",
              "                ('dense', DenseTransformer()),\n",
              "                ('clf',\n",
              "                 RandomForestClassifier(class_weight='balanced',\n",
              "                                        min_samples_split=100))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e86b1327-bc1f-4cc5-ae6f-5f48643b6629\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e86b1327-bc1f-4cc5-ae6f-5f48643b6629\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                  transformers=[('countvectorizer', CountVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-1', TfidfVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-2',\n",
              "                                 TfidfVectorizer(use_idf=False),\n",
              "                                 'cleaned_comment_text')])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"ebcfd5ae-f81d-4087-8848-7e0261ece6cd\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"ebcfd5ae-f81d-4087-8848-7e0261ece6cd\">countvectorizer</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"b3f903d4-6caf-4d8b-a10c-1015e6e903f7\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"b3f903d4-6caf-4d8b-a10c-1015e6e903f7\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"bd2872e0-d5af-4cd0-8499-d983ce19e014\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"bd2872e0-d5af-4cd0-8499-d983ce19e014\">tfidfvectorizer-1</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"61b77085-186c-43aa-bd17-4a68aa13caea\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"61b77085-186c-43aa-bd17-4a68aa13caea\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"130f44d6-c732-4cc6-88ff-3c030b53c9e2\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"130f44d6-c732-4cc6-88ff-3c030b53c9e2\">tfidfvectorizer-2</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"4c5e2bbc-ec46-445b-8050-1cfc2c21fd13\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"4c5e2bbc-ec46-445b-8050-1cfc2c21fd13\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(use_idf=False)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"154dd701-30b1-4400-932e-6bf022a6e186\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"154dd701-30b1-4400-932e-6bf022a6e186\">remainder</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps', 'perc_exclamations', 'num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"d3547037-c609-42dc-9d2a-46a8b06366ff\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"d3547037-c609-42dc-9d2a-46a8b06366ff\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"89c514f5-ff5a-4937-8430-049cfd0802ba\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"89c514f5-ff5a-4937-8430-049cfd0802ba\">DenseTransformer</label><div class=\"sk-toggleable__content\"><pre>DenseTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"2d4b50d3-8c6b-4fe5-969e-8fc2f068bcaa\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"2d4b50d3-8c6b-4fe5-969e-8fc2f068bcaa\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight='balanced', min_samples_split=100)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('column_transformer',\n",
              "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                                   transformers=[('countvectorizer',\n",
              "                                                  CountVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-1',\n",
              "                                                  TfidfVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-2',\n",
              "                                                  TfidfVectorizer(use_idf=False),\n",
              "                                                  'cleaned_comment_text')])),\n",
              "                ('dense', DenseTransformer()),\n",
              "                ('clf',\n",
              "                 RandomForestClassifier(class_weight='balanced',\n",
              "                                        min_samples_split=100))])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Sub2_Engaging RandomForest1 acc =  0.7984981226533167 f1 =  0.608272506082725\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Running:  Sub3_FactClaiming MultinomialNB1\n",
            "best params:  {'clf__fit_prior': False}\n",
            "best f1 score:  0.6029812346017276\n",
            "----------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 {color: black;background-color: white;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 pre{padding: 0;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-toggleable {background-color: white;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-estimator:hover {background-color: #d4ebff;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-item {z-index: 1;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-parallel::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 2em;bottom: 0;left: 50%;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-parallel-item {display: flex;flex-direction: column;position: relative;background-color: white;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-parallel-item:only-child::after {width: 0;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;position: relative;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-label label {font-family: monospace;font-weight: bold;background-color: white;display: inline-block;line-height: 1.2em;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-label-container {position: relative;z-index: 2;text-align: center;}#sk-5e3667fc-ef02-499b-a4f3-c372532c6425 div.sk-container {display: inline-block;position: relative;}</style><div id=\"sk-5e3667fc-ef02-499b-a4f3-c372532c6425\" class\"sk-top-container\"><div class=\"sk-container\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e30a6b7a-5eda-4254-9602-b2c38e51e39a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e30a6b7a-5eda-4254-9602-b2c38e51e39a\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[('column_transformer',\n",
              "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                                   transformers=[('countvectorizer',\n",
              "                                                  CountVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-1',\n",
              "                                                  TfidfVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-2',\n",
              "                                                  TfidfVectorizer(use_idf=False),\n",
              "                                                  'cleaned_comment_text')])),\n",
              "                ('dense', DenseTransformer()),\n",
              "                ('clf', MultinomialNB(fit_prior=False))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"43dfe2a8-6c62-42a3-8059-f948696482dc\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"43dfe2a8-6c62-42a3-8059-f948696482dc\">column_transformer: ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                  transformers=[('countvectorizer', CountVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-1', TfidfVectorizer(),\n",
              "                                 'cleaned_comment_text'),\n",
              "                                ('tfidfvectorizer-2',\n",
              "                                 TfidfVectorizer(use_idf=False),\n",
              "                                 'cleaned_comment_text')])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"e5fbc8ae-eade-4697-8856-d49268230b11\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"e5fbc8ae-eade-4697-8856-d49268230b11\">countvectorizer</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"bcc2dc4f-fb3f-4b58-a018-1cccc904326a\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"bcc2dc4f-fb3f-4b58-a018-1cccc904326a\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"02f01c68-2b33-4d7e-904d-8d21607cf131\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"02f01c68-2b33-4d7e-904d-8d21607cf131\">tfidfvectorizer-1</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"c1cc1c2a-e1a5-48ee-be58-983f0f7a3037\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"c1cc1c2a-e1a5-48ee-be58-983f0f7a3037\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"10e1f17d-3ac3-4e3c-9e55-b15577875052\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"10e1f17d-3ac3-4e3c-9e55-b15577875052\">tfidfvectorizer-2</label><div class=\"sk-toggleable__content\"><pre>cleaned_comment_text</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5abe9e2c-c462-4025-808f-04bff6790456\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"5abe9e2c-c462-4025-808f-04bff6790456\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(use_idf=False)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"d794622b-c4f1-41c3-8adb-77977aed2888\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"d794622b-c4f1-41c3-8adb-77977aed2888\">remainder</label><div class=\"sk-toggleable__content\"><pre>['n_all_caps', 'perc_exclamations', 'num_exclamations']</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"ccf555e4-63d0-4f7b-9a66-7d1d77fb02d7\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"ccf555e4-63d0-4f7b-9a66-7d1d77fb02d7\">MinMaxScaler</label><div class=\"sk-toggleable__content\"><pre>MinMaxScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"5a43a025-1a78-4dd2-8efe-99b14476f9f6\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"5a43a025-1a78-4dd2-8efe-99b14476f9f6\">DenseTransformer</label><div class=\"sk-toggleable__content\"><pre>DenseTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"d419370d-65bd-445a-bf93-1e8a6504bbc2\" type=\"checkbox\" ><label class=\"sk-toggleable__label\" for=\"d419370d-65bd-445a-bf93-1e8a6504bbc2\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB(fit_prior=False)</pre></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('column_transformer',\n",
              "                 ColumnTransformer(remainder=MinMaxScaler(),\n",
              "                                   transformers=[('countvectorizer',\n",
              "                                                  CountVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-1',\n",
              "                                                  TfidfVectorizer(),\n",
              "                                                  'cleaned_comment_text'),\n",
              "                                                 ('tfidfvectorizer-2',\n",
              "                                                  TfidfVectorizer(use_idf=False),\n",
              "                                                  'cleaned_comment_text')])),\n",
              "                ('dense', DenseTransformer()),\n",
              "                ('clf', MultinomialNB(fit_prior=False))])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Sub3_FactClaiming MultinomialNB1 acc =  0.704630788485607 f1 =  0.5902777777777777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qVRvfy5Y8eGV",
        "outputId": "ce3015fa-61ab-4516-e243-f3913ab77fa1"
      },
      "source": [
        "result_df"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment_text</th>\n",
              "      <th>Sub1_Toxic</th>\n",
              "      <th>Sub2_Engaging</th>\n",
              "      <th>Sub3_FactClaiming</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ziemlich traurig diese Kommentare zu lesen. Ih...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Sag ich doch, wir befeuern den Klimawandel. Ra...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dummerweise haben wir in der EU und in der USA...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>\"So lange Gewinnmaximierung Vorrang hat, wird ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sollte es dann doch einen Klimawandel geben, d...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>@USER Ja o.k aber nicht mit so Schwachsinnigen...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>@USER seltsamerweise steigen die Temperaturen ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>@USER aus Sicht der grÃƒÂ¼nen Fraktion kurz ÃƒÂ¼be...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Wenn die Industrie angeblich so extrem viel tu...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Die LuftqualitÃƒÂ¤t ist nicht besser</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Schublade auf, Schublade zu. Zu mehr Denkleist...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>@USER Schon mal was von Physik gehÃƒÂ¶rt?</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>@USER geh wieder pennen...dein GesÃƒÂ¼lze intere...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Wenn ich einen Konjunktiv verwende hat das ein...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Nach mir die Sintflut Ã¢ËœÂº</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>@USER Staatsfernsehen ! !</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>@USER Oh weh das mÃƒÂ¶chten wir uns garnicht vor...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Aber eure Generation waren die schlimmsten Ver...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Eine ernsthafte Diskussion ist doch vÃƒÂ¶llig ÃƒÂ¼...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>FÃƒÂ¼r Deutsche/Einzahler gibt es wieder nix ! A...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>\" 675 Milliarden!\" Das ist dein Denkfehler. Au...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>@USER Geld ist genug da. Oder mit Volker Pispe...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>@USER Wir bezahlen auch nur mit unserem Vertra...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>War schon Thema bei Illner. KÃƒÂ¶nnt ihr euch ni...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>soll er keine Rente erhalten? ÃƒÅ“brigens bekomm...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Wie doof muÃƒÅ¸ man eigentlich sein,um diesen LÃƒ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Passt zum Thema Friedmann gestern wo die CDU P...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Diese AnkÃƒÂ¼ndigungen werden immer vor Wahlen g...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>...ohne Kommentar!!!!</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>WER zahlt den die Steuern, @USER....das ist da...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Die \"Rentenreform\" der SPD ist ein schlechter ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Die \"Rentenreform\" der SPD ist ein schlechter ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Wie wÃƒÂ¤re es dann mit einem (bedingungslosen) ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Und jede Form des Einkommens wird ab einer bes...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>@USER Nein, bei der Finanzierung liegst du kom...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>@USER Artikel 14 GG ist sicherlich bekannt. Wo...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>3) Unser Geldsystem (Euro, Eurokrise, alternat...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>\"Das Geld dafÃƒÂ¼r kann man doch einfach drucken...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Zu dem verweise ich gerne auf meine Facebookse...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>Die Idee einer Grundrente kommt von der Linksp...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>Sehr gut analysiert - bin ich vollkommen konfo...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>@USER noch mehr BÃƒÂ¼rokratie? Gleiches Geld fÃƒÂ¼...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>@USER alle ausreisepflichtige FlÃƒÂ¼chtlinge raus.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>@USER Doch wurde und wird was verteilt , die D...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>@USER Wieso Schule</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>Es ist fatal die einen hilfesuchenden Menschen...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>Ich bin ganz klar fÃƒÂ¼r die Besteuerung. Nur mÃƒ...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>Also eine gleiche Rente fÃƒÂ¼r alle? Wie wÃƒÂ¤re e...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>@USER, warum wÃƒÂ¤hlt man eine Partei, die den T...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>Wie wÃƒÂ¤re es mit der Steuerpolitik der Partei ...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>Gestern bei Illner, Montag bei @MODERATOR ...i...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>51</th>\n",
              "      <td>Mein Gott der war erst gestern bei Illner. Die...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>@USER Die CDU lÃƒÂ¤sst das so wie so nicht zu . ...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>Bei meiner beschissenen Rente als 2x Geschiede...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>Wer 40 Jahre zum Mindestlohn arbeiten muÃƒÅ¸, er...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>Wenn ich immer lese \"Das muss die jÃƒÂ¼ngere Gen...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>56</th>\n",
              "      <td>Das machen wir doch. Und deswegen zerstÃƒÂ¶ren w...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>57</th>\n",
              "      <td>Die jÃƒÂ¼ngere Generation bekommt so viel Kinder...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58</th>\n",
              "      <td>keine Sorge, sie wussten es.</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>Gute Kritik, doch wir brauchen nicht nur 13 Eu...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         comment_text  ...  Sub3_FactClaiming\n",
              "0   Ziemlich traurig diese Kommentare zu lesen. Ih...  ...                  1\n",
              "1   Sag ich doch, wir befeuern den Klimawandel. Ra...  ...                  0\n",
              "2   Dummerweise haben wir in der EU und in der USA...  ...                  1\n",
              "3   \"So lange Gewinnmaximierung Vorrang hat, wird ...  ...                  0\n",
              "4   Sollte es dann doch einen Klimawandel geben, d...  ...                  1\n",
              "5   @USER Ja o.k aber nicht mit so Schwachsinnigen...  ...                  1\n",
              "6   @USER seltsamerweise steigen die Temperaturen ...  ...                  1\n",
              "7   @USER aus Sicht der grÃƒÂ¼nen Fraktion kurz ÃƒÂ¼be...  ...                  1\n",
              "8   Wenn die Industrie angeblich so extrem viel tu...  ...                  0\n",
              "9                  Die LuftqualitÃƒÂ¤t ist nicht besser  ...                  0\n",
              "10  Schublade auf, Schublade zu. Zu mehr Denkleist...  ...                  0\n",
              "11            @USER Schon mal was von Physik gehÃƒÂ¶rt?  ...                  0\n",
              "12  @USER geh wieder pennen...dein GesÃƒÂ¼lze intere...  ...                  0\n",
              "13  Wenn ich einen Konjunktiv verwende hat das ein...  ...                  1\n",
              "14                          Nach mir die Sintflut Ã¢ËœÂº  ...                  0\n",
              "15                          @USER Staatsfernsehen ! !  ...                  0\n",
              "16  @USER Oh weh das mÃƒÂ¶chten wir uns garnicht vor...  ...                  0\n",
              "17  Aber eure Generation waren die schlimmsten Ver...  ...                  1\n",
              "18  Eine ernsthafte Diskussion ist doch vÃƒÂ¶llig ÃƒÂ¼...  ...                  1\n",
              "19  FÃƒÂ¼r Deutsche/Einzahler gibt es wieder nix ! A...  ...                  1\n",
              "20  \" 675 Milliarden!\" Das ist dein Denkfehler. Au...  ...                  1\n",
              "21  @USER Geld ist genug da. Oder mit Volker Pispe...  ...                  0\n",
              "22  @USER Wir bezahlen auch nur mit unserem Vertra...  ...                  1\n",
              "23  War schon Thema bei Illner. KÃƒÂ¶nnt ihr euch ni...  ...                  0\n",
              "24  soll er keine Rente erhalten? ÃƒÅ“brigens bekomm...  ...                  1\n",
              "25  Wie doof muÃƒÅ¸ man eigentlich sein,um diesen LÃƒ...  ...                  1\n",
              "26  Passt zum Thema Friedmann gestern wo die CDU P...  ...                  1\n",
              "27  Diese AnkÃƒÂ¼ndigungen werden immer vor Wahlen g...  ...                  1\n",
              "28                              ...ohne Kommentar!!!!  ...                  0\n",
              "29  WER zahlt den die Steuern, @USER....das ist da...  ...                  1\n",
              "30  Die \"Rentenreform\" der SPD ist ein schlechter ...  ...                  1\n",
              "31  Die \"Rentenreform\" der SPD ist ein schlechter ...  ...                  1\n",
              "32  Wie wÃƒÂ¤re es dann mit einem (bedingungslosen) ...  ...                  1\n",
              "33  Und jede Form des Einkommens wird ab einer bes...  ...                  1\n",
              "34  @USER Nein, bei der Finanzierung liegst du kom...  ...                  1\n",
              "35  @USER Artikel 14 GG ist sicherlich bekannt. Wo...  ...                  1\n",
              "36  3) Unser Geldsystem (Euro, Eurokrise, alternat...  ...                  1\n",
              "37  \"Das Geld dafÃƒÂ¼r kann man doch einfach drucken...  ...                  1\n",
              "38  Zu dem verweise ich gerne auf meine Facebookse...  ...                  1\n",
              "39  Die Idee einer Grundrente kommt von der Linksp...  ...                  1\n",
              "40  Sehr gut analysiert - bin ich vollkommen konfo...  ...                  1\n",
              "41  @USER noch mehr BÃƒÂ¼rokratie? Gleiches Geld fÃƒÂ¼...  ...                  0\n",
              "42   @USER alle ausreisepflichtige FlÃƒÂ¼chtlinge raus.  ...                  0\n",
              "43  @USER Doch wurde und wird was verteilt , die D...  ...                  0\n",
              "44                                 @USER Wieso Schule  ...                  0\n",
              "45  Es ist fatal die einen hilfesuchenden Menschen...  ...                  1\n",
              "46  Ich bin ganz klar fÃƒÂ¼r die Besteuerung. Nur mÃƒ...  ...                  1\n",
              "47  Also eine gleiche Rente fÃƒÂ¼r alle? Wie wÃƒÂ¤re e...  ...                  1\n",
              "48  @USER, warum wÃƒÂ¤hlt man eine Partei, die den T...  ...                  1\n",
              "49  Wie wÃƒÂ¤re es mit der Steuerpolitik der Partei ...  ...                  1\n",
              "50  Gestern bei Illner, Montag bei @MODERATOR ...i...  ...                  1\n",
              "51  Mein Gott der war erst gestern bei Illner. Die...  ...                  1\n",
              "52  @USER Die CDU lÃƒÂ¤sst das so wie so nicht zu . ...  ...                  1\n",
              "53  Bei meiner beschissenen Rente als 2x Geschiede...  ...                  1\n",
              "54  Wer 40 Jahre zum Mindestlohn arbeiten muÃƒÅ¸, er...  ...                  1\n",
              "55  Wenn ich immer lese \"Das muss die jÃƒÂ¼ngere Gen...  ...                  1\n",
              "56  Das machen wir doch. Und deswegen zerstÃƒÂ¶ren w...  ...                  1\n",
              "57  Die jÃƒÂ¼ngere Generation bekommt so viel Kinder...  ...                  1\n",
              "58                       keine Sorge, sie wussten es.  ...                  1\n",
              "59  Gute Kritik, doch wir brauchen nicht nur 13 Eu...  ...                  1\n",
              "\n",
              "[60 rows x 4 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsoIIo3O8IqI",
        "outputId": "86cccfb6-da98-4624-fba2-51a9937280ec"
      },
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    os.mkdir('/content/drive/MyDrive/NLPAssignment')\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "    print(\"Continue to copy to destination...\")\n",
        "result_df.to_csv('/content/drive/MyDrive/NLPAssignment/GermEval2020_test_predicted.csv', index=False)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[Errno 17] File exists: '/content/drive/MyDrive/NLPAssignment'\n",
            "Continue to copy to destination...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crH9Isqp9tCz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}