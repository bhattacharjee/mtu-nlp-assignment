{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM8bnszvGs8tzMI996SDjka",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment1/NLP_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-DAX9VLZlw3"
      },
      "source": [
        "import requests\n",
        "def get_train_test_files():\n",
        "    TRAIN_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Test_For_Evaluation.csv'\n",
        "    TRAIN_FILE_LOCAL = 'Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE_LOCAL = 'Assessment1_Toxic_Test.csv'\n",
        "\n",
        "    def download(url, localfile):\n",
        "        with open(localfile, 'wb') as f:\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            f.write(r.content)\n",
        "\n",
        "    download(TRAIN_FILE, TRAIN_FILE_LOCAL)\n",
        "    download(TEST_FILE, TEST_FILE_LOCAL)\n",
        "\n",
        "    return TRAIN_FILE_LOCAL, TEST_FILE_LOCAL\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERvBOEn0wss6"
      },
      "source": [
        "!pip install spacy nltk huggingface -q                  >/dev/null 2>&1\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v6ohiY0s5N1"
      },
      "source": [
        "import pandas as pd\n",
        "def get_train_test_df():\n",
        "    train_csv, test_csv = get_train_test_files()\n",
        "\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "\n",
        "    return train_df, test_df"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37ljpCC2uFLn"
      },
      "source": [
        "train_df, test_df = get_train_test_df()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVfYulLnuGVJ"
      },
      "source": [
        "import re\n",
        "def remove_roles(line:str)->str:\n",
        "    # Remove texts like @USER, @MODERATOR etc\n",
        "    pat = re.compile(u'\\@[A-Za-z]+')\n",
        "    return re.sub(pat, '', line)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nU-BO8guUcM"
      },
      "source": [
        "import re\n",
        "def remove_emojis(line:str)->str:\n",
        "    pat = re.compile(\n",
        "        \"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"\n",
        "            u\"\\U0001F300-\\U0001F5FF\"\n",
        "            u\"\\U0001F680-\\U0001F6FF\"\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "        \"]\", flags=re.UNICODE)\n",
        "    return re.sub(pat, '', line)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Hi-K1gSDZW"
      },
      "source": [
        "import re\n",
        "def remove_ellipses(line:str)->str:\n",
        "    pat = re.compile(u'\\.\\.+')\n",
        "    return re.sub(pat, ' ', line)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6JFm16r7M3E"
      },
      "source": [
        "def to_lower(line:str)->str:\n",
        "    return line.lower()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_TxMNrC7doY",
        "outputId": "8ed2c095-fb62-4583-9f60-db6ab7825fba"
      },
      "source": [
        "def basic_clean(s:pd.Series)->pd.Series:\n",
        "    return s.map(to_lower)                                                  \\\n",
        "            .map(remove_emojis)                                             \\\n",
        "            .map(remove_roles)                                              \\\n",
        "            .map(remove_ellipses)\n",
        "    \n",
        "print(train_df.columns)\n",
        "print(train_df['comment_text'])\n",
        "\n",
        "train_df['comment_text'] = basic_clean(train_df['comment_text'])\n",
        "test_df['comment_text'] = basic_clean(test_df['comment_text'])\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['comment_text', 'Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming'], dtype='object')\n",
            "0       Gestern bei Illner, Montag bei @MODERATOR ...i...\n",
            "1       Mein Gott der war erst gestern bei Illner. Die...\n",
            "2       @USER Die CDU lässt das so wie so nicht zu . S...\n",
            "3       Bei meiner beschissenen Rente als 2x Geschiede...\n",
            "4       Wer 40 Jahre zum Mindestlohn arbeiten muß, erh...\n",
            "                              ...                        \n",
            "3189    Hier mal eine Info. Flüchtlinge werden 10 km v...\n",
            "3190    @USER.aha .Mal abwarten kommt bei uns auch .Fi...\n",
            "3191                                     @USER .So ist es\n",
            "3192                                 @USER .Die warten da\n",
            "3193    @USER .Das bekommen die gesagt wie sich verhal...\n",
            "Name: comment_text, Length: 3194, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "s8YSZZ4vxTk5",
        "outputId": "b18d5a57-e82b-46da-b9bc-f445592248d7"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string\n",
        "\n",
        "\n",
        "def is_punct_only(token:str)->bool:\n",
        "    for c in list(token):\n",
        "        if c not in string.punctuation:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_same(l1:list, l2:list)->bool:\n",
        "    if (len(l1) != len(l2)):\n",
        "        return False\n",
        "    for x, y in zip(l1, l2):\n",
        "        if x != y:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def do_basic_nlp_cleaning(line:str)->str:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(line)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"german\"))\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [w for w in tokens if not is_punct_only(w)]\n",
        "\n",
        "    # Stem words\n",
        "    stem = SnowballStemmer('german')\n",
        "    tokens = [stem.stem(w) for w in tokens]\n",
        "        \n",
        "    # Some tokens start with a punctuation, remove the first one\n",
        "    def remove_first_punctuation(tok:str)->str:\n",
        "        return                                                              \\\n",
        "            tok[1:]                                                         \\\n",
        "            if tok[0] in set(string.punctuation) and len(tok) != 0          \\\n",
        "            else tok\n",
        "    tokens = [remove_first_punctuation(w) for w in tokens]\n",
        "\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "new_series = train_df['comment_text'].map(do_basic_nlp_cleaning)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "\n",
        "sentence = \"I'm a dog and it's great! You're cool and Sandy's book is big. Don't tell her, you'll regret it! 'Hey', she'll say!\"\n",
        "nlp = English()\n",
        "tokenizer = nlp.tokenizer\n",
        "for x in nlp.tokenizer(sentence):\n",
        "    print(x)\n",
        "\"\"\""
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom spacy.tokenizer import Tokenizer\\nfrom spacy.lang.en import English\\n\\nsentence = \"I\\'m a dog and it\\'s great! You\\'re cool and Sandy\\'s book is big. Don\\'t tell her, you\\'ll regret it! \\'Hey\\', she\\'ll say!\"\\nnlp = English()\\ntokenizer = nlp.tokenizer\\nfor x in nlp.tokenizer(sentence):\\n    print(x)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6HysGzPvUsJ",
        "outputId": "2a9d2414-ceac-4b69-8c42-f9cce51d2e78"
      },
      "source": [
        "train_df['comment_text'].map(remove_roles).map(remove_emojis)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       gestern bei illner, montag bei   ist das nicht...\n",
              "1       mein gott der war erst gestern bei illner. die...\n",
              "2        die cdu lässt das so wie so nicht zu . sagen ...\n",
              "3       bei meiner beschissenen rente als 2x geschiede...\n",
              "4       wer 40 jahre zum mindestlohn arbeiten muß, erh...\n",
              "                              ...                        \n",
              "3189    hier mal eine info. flüchtlinge werden 10 km v...\n",
              "3190    .aha .mal abwarten kommt bei uns auch .firmen ...\n",
              "3191                                           .so ist es\n",
              "3192                                       .die warten da\n",
              "3193     .das bekommen die gesagt wie sich verhalten s...\n",
              "Name: comment_text, Length: 3194, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4omBVP_7wBM2"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    }
  ]
}