{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Assignment_1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMGtxyQ8yNNQyAfNhaQ4STa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhattacharjee/mtu-nlp-assignment/blob/main/assignment1/NLP_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-DAX9VLZlw3"
      },
      "source": [
        "import requests\n",
        "def get_train_test_files():\n",
        "    TRAIN_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE = 'https://raw.githubusercontent.com/bhattacharjee/mtu-nlp-assignment/main/assignment1/Assessment1_Toxic_Test_For_Evaluation.csv'\n",
        "    TRAIN_FILE_LOCAL = 'Assessment1_Toxic_Train.csv'\n",
        "    TEST_FILE_LOCAL = 'Assessment1_Toxic_Test.csv'\n",
        "\n",
        "    def download(url, localfile):\n",
        "        with open(localfile, 'wb') as f:\n",
        "            r = requests.get(url, allow_redirects=True)\n",
        "            f.write(r.content)\n",
        "\n",
        "    download(TRAIN_FILE, TRAIN_FILE_LOCAL)\n",
        "    download(TEST_FILE, TEST_FILE_LOCAL)\n",
        "\n",
        "    return TRAIN_FILE_LOCAL, TEST_FILE_LOCAL\n"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERvBOEn0wss6"
      },
      "source": [
        "!pip install spacy nltk huggingface -q                  >/dev/null 2>&1\n",
        "!python -m spacy download de_core_news_sm               >/dev/null 2>&1"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v6ohiY0s5N1"
      },
      "source": [
        "import pandas as pd\n",
        "def get_train_test_df():\n",
        "    train_csv, test_csv = get_train_test_files()\n",
        "\n",
        "    train_df = pd.read_csv(train_csv)\n",
        "    test_df = pd.read_csv(test_csv)\n",
        "\n",
        "    return train_df, test_df"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37ljpCC2uFLn"
      },
      "source": [
        "train_df, test_df = get_train_test_df()"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVfYulLnuGVJ"
      },
      "source": [
        "import re\n",
        "def remove_roles(line:str)->str:\n",
        "    # Remove texts like @USER, @MODERATOR etc\n",
        "    pat = re.compile(u'\\@[A-Za-z]+')\n",
        "    return re.sub(pat, '', line)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nU-BO8guUcM"
      },
      "source": [
        "import re\n",
        "def remove_emojis(line:str)->str:\n",
        "    pat = re.compile(\n",
        "        \"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"\n",
        "            u\"\\U0001F300-\\U0001F5FF\"\n",
        "            u\"\\U0001F680-\\U0001F6FF\"\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "        \"]\", flags=re.UNICODE)\n",
        "    return re.sub(pat, '', line)"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_Hi-K1gSDZW"
      },
      "source": [
        "import re\n",
        "def remove_ellipses(line:str)->str:\n",
        "    pat = re.compile(u'\\.\\.+')\n",
        "    return re.sub(pat, ' ', line)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6JFm16r7M3E"
      },
      "source": [
        "def to_lower(line:str)->str:\n",
        "    return line.lower()"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_TxMNrC7doY",
        "outputId": "540334f4-d20b-43e9-f819-7d12b8cb9b52"
      },
      "source": [
        "def basic_clean(s:pd.Series)->pd.Series:\n",
        "    return s.map(to_lower)                                                  \\\n",
        "            .map(remove_emojis)                                             \\\n",
        "            .map(remove_roles)                                              \\\n",
        "            .map(remove_ellipses)\n",
        "    \n",
        "print(train_df.columns)\n",
        "print(train_df['comment_text'])\n",
        "\n",
        "train_df['comment_text'] = basic_clean(train_df['comment_text'])\n",
        "test_df['comment_text'] = basic_clean(test_df['comment_text'])\n"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['comment_text', 'Sub1_Toxic', 'Sub2_Engaging', 'Sub3_FactClaiming'], dtype='object')\n",
            "0       Gestern bei Illner, Montag bei @MODERATOR ...i...\n",
            "1       Mein Gott der war erst gestern bei Illner. Die...\n",
            "2       @USER Die CDU lässt das so wie so nicht zu . S...\n",
            "3       Bei meiner beschissenen Rente als 2x Geschiede...\n",
            "4       Wer 40 Jahre zum Mindestlohn arbeiten muß, erh...\n",
            "                              ...                        \n",
            "3189    Hier mal eine Info. Flüchtlinge werden 10 km v...\n",
            "3190    @USER.aha .Mal abwarten kommt bei uns auch .Fi...\n",
            "3191                                     @USER .So ist es\n",
            "3192                                 @USER .Die warten da\n",
            "3193    @USER .Das bekommen die gesagt wie sich verhal...\n",
            "Name: comment_text, Length: 3194, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "s8YSZZ4vxTk5",
        "outputId": "a3fbbf43-0f89-4864-d5e7-11aae3d6da08"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "import string\n",
        "\n",
        "\n",
        "def is_punct_only(token:str)->bool:\n",
        "    for c in list(token):\n",
        "        if c not in string.punctuation:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def is_same(l1:list, l2:list)->bool:\n",
        "    if (len(l1) != len(l2)):\n",
        "        return False\n",
        "    for x, y in zip(l1, l2):\n",
        "        if x != y:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def do_basic_nlp_cleaning(line:str)->str:\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('wordnet', quiet=True)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(line)\n",
        "\n",
        "    # Remove stop words\n",
        "    stop_words = set(stopwords.words(\"german\"))\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "\n",
        "    # Remove punctuations\n",
        "    tokens = [w for w in tokens if not is_punct_only(w)]\n",
        "\n",
        "    # Stem words\n",
        "    stem = SnowballStemmer('german')\n",
        "    tokens = [stem.stem(w) for w in tokens]\n",
        "        \n",
        "    # Some tokens start with a punctuation, remove the first one\n",
        "    def remove_first_punctuation(tok:str)->str:\n",
        "        return tok[1:] if tok[0] in set(string.punctuation) else tok\n",
        "    tokens = [remove_first_punctuation(w) for w in tokens]\n",
        "\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "new_series = train_df['comment_text'].map(do_basic_nlp_cleaning)\n",
        "\n",
        "print(len(new_series))\n",
        "print(new_series)\n",
        "print(new_series)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "\n",
        "sentence = \"I'm a dog and it's great! You're cool and Sandy's book is big. Don't tell her, you'll regret it! 'Hey', she'll say!\"\n",
        "nlp = English()\n",
        "tokenizer = nlp.tokenizer\n",
        "for x in nlp.tokenizer(sentence):\n",
        "    print(x)\n",
        "\"\"\""
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3194\n",
            "0       gest illn montag langsam or-parteihilf grunlin...\n",
            "1                    gott erst gest illn redaktion versag\n",
            "2       cdu lasst sag reich bekomm soll 10 milliard ho...\n",
            "3       beschiss rent 2x geschied mann steu krankenkas...\n",
            "4       wer 40 jahr mindestlohn arbeit muss erhalt 514...\n",
            "                              ...                        \n",
            "3189    mal info fluchtling 10 km kust schlauchboot st...\n",
            "3190    aha mal abwart kommt firm entlass gerad viel m...\n",
            "3191                                                   so\n",
            "3192                                             die wart\n",
            "3193    das bekomm gesagt verhalt soll kameras gericht...\n",
            "Name: comment_text, Length: 3194, dtype: object\n",
            "0       gest illn montag langsam or-parteihilf grunlin...\n",
            "1                    gott erst gest illn redaktion versag\n",
            "2       cdu lasst sag reich bekomm soll 10 milliard ho...\n",
            "3       beschiss rent 2x geschied mann steu krankenkas...\n",
            "4       wer 40 jahr mindestlohn arbeit muss erhalt 514...\n",
            "                              ...                        \n",
            "3189    mal info fluchtling 10 km kust schlauchboot st...\n",
            "3190    aha mal abwart kommt firm entlass gerad viel m...\n",
            "3191                                                   so\n",
            "3192                                             die wart\n",
            "3193    das bekomm gesagt verhalt soll kameras gericht...\n",
            "Name: comment_text, Length: 3194, dtype: object\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfrom spacy.tokenizer import Tokenizer\\nfrom spacy.lang.en import English\\n\\nsentence = \"I\\'m a dog and it\\'s great! You\\'re cool and Sandy\\'s book is big. Don\\'t tell her, you\\'ll regret it! \\'Hey\\', she\\'ll say!\"\\nnlp = English()\\ntokenizer = nlp.tokenizer\\nfor x in nlp.tokenizer(sentence):\\n    print(x)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6HysGzPvUsJ",
        "outputId": "a7c03279-7108-409f-8cd4-0b65e11b7b39"
      },
      "source": [
        "train_df['comment_text'].map(remove_roles).map(remove_emojis)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Gestern bei Illner, Montag bei  ...ist das nic...\n",
              "1       Mein Gott der war erst gestern bei Illner. Die...\n",
              "2        Die CDU lässt das so wie so nicht zu . Sagen ...\n",
              "3       Bei meiner beschissenen Rente als 2x Geschiede...\n",
              "4       Wer 40 Jahre zum Mindestlohn arbeiten muß, erh...\n",
              "                              ...                        \n",
              "3189    Hier mal eine Info. Flüchtlinge werden 10 km v...\n",
              "3190    .aha .Mal abwarten kommt bei uns auch .Firmen ...\n",
              "3191                                           .So ist es\n",
              "3192                                       .Die warten da\n",
              "3193     .Das bekommen die gesagt wie sich verhalten s...\n",
              "Name: comment_text, Length: 3194, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4omBVP_7wBM2"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    }
  ]
}